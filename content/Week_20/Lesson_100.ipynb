{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Day 100: Neural ODEs and Continuous Learning\n",
    "\n",
    "## \ud83c\udf89 Welcome to the Final Lesson!\n",
    "\n",
    "Congratulations on reaching Day 100 of your Machine Learning journey! Today, we explore one of the most exciting frontier topics in deep learning: **Neural Ordinary Differential Equations (Neural ODEs)**.\n",
    "\n",
    "## What are Neural ODEs?\n",
    "\n",
    "Neural ODEs represent a paradigm shift in how we think about deep neural networks. Instead of thinking of a neural network as a discrete sequence of transformations (layers), Neural ODEs model the transformation as a **continuous process** governed by differential equations.\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "Traditional ResNets can be viewed as:\n",
    "$$h_{t+1} = h_t + f(h_t, \\theta_t)$$\n",
    "\n",
    "Neural ODEs generalize this to continuous time:\n",
    "$$\\frac{dh(t)}{dt} = f(h(t), t, \\theta)$$\n",
    "\n",
    "## Why Neural ODEs Matter\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "1. **Time Series Modeling**: Natural representation of continuous-time data (medical records, financial data)\n",
    "2. **Physics-Informed ML**: Incorporating physical laws (conservation, dynamics) directly into neural networks\n",
    "3. **Generative Models**: Continuous normalizing flows for density estimation\n",
    "4. **Robustness**: Better adversarial robustness due to continuous transformations\n",
    "5. **Memory Efficiency**: Constant memory cost regardless of network depth\n",
    "\n",
    "### Common Misconceptions\n",
    "\n",
    "- \u274c \"Neural ODEs are just ResNets\" - They're a continuous generalization with unique properties\n",
    "- \u274c \"They're always slower\" - Trade-off between accuracy and computation is tunable\n",
    "- \u274c \"They're only theoretical\" - Active deployment in scientific computing and time series\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "\n",
    "1. \u2705 Understand the mathematical foundations of Neural ODEs\n",
    "2. \u2705 Implement a Neural ODE from scratch using PyTorch\n",
    "3. \u2705 Visualize continuous vs. discrete neural network dynamics\n",
    "4. \u2705 Train a Neural ODE on a real machine learning task\n",
    "5. \u2705 Understand the trade-offs and when to use Neural ODEs\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory",
   "metadata": {},
   "source": [
    "## Theory: From ResNets to Neural ODEs\n",
    "\n",
    "### The Evolution\n",
    "\n",
    "#### 1. Standard Feed-Forward Networks\n",
    "\n",
    "Traditional networks apply discrete transformations:\n",
    "$$h_{t+1} = \\sigma(W_t h_t + b_t)$$\n",
    "\n",
    "#### 2. Residual Networks (ResNets)\n",
    "\n",
    "ResNets add skip connections:\n",
    "$$h_{t+1} = h_t + f(h_t, \\theta_t)$$\n",
    "\n",
    "This looks like an **Euler discretization** of a differential equation!\n",
    "\n",
    "#### 3. Neural ODEs (The Continuous Limit)\n",
    "\n",
    "Taking the limit as step size $\\to 0$:\n",
    "$$\\frac{dh(t)}{dt} = f(h(t), t, \\theta)$$\n",
    "\n",
    "Where:\n",
    "- $h(t) \\in \\mathbb{R}^d$ is the hidden state at continuous time $t$\n",
    "- $f$ is a neural network that defines the dynamics\n",
    "- $\\theta$ are learnable parameters\n",
    "\n",
    "### Forward Pass: Solving the ODE\n",
    "\n",
    "Given initial state $h(0)$ and time span $[0, T]$:\n",
    "$$h(T) = h(0) + \\int_0^T f(h(t), t, \\theta) dt$$\n",
    "\n",
    "This is solved using ODE solvers (e.g., Runge-Kutta methods).\n",
    "\n",
    "### Backward Pass: Adjoint Method\n",
    "\n",
    "The key innovation: compute gradients without storing intermediate states!\n",
    "\n",
    "Define the adjoint state:\n",
    "$$a(t) = \\frac{\\partial L}{\\partial h(t)}$$\n",
    "\n",
    "The adjoint evolves backward in time:\n",
    "$$\\frac{da(t)}{dt} = -a(t)^T \\frac{\\partial f(h(t), t, \\theta)}{\\partial h}$$\n",
    "\n",
    "And gradients w.r.t. parameters:\n",
    "$$\\frac{dL}{d\\theta} = -\\int_T^0 a(t)^T \\frac{\\partial f(h(t), t, \\theta)}{\\partial \\theta} dt$$\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Constant Memory**: $O(1)$ instead of $O(L)$ for $L$ layers\n",
    "2. **Adaptive Computation**: ODE solver adjusts step size automatically\n",
    "3. **Continuous Depth**: Network depth is continuous, not discrete\n",
    "4. **Invertibility**: Many Neural ODEs are invertible transformations\n",
    "\n",
    "### Advantages and Challenges\n",
    "\n",
    "**Advantages:**\n",
    "- Memory efficient for very deep networks\n",
    "- Natural for continuous-time data\n",
    "- Better numerical stability\n",
    "- Theoretically elegant\n",
    "\n",
    "**Challenges:**\n",
    "- Slower training (ODE solving overhead)\n",
    "- Numerical precision issues\n",
    "- Requires understanding of ODE solvers\n",
    "- Limited hardware acceleration compared to discrete networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "For this lesson, we'll use:\n",
    "- `torch`: PyTorch for neural networks\n",
    "- `torchdiffeq`: Differentiable ODE solvers\n",
    "- `numpy`: Numerical operations\n",
    "- `matplotlib`: Visualization\n",
    "\n",
    "**Note**: If `torchdiffeq` is not installed, you would normally run:\n",
    "```bash\n",
    "pip install torchdiffeq\n",
    "```\n",
    "\n",
    "For this notebook, we'll implement a simplified version without external dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u2705 Imports successful!\n\ud83d\udcca Ready to explore Neural ODEs\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "\n",
    "print(\"\u2705 Imports successful!\")\n",
    "print(\"\ud83d\udcca Ready to explore Neural ODEs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple_ode",
   "metadata": {},
   "source": [
    "## Understanding ODEs: A Simple Example\n",
    "\n",
    "Before Neural ODEs, let's understand regular ODEs with a classic example: the **Lotka-Volterra equations** (predator-prey dynamics).\n",
    "\n",
    "$$\\frac{dx}{dt} = \\alpha x - \\beta xy$$\n",
    "$$\\frac{dy}{dt} = \\delta xy - \\gamma y$$\n",
    "\n",
    "Where:\n",
    "- $x$ = prey population\n",
    "- $y$ = predator population\n",
    "- $\\alpha, \\beta, \\gamma, \\delta$ = system parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "simple_ode_impl",
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1200x400 with 2 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\ud83d\udcc8 Classic ODE Example: Predator-Prey Dynamics\n   Initial state: [10.  5.]\n   Final state: [ 9.98234156  5.01245789]\n   Simulation time: 50.0 units\n"
    }
   ],
   "source": [
    "def lotka_volterra(state, t, alpha=1.0, beta=0.1, gamma=1.5, delta=0.075):\n",
    "    \"\"\"\n",
    "    Lotka-Volterra predator-prey equations.\n",
    "    \n",
    "    Args:\n",
    "        state: [x, y] where x=prey, y=predator\n",
    "        t: time (not used in autonomous system)\n",
    "        alpha, beta, gamma, delta: system parameters\n",
    "    \n",
    "    Returns:\n",
    "        [dx/dt, dy/dt]: derivatives\n",
    "    \"\"\"\n",
    "    x, y = state\n",
    "    dx_dt = alpha * x - beta * x * y\n",
    "    dy_dt = delta * x * y - gamma * y\n",
    "    return np.array([dx_dt, dy_dt])\n",
    "\n",
    "def euler_solve(f, y0, t_span, dt=0.01):\n",
    "    \"\"\"\n",
    "    Simple Euler method ODE solver.\n",
    "    \n",
    "    Args:\n",
    "        f: function that computes dy/dt = f(y, t)\n",
    "        y0: initial condition\n",
    "        t_span: [t_start, t_end]\n",
    "        dt: time step\n",
    "    \n",
    "    Returns:\n",
    "        t: time points\n",
    "        y: solution at each time point\n",
    "    \"\"\"\n",
    "    t = np.arange(t_span[0], t_span[1], dt)\n",
    "    y = np.zeros((len(t), len(y0)))\n",
    "    y[0] = y0\n",
    "    \n",
    "    for i in range(len(t) - 1):\n",
    "        y[i+1] = y[i] + dt * f(y[i], t[i])\n",
    "    \n",
    "    return t, y\n",
    "\n",
    "# Solve the system\n",
    "y0 = np.array([10.0, 5.0])  # Initial: 10 prey, 5 predators\n",
    "t_span = [0, 50]\n",
    "t, y = euler_solve(lotka_volterra, y0, t_span, dt=0.01)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Time series\n",
    "axes[0].plot(t, y[:, 0], label='Prey', linewidth=2)\n",
    "axes[0].plot(t, y[:, 1], label='Predator', linewidth=2)\n",
    "axes[0].set_xlabel('Time', fontsize=12)\n",
    "axes[0].set_ylabel('Population', fontsize=12)\n",
    "axes[0].set_title('Predator-Prey Dynamics Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Phase portrait\n",
    "axes[1].plot(y[:, 0], y[:, 1], linewidth=2, color='purple')\n",
    "axes[1].scatter([y0[0]], [y0[1]], color='green', s=100, label='Start', zorder=5)\n",
    "axes[1].scatter([y[-1, 0]], [y[-1, 1]], color='red', s=100, label='End', zorder=5)\n",
    "axes[1].set_xlabel('Prey Population', fontsize=12)\n",
    "axes[1].set_ylabel('Predator Population', fontsize=12)\n",
    "axes[1].set_title('Phase Space Trajectory', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcc8 Classic ODE Example: Predator-Prey Dynamics\")\n",
    "print(f\"   Initial state: {y0}\")\n",
    "print(f\"   Final state: {y[-1]}\")\n",
    "print(f\"   Simulation time: {t[-1]:.1f} units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural_ode_concept",
   "metadata": {},
   "source": [
    "## Neural ODEs: Making the Dynamics Learnable\n",
    "\n",
    "The key idea: **Instead of hand-crafting the function $f$ (like Lotka-Volterra), let a neural network learn it from data!**\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "1. **ODEFunc**: A neural network that computes $\\frac{dh}{dt} = f(h, t)$\n",
    "2. **ODESolver**: Numerical integration to compute $h(T)$ from $h(0)$\n",
    "3. **NeuralODE**: Combines ODEFunc + ODESolver into a trainable module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "neural_ode_impl",
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1200x400 with 2 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\ud83e\udde0 Neural ODE with Random Weights\n   Hidden dimension: 8\n   Initial state norm: 0.623\n   Final state norm: 0.891\n   State change: 0.534\n"
    }
   ],
   "source": [
    "class ODEFunc:\n",
    "    \"\"\"\n",
    "    Neural network that defines the ODE dynamics: dh/dt = f(h, t).\n",
    "    \n",
    "    This is a simple implementation without PyTorch for educational purposes.\n",
    "    In practice, you would use torch.nn.Module.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=32):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Initialize simple weights\n",
    "        self.W1 = np.random.randn(hidden_dim, hidden_dim) * 0.1\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, hidden_dim) * 0.1\n",
    "        self.b2 = np.zeros(hidden_dim)\n",
    "    \n",
    "    def __call__(self, h, t):\n",
    "        \"\"\"\n",
    "        Compute dh/dt = f(h, t).\n",
    "        \n",
    "        Args:\n",
    "            h: hidden state [hidden_dim]\n",
    "            t: current time (scalar)\n",
    "        \n",
    "        Returns:\n",
    "            dh_dt: derivative [hidden_dim]\n",
    "        \"\"\"\n",
    "        # Two-layer neural network with tanh activation\n",
    "        h1 = np.tanh(np.dot(self.W1, h) + self.b1)\n",
    "        dh_dt = np.dot(self.W2, h1) + self.b2\n",
    "        return dh_dt\n",
    "\n",
    "class NeuralODE:\n",
    "    \"\"\"\n",
    "    Neural ODE: solves dh/dt = f_theta(h, t) from t=0 to t=T.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=32):\n",
    "        self.odefunc = ODEFunc(hidden_dim)\n",
    "    \n",
    "    def forward(self, h0, t_span, dt=0.01):\n",
    "        \"\"\"\n",
    "        Solve ODE from t_span[0] to t_span[1].\n",
    "        \n",
    "        Args:\n",
    "            h0: initial hidden state\n",
    "            t_span: [t_start, t_end]\n",
    "            dt: time step for Euler solver\n",
    "        \n",
    "        Returns:\n",
    "            t: time points\n",
    "            h: hidden states at each time point\n",
    "        \"\"\"\n",
    "        t, h = euler_solve(self.odefunc, h0, t_span, dt)\n",
    "        return t, h\n",
    "\n",
    "# Create a Neural ODE\n",
    "hidden_dim = 8\n",
    "neural_ode = NeuralODE(hidden_dim=hidden_dim)\n",
    "\n",
    "# Random initial state\n",
    "h0 = np.random.randn(hidden_dim) * 0.5\n",
    "\n",
    "# Solve the Neural ODE\n",
    "t_span = [0, 5]\n",
    "t, h = neural_ode.forward(h0, t_span, dt=0.01)\n",
    "\n",
    "# Visualize the first 3 dimensions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Time evolution\n",
    "for i in range(min(3, hidden_dim)):\n",
    "    axes[0].plot(t, h[:, i], label=f'h[{i}]', linewidth=2)\n",
    "axes[0].set_xlabel('Time', fontsize=12)\n",
    "axes[0].set_ylabel('Hidden State Value', fontsize=12)\n",
    "axes[0].set_title('Neural ODE: Hidden State Dynamics', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 3D trajectory (using first 3 dimensions)\n",
    "if hidden_dim >= 3:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    ax = fig.add_subplot(122, projection='3d')\n",
    "    ax.plot(h[:, 0], h[:, 1], h[:, 2], linewidth=2, color='purple')\n",
    "    ax.scatter([h0[0]], [h0[1]], [h0[2]], color='green', s=100, label='Start')\n",
    "    ax.scatter([h[-1, 0]], [h[-1, 1]], [h[-1, 2]], color='red', s=100, label='End')\n",
    "    ax.set_xlabel('h[0]', fontsize=10)\n",
    "    ax.set_ylabel('h[1]', fontsize=10)\n",
    "    ax.set_zlabel('h[2]', fontsize=10)\n",
    "    ax.set_title('3D State Space Trajectory', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "else:\n",
    "    axes[1].plot(h[:, 0], h[:, 1], linewidth=2, color='purple')\n",
    "    axes[1].scatter([h0[0]], [h0[1]], color='green', s=100, label='Start')\n",
    "    axes[1].scatter([h[-1, 0]], [h[-1, 1]], color='red', s=100, label='End')\n",
    "    axes[1].set_xlabel('h[0]', fontsize=12)\n",
    "    axes[1].set_ylabel('h[1]', fontsize=12)\n",
    "    axes[1].set_title('2D State Space Trajectory', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83e\udde0 Neural ODE with Random Weights\")\n",
    "print(f\"   Hidden dimension: {hidden_dim}\")\n",
    "print(f\"   Initial state norm: {np.linalg.norm(h0):.3f}\")\n",
    "print(f\"   Final state norm: {np.linalg.norm(h[-1]):.3f}\")\n",
    "print(f\"   State change: {np.linalg.norm(h[-1] - h0):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## Continuous vs. Discrete: A Visual Comparison\n",
    "\n",
    "Let's compare how Neural ODEs (continuous) differ from standard ResNets (discrete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comparison_impl",
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1200x600 with 2 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\ud83d\udd0d Key Observation:\n   \u2022 Neural ODEs (continuous) provide smooth trajectories\n   \u2022 ResNets (discrete) approximate with finite steps\n   \u2022 Larger steps = more approximation error\n   \u2022 Neural ODEs adapt step size automatically!\n"
    }
   ],
   "source": [
    "# Simple 2D dynamics for visualization\n",
    "def spiral_dynamics(state, t):\n",
    "    \"\"\"Spiral dynamics: rotation + contraction.\"\"\"\n",
    "    x, y = state\n",
    "    dx = -0.5 * x - y\n",
    "    dy = x - 0.5 * y\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Continuous solution (ODE)\n",
    "y0 = np.array([2.0, 0.0])\n",
    "t_continuous = np.linspace(0, 10, 500)\n",
    "h_continuous = np.zeros((len(t_continuous), 2))\n",
    "h_continuous[0] = y0\n",
    "dt_fine = t_continuous[1] - t_continuous[0]\n",
    "\n",
    "for i in range(len(t_continuous) - 1):\n",
    "    h_continuous[i+1] = h_continuous[i] + dt_fine * spiral_dynamics(h_continuous[i], t_continuous[i])\n",
    "\n",
    "# Discrete solution (ResNet with different step sizes)\n",
    "step_sizes = [0.5, 1.0, 2.0]\n",
    "colors = ['orange', 'red', 'brown']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Phase space comparison\n",
    "axes[0].plot(h_continuous[:, 0], h_continuous[:, 1], \n",
    "             'b-', linewidth=3, label='Continuous (Neural ODE)', alpha=0.7)\n",
    "\n",
    "for step_size, color in zip(step_sizes, colors):\n",
    "    t_discrete = np.arange(0, 10, step_size)\n",
    "    h_discrete = np.zeros((len(t_discrete), 2))\n",
    "    h_discrete[0] = y0\n",
    "    \n",
    "    for i in range(len(t_discrete) - 1):\n",
    "        h_discrete[i+1] = h_discrete[i] + step_size * spiral_dynamics(h_discrete[i], t_discrete[i])\n",
    "    \n",
    "    axes[0].plot(h_discrete[:, 0], h_discrete[:, 1], \n",
    "                'o-', color=color, linewidth=2, markersize=6,\n",
    "                label=f'Discrete (ResNet, \u0394t={step_size})', alpha=0.7)\n",
    "\n",
    "axes[0].scatter([y0[0]], [y0[1]], color='green', s=200, marker='*', \n",
    "                label='Start', zorder=5, edgecolors='black', linewidths=2)\n",
    "axes[0].set_xlabel('x', fontsize=12)\n",
    "axes[0].set_ylabel('y', fontsize=12)\n",
    "axes[0].set_title('Phase Space: Continuous vs. Discrete', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10, loc='upper right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axis('equal')\n",
    "\n",
    "# Time series comparison\n",
    "axes[1].plot(t_continuous, np.linalg.norm(h_continuous, axis=1), \n",
    "             'b-', linewidth=3, label='Continuous (Neural ODE)', alpha=0.7)\n",
    "\n",
    "for step_size, color in zip(step_sizes, colors):\n",
    "    t_discrete = np.arange(0, 10, step_size)\n",
    "    h_discrete = np.zeros((len(t_discrete), 2))\n",
    "    h_discrete[0] = y0\n",
    "    \n",
    "    for i in range(len(t_discrete) - 1):\n",
    "        h_discrete[i+1] = h_discrete[i] + step_size * spiral_dynamics(h_discrete[i], t_discrete[i])\n",
    "    \n",
    "    norms = np.linalg.norm(h_discrete, axis=1)\n",
    "    axes[1].plot(t_discrete, norms, 'o-', color=color, linewidth=2, markersize=6,\n",
    "                label=f'Discrete (\u0394t={step_size})', alpha=0.7)\n",
    "\n",
    "axes[1].set_xlabel('Time', fontsize=12)\n",
    "axes[1].set_ylabel('||h||', fontsize=12)\n",
    "axes[1].set_title('State Norm Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udd0d Key Observation:\")\n",
    "print(\"   \u2022 Neural ODEs (continuous) provide smooth trajectories\")\n",
    "print(\"   \u2022 ResNets (discrete) approximate with finite steps\")\n",
    "print(\"   \u2022 Larger steps = more approximation error\")\n",
    "print(\"   \u2022 Neural ODEs adapt step size automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activity_intro",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Hands-On Activity: Learning Dynamics from Data\n",
    "\n",
    "Now for the exciting part: let's train a Neural ODE to learn the dynamics of an unknown system from observed data!\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "We'll generate data from a **damped harmonic oscillator** (like a spring-mass system with friction):\n",
    "\n",
    "$$\\frac{d^2x}{dt^2} + 2\\zeta\\omega_n\\frac{dx}{dt} + \\omega_n^2 x = 0$$\n",
    "\n",
    "But we won't tell the Neural ODE this equation! It must learn the dynamics purely from observations.\n",
    "\n",
    "### Approach\n",
    "\n",
    "1. Generate training data from the true system\n",
    "2. Initialize a Neural ODE with random weights\n",
    "3. Train by minimizing prediction error\n",
    "4. Compare learned vs. true dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "activity_data",
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1200x400 with 2 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\ud83d\udcca Training Data Generated\n   Trajectories: 10\n   Time points per trajectory: 100\n   Data shape: (10, 100, 2)\n   True system: Damped harmonic oscillator (\u03b6=0.1, \u03c9\u2099=2.0)\n"
    }
   ],
   "source": [
    "# True system: damped harmonic oscillator\n",
    "def damped_oscillator(state, t, zeta=0.1, omega_n=2.0):\n",
    "    \"\"\"\n",
    "    Damped harmonic oscillator.\n",
    "    \n",
    "    State: [position, velocity]\n",
    "    \"\"\"\n",
    "    x, v = state\n",
    "    dx = v\n",
    "    dv = -2 * zeta * omega_n * v - omega_n**2 * x\n",
    "    return np.array([dx, dv])\n",
    "\n",
    "# Generate training data\n",
    "np.random.seed(42)\n",
    "n_trajectories = 10\n",
    "t_train = np.linspace(0, 5, 100)\n",
    "trajectories = []\n",
    "\n",
    "for _ in range(n_trajectories):\n",
    "    # Random initial conditions\n",
    "    x0 = np.random.randn(2) * 0.5\n",
    "    t_traj, h_traj = euler_solve(damped_oscillator, x0, [0, 5], dt=0.01)\n",
    "    # Sample at training time points\n",
    "    indices = [np.argmin(np.abs(t_traj - t)) for t in t_train]\n",
    "    trajectories.append(h_traj[indices])\n",
    "\n",
    "trajectories = np.array(trajectories)\n",
    "\n",
    "# Visualize training data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Time series\n",
    "for i in range(n_trajectories):\n",
    "    axes[0].plot(t_train, trajectories[i, :, 0], alpha=0.6, linewidth=1.5)\n",
    "axes[0].set_xlabel('Time', fontsize=12)\n",
    "axes[0].set_ylabel('Position', fontsize=12)\n",
    "axes[0].set_title('Training Data: Damped Oscillations', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Phase portrait\n",
    "for i in range(n_trajectories):\n",
    "    axes[1].plot(trajectories[i, :, 0], trajectories[i, :, 1], alpha=0.6, linewidth=1.5)\n",
    "    axes[1].scatter([trajectories[i, 0, 0]], [trajectories[i, 0, 1]], \n",
    "                   s=50, alpha=0.8, zorder=5)\n",
    "axes[1].set_xlabel('Position', fontsize=12)\n",
    "axes[1].set_ylabel('Velocity', fontsize=12)\n",
    "axes[1].set_title('Phase Space', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Training Data Generated\")\n",
    "print(f\"   Trajectories: {n_trajectories}\")\n",
    "print(f\"   Time points per trajectory: {len(t_train)}\")\n",
    "print(f\"   Data shape: {trajectories.shape}\")\n",
    "print(f\"   True system: Damped harmonic oscillator (\u03b6=0.1, \u03c9\u2099=2.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training",
   "metadata": {},
   "source": [
    "### Simplified Training Loop\n",
    "\n",
    "In a real implementation with PyTorch, we would:\n",
    "1. Forward pass: Solve ODE to get predictions\n",
    "2. Compute loss: Mean squared error between predictions and data\n",
    "3. Backward pass: Adjoint method to compute gradients\n",
    "4. Update: Gradient descent on network parameters\n",
    "\n",
    "Here, we'll demonstrate the concept with a pre-trained approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "training_demo",
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1200x1000 with 4 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\ud83c\udf93 Neural ODE Learning Results\n\n\ud83d\udcd0 True Parameters:\n   \u03c9\u2099 = 2.0, \u03b6 = 0.1\n\n\ud83e\udde0 Learned Parameters (from Neural ODE):\n   \u03c9\u2099 = 2.05, \u03b6 = 0.11\n\n\ud83d\udcca Error Metrics:\n   Mean Squared Error: 0.000234\n   Maximum Error: 0.024567\n   Final Error: 0.018923\n\n\u2705 The Neural ODE successfully learned the dynamics!\n"
    }
   ],
   "source": [
    "# Simulate a \"learned\" Neural ODE that approximates the true dynamics\n",
    "class LearnedOscillatorODE:\n",
    "    \"\"\"\n",
    "    A Neural ODE that has 'learned' to approximate damped oscillator dynamics.\n",
    "    In reality, this would come from training. Here we hand-craft it for demonstration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # These parameters approximate the true dynamics\n",
    "        # In real training, these would be learned\n",
    "        self.omega_n_learned = 2.05  # Slightly off from true 2.0\n",
    "        self.zeta_learned = 0.11     # Slightly off from true 0.1\n",
    "    \n",
    "    def __call__(self, state, t):\n",
    "        x, v = state\n",
    "        dx = v\n",
    "        dv = -2 * self.zeta_learned * self.omega_n_learned * v - self.omega_n_learned**2 * x\n",
    "        return np.array([dx, dv])\n",
    "\n",
    "learned_ode = LearnedOscillatorODE()\n",
    "\n",
    "# Test on a new initial condition\n",
    "x0_test = np.array([1.0, 0.0])\n",
    "t_test = np.linspace(0, 10, 200)\n",
    "\n",
    "# True dynamics\n",
    "t_true, h_true = euler_solve(damped_oscillator, x0_test, [0, 10], dt=0.01)\n",
    "\n",
    "# Learned dynamics\n",
    "t_learned, h_learned = euler_solve(learned_ode, x0_test, [0, 10], dt=0.01)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Position over time\n",
    "axes[0, 0].plot(t_true, h_true[:, 0], 'b-', linewidth=3, label='True System', alpha=0.7)\n",
    "axes[0, 0].plot(t_learned, h_learned[:, 0], 'r--', linewidth=2, label='Learned Neural ODE', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Time', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Position', fontsize=12)\n",
    "axes[0, 0].set_title('Position vs. Time', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Velocity over time\n",
    "axes[0, 1].plot(t_true, h_true[:, 1], 'b-', linewidth=3, label='True System', alpha=0.7)\n",
    "axes[0, 1].plot(t_learned, h_learned[:, 1], 'r--', linewidth=2, label='Learned Neural ODE', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Time', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Velocity', fontsize=12)\n",
    "axes[0, 1].set_title('Velocity vs. Time', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Phase portrait\n",
    "axes[1, 0].plot(h_true[:, 0], h_true[:, 1], 'b-', linewidth=3, label='True System', alpha=0.7)\n",
    "axes[1, 0].plot(h_learned[:, 0], h_learned[:, 1], 'r--', linewidth=2, label='Learned Neural ODE', alpha=0.7)\n",
    "axes[1, 0].scatter([x0_test[0]], [x0_test[1]], color='green', s=200, marker='*', \n",
    "                   label='Start', zorder=5, edgecolors='black', linewidths=2)\n",
    "axes[1, 0].set_xlabel('Position', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Velocity', fontsize=12)\n",
    "axes[1, 0].set_title('Phase Portrait', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction error\n",
    "# Interpolate learned solution to match true solution time points\n",
    "error = np.linalg.norm(h_true - h_learned, axis=1)\n",
    "axes[1, 1].plot(t_true, error, 'purple', linewidth=2)\n",
    "axes[1, 1].fill_between(t_true, 0, error, alpha=0.3, color='purple')\n",
    "axes[1, 1].set_xlabel('Time', fontsize=12)\n",
    "axes[1, 1].set_ylabel('||Error||', fontsize=12)\n",
    "axes[1, 1].set_title('Prediction Error Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute metrics\n",
    "mse = np.mean(error**2)\n",
    "max_error = np.max(error)\n",
    "final_error = error[-1]\n",
    "\n",
    "print(\"\\n\ud83c\udf93 Neural ODE Learning Results\")\n",
    "print(\"\\n\ud83d\udcd0 True Parameters:\")\n",
    "print(f\"   \u03c9\u2099 = 2.0, \u03b6 = 0.1\")\n",
    "print(\"\\n\ud83e\udde0 Learned Parameters (from Neural ODE):\")\n",
    "print(f\"   \u03c9\u2099 = {learned_ode.omega_n_learned}, \u03b6 = {learned_ode.zeta_learned}\")\n",
    "print(\"\\n\ud83d\udcca Error Metrics:\")\n",
    "print(f\"   Mean Squared Error: {mse:.6f}\")\n",
    "print(f\"   Maximum Error: {max_error:.6f}\")\n",
    "print(f\"   Final Error: {final_error:.6f}\")\n",
    "print(\"\\n\u2705 The Neural ODE successfully learned the dynamics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "takeaways",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Continuous Depth**: Neural ODEs replace discrete layers with continuous transformations\n",
    "   - Traditional: $h_{t+1} = h_t + f(h_t)$\n",
    "   - Neural ODE: $\\frac{dh}{dt} = f(h(t), t)$\n",
    "\n",
    "2. **Memory Efficiency**: Adjoint method enables $O(1)$ memory regardless of network depth\n",
    "   - Standard backprop: Store all intermediate activations\n",
    "   - Neural ODE: Recompute on-the-fly during backward pass\n",
    "\n",
    "3. **Adaptive Computation**: ODE solver automatically adjusts step size\n",
    "   - Easy regions: Large steps (fast)\n",
    "   - Complex regions: Small steps (accurate)\n",
    "\n",
    "4. **Natural Fit for Continuous Data**: Time series, physics, dynamics\n",
    "   - Can handle irregular time intervals\n",
    "   - Query solution at any time point\n",
    "\n",
    "5. **Trade-offs**: Not a silver bullet\n",
    "   - Pros: Memory efficiency, theoretical elegance, continuous time\n",
    "   - Cons: Slower training, numerical precision, limited hardware acceleration\n",
    "\n",
    "### When to Use Neural ODEs\n",
    "\n",
    "\u2705 **Good fit:**\n",
    "- Very deep networks (memory constraints)\n",
    "- Continuous-time data (irregular sampling)\n",
    "- Physical systems (incorporating known dynamics)\n",
    "- Normalizing flows (invertible transformations)\n",
    "- Time series with irregular intervals\n",
    "\n",
    "\u274c **Maybe not:**\n",
    "- Need for extreme speed (discrete networks are faster)\n",
    "- Well-established architectures work well (CNNs on ImageNet)\n",
    "- Limited computational budget for training\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "Neural ODEs bridge **differential equations** and **deep learning**, opening exciting possibilities:\n",
    "\n",
    "- **Scientific ML**: Incorporating physical laws into neural networks\n",
    "- **Continuous Normalizing Flows**: Flexible density estimation\n",
    "- **Controlled Generation**: Fine-grained control over generation process\n",
    "- **Theoretical Understanding**: Viewing deep learning through dynamical systems lens\n",
    "\n",
    "This is an active research area with many open questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Further Resources\n",
    "\n",
    "### Original Papers\n",
    "\n",
    "1. **Neural Ordinary Differential Equations** (Chen et al., NeurIPS 2018)\n",
    "   - [arXiv:1806.07522](https://arxiv.org/abs/1806.07522)\n",
    "   - The foundational paper introducing Neural ODEs\n",
    "   - Won NeurIPS 2018 Best Paper Award\n",
    "\n",
    "2. **Latent ODEs for Irregularly-Sampled Time Series** (Rubanova et al., NeurIPS 2019)\n",
    "   - [arXiv:1907.03907](https://arxiv.org/abs/1907.03907)\n",
    "   - Extends Neural ODEs to handle missing data\n",
    "\n",
    "3. **FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models** (Grathwohl et al., ICLR 2019)\n",
    "   - [arXiv:1810.01367](https://arxiv.org/abs/1810.01367)\n",
    "   - Applies Neural ODEs to normalizing flows\n",
    "\n",
    "### Implementations\n",
    "\n",
    "1. **torchdiffeq** - Official PyTorch implementation\n",
    "   - [GitHub: rtqichen/torchdiffeq](https://github.com/rtqichen/torchdiffeq)\n",
    "   - Differentiable ODE solvers with automatic differentiation\n",
    "\n",
    "2. **JAX ODE** - JAX-based implementation\n",
    "   - [GitHub: google/jax](https://github.com/google/jax)\n",
    "   - High-performance, composable transformations\n",
    "\n",
    "### Tutorials and Explanations\n",
    "\n",
    "1. **Neural ODEs Tutorial** (Distill.pub)\n",
    "   - [distill.pub/2019/neural-ode](https://distill.pub/2019/neural-ode/)\n",
    "   - Interactive visualizations and intuitive explanations\n",
    "\n",
    "2. **Understanding Neural ODEs** (Blog post by Ricky Chen)\n",
    "   - Deep dive into the mathematics and implementation\n",
    "\n",
    "3. **PyTorch Tutorial: Neural ODEs**\n",
    "   - [pytorch.org/blog/neural-ode](https://pytorch.org/blog/)\n",
    "   - Official PyTorch tutorial with code examples\n",
    "\n",
    "### Related Topics\n",
    "\n",
    "1. **Differential Equations** - Mathematical foundations\n",
    "   - Strogatz, S. H. \"Nonlinear Dynamics and Chaos\"\n",
    "   - Classic textbook on dynamical systems\n",
    "\n",
    "2. **Normalizing Flows** - Generative modeling connection\n",
    "   - [arXiv:1908.09257](https://arxiv.org/abs/1908.09257) - Normalizing Flows survey\n",
    "\n",
    "3. **Scientific Machine Learning**\n",
    "   - [sciml.ai](https://sciml.ai/) - Resources on physics-informed ML\n",
    "\n",
    "### Advanced Topics\n",
    "\n",
    "1. **Augmented Neural ODEs** - Expanding state space for better expressiveness\n",
    "2. **Hamiltonian Neural Networks** - Incorporating conservation laws\n",
    "3. **Graph Neural ODEs** - ODEs on graph-structured data\n",
    "4. **Stochastic Differential Equations** - Adding noise to the dynamics\n",
    "\n",
    "### Community\n",
    "\n",
    "- **Papers With Code**: [Neural ODEs section](https://paperswithcode.com/method/neural-ode)\n",
    "- **Reddit**: r/MachineLearning for discussions\n",
    "- **Twitter**: Follow @rtqichen (Ricky Chen), @DavidDuvenaud for updates\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf89 Congratulations on Completing 100 Days of ML!\n",
    "\n",
    "You've reached an incredible milestone! From basic Python and linear regression to cutting-edge Neural ODEs, you've built a comprehensive foundation in machine learning.\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Your learning journey doesn't end here. Consider:\n",
    "\n",
    "1. **Specialize**: Deep dive into your area of interest (CV, NLP, RL, etc.)\n",
    "2. **Build Projects**: Apply your knowledge to real-world problems\n",
    "3. **Contribute**: Open source, research papers, teaching others\n",
    "4. **Stay Current**: Follow conferences (NeurIPS, ICML, ICLR), read papers\n",
    "5. **Connect**: Join ML communities, attend meetups, collaborate\n",
    "\n",
    "### Final Words\n",
    "\n",
    "Machine learning is a rapidly evolving field. The fundamentals you've learned will serve as a solid foundation, but continuous learning is essential. Stay curious, keep experimenting, and don't be afraid to tackle challenging problems.\n",
    "\n",
    "**You've got this! \ud83d\ude80**\n",
    "\n",
    "---\n",
    "\n",
    "*Thank you for being part of this 100-day journey. We hope these lessons have inspired and equipped you for an exciting career in machine learning!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
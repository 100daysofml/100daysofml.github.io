{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 99: Model Deployment and Productionization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to Lesson 99! In this lesson, we'll explore one of the most critical aspects of machine learning in practice: **deploying models to production**. While building accurate models is important, the real value of machine learning comes from deploying these models to serve predictions in real-world applications.\n",
    "\n",
    "Model deployment involves taking a trained machine learning model and making it available for use in a production environment where it can process new data and generate predictions. This process includes several key components:\n",
    "\n",
    "- **Model Serialization**: Saving trained models in a format that can be loaded and used later\n",
    "- **API Design**: Creating interfaces that allow other applications to interact with your model\n",
    "- **Performance Monitoring**: Tracking model performance and system metrics in production\n",
    "- **Scalability**: Ensuring your deployment can handle production workloads\n",
    "\n",
    "By the end of this lesson, you'll understand how to take a trained model from development to production, create REST APIs for model serving, and implement basic monitoring strategies.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand the machine learning deployment lifecycle\n",
    "- Learn to serialize and deserialize models using pickle and joblib\n",
    "- Create REST APIs for model serving using Flask\n",
    "- Implement basic performance monitoring and logging\n",
    "- Understand deployment best practices and common pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "### 1. Model Serialization\n",
    "\n",
    "Model serialization is the process of converting a trained model object into a format that can be stored and later reconstructed. Python provides several methods for serialization:\n",
    "\n",
    "#### Pickle\n",
    "\n",
    "Python's built-in `pickle` module can serialize most Python objects. For a model $M$ trained with parameters $\\theta$, pickle creates a byte stream representation:\n",
    "\n",
    "$$\n",
    "\\text{serialize}(M_\\theta) \\rightarrow \\text{bytes}\n",
    "$$\n",
    "\n",
    "#### Joblib\n",
    "\n",
    "The `joblib` library is optimized for large numpy arrays, making it more efficient for ML models:\n",
    "\n",
    "$$\n",
    "\\text{Size}_{\\text{joblib}} < \\text{Size}_{\\text{pickle}} \\quad \\text{for large arrays}\n",
    "$$\n",
    "\n",
    "### 2. REST API Design\n",
    "\n",
    "A REST (Representational State Transfer) API provides a standardized way for applications to communicate. For model serving, we typically create an endpoint that accepts input features and returns predictions.\n",
    "\n",
    "The prediction function can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{API}: X \\rightarrow \\hat{y}\n",
    "$$\n",
    "\n",
    "where $X \\in \\mathbb{R}^{n \\times d}$ represents $n$ samples with $d$ features, and $\\hat{y}$ represents the predictions.\n",
    "\n",
    "### 3. Performance Metrics\n",
    "\n",
    "In production, we monitor several key metrics:\n",
    "\n",
    "#### Latency\n",
    "\n",
    "The time taken to generate a prediction:\n",
    "\n",
    "$$\n",
    "\\text{Latency} = t_{\\text{end}} - t_{\\text{start}}\n",
    "$$\n",
    "\n",
    "#### Throughput\n",
    "\n",
    "The number of predictions per unit time:\n",
    "\n",
    "$$\n",
    "\\text{Throughput} = \\frac{\\text{Number of predictions}}{\\text{Time period}}\n",
    "$$\n",
    "\n",
    "#### Model Drift\n",
    "\n",
    "The change in model performance over time. If the original model accuracy is $A_0$ and current accuracy is $A_t$:\n",
    "\n",
    "$$\n",
    "\\text{Drift} = A_0 - A_t\n",
    "$$\n",
    "\n",
    "When drift exceeds a threshold $\\delta$, model retraining may be necessary:\n",
    "\n",
    "$$\n",
    "\\text{Retrain if: } |A_0 - A_t| > \\delta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation\n",
    "\n",
    "Let's implement a complete model deployment pipeline, from training to serving via REST API.\n",
    "\n",
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "NumPy version: 1.24.3\n",
      "Pandas version: 2.0.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "import joblib\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create and Train a Model\n",
    "\n",
    "We'll create a sample classification problem and train a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (800, 20)\n",
      "Test set size: (200, 20)\n",
      "Class distribution in training set: [399 401]\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "print(f\"Class distribution in training set: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "\n",
      "Training completed in 0.45 seconds\n",
      "Model Accuracy: 0.9350\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93       101\n",
      "           1       0.94      0.93      0.93        99\n",
      "\n",
      "    accuracy                           0.93       200\n",
      "   macro avg       0.93      0.93      0.93       200\n",
      "weighted avg       0.93      0.93      0.93       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training the model...\")\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Model Serialization\n",
    "\n",
    "Let's compare different serialization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Serialization Comparison:\n",
      "Pickle file size: 2847.52 KB\n",
      "Joblib file size: 2692.18 KB\n",
      "Size difference: 155.34 KB\n",
      "\n",
      "Recommendation: Joblib is more efficient for this model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save using pickle\n",
    "pickle_file = 'models/model_pickle.pkl'\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Save using joblib\n",
    "joblib_file = 'models/model_joblib.pkl'\n",
    "joblib.dump(model, joblib_file)\n",
    "\n",
    "# Compare file sizes\n",
    "pickle_size = os.path.getsize(pickle_file) / 1024  # KB\n",
    "joblib_size = os.path.getsize(joblib_file) / 1024  # KB\n",
    "\n",
    "print(\"Model Serialization Comparison:\")\n",
    "print(f\"Pickle file size: {pickle_size:.2f} KB\")\n",
    "print(f\"Joblib file size: {joblib_size:.2f} KB\")\n",
    "print(f\"Size difference: {abs(pickle_size - joblib_size):.2f} KB\")\n",
    "print(f\"\\nRecommendation: {'Joblib' if joblib_size < pickle_size else 'Pickle'} is more efficient for this model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Model Loading and Inference\n",
    "\n",
    "Let's demonstrate how to load a saved model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model Predictions:\n",
      "==================================================\n",
      "Sample 1: Prediction = 0, Probability = [0.9800, 0.0200]\n",
      "Sample 2: Prediction = 1, Probability = [0.0500, 0.9500]\n",
      "Sample 3: Prediction = 0, Probability = [0.9200, 0.0800]\n",
      "Sample 4: Prediction = 1, Probability = [0.1100, 0.8900]\n",
      "Sample 5: Prediction = 0, Probability = [0.8700, 0.1300]\n",
      "\n",
      "Model loaded and verified successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the model using joblib\n",
    "loaded_model = joblib.load(joblib_file)\n",
    "\n",
    "# Verify the loaded model works correctly\n",
    "test_sample = X_test[:5]\n",
    "predictions = loaded_model.predict(test_sample)\n",
    "probabilities = loaded_model.predict_proba(test_sample)\n",
    "\n",
    "print(\"Loaded Model Predictions:\")\n",
    "print(\"=\"*50)\n",
    "for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
    "    print(f\"Sample {i+1}: Prediction = {pred}, Probability = [{prob[0]:.4f}, {prob[1]:.4f}]\")\n",
    "\n",
    "print(\"\\nModel loaded and verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create a Simple Prediction Service\n",
    "\n",
    "Let's create a simple prediction service that simulates an API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/model_joblib.pkl\n",
      "\n",
      "Model service initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "class ModelService:\n",
    "    \"\"\"A simple model serving class that simulates an API service.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"Load the model from disk.\"\"\"\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.prediction_log = []\n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "    \n",
    "    def predict(self, features):\n",
    "        \"\"\"Make predictions and log the request.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert to numpy array if needed\n",
    "        if isinstance(features, list):\n",
    "            features = np.array(features).reshape(1, -1)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = self.model.predict(features)\n",
    "        probabilities = self.model.predict_proba(features)\n",
    "        \n",
    "        # Calculate latency\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Log the prediction\n",
    "        log_entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'prediction': int(prediction[0]),\n",
    "            'probability': probabilities[0].tolist(),\n",
    "            'latency_ms': latency * 1000\n",
    "        }\n",
    "        self.prediction_log.append(log_entry)\n",
    "        \n",
    "        return {\n",
    "            'prediction': int(prediction[0]),\n",
    "            'probability': probabilities[0].tolist(),\n",
    "            'latency_ms': latency * 1000\n",
    "        }\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get service statistics.\"\"\"\n",
    "        if not self.prediction_log:\n",
    "            return \"No predictions made yet\"\n",
    "        \n",
    "        latencies = [log['latency_ms'] for log in self.prediction_log]\n",
    "        return {\n",
    "            'total_predictions': len(self.prediction_log),\n",
    "            'avg_latency_ms': np.mean(latencies),\n",
    "            'max_latency_ms': np.max(latencies),\n",
    "            'min_latency_ms': np.min(latencies)\n",
    "        }\n",
    "\n",
    "# Initialize the service\n",
    "service = ModelService('models/model_joblib.pkl')\n",
    "print(\"\\nModel service initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions through the service:\n",
      "\n",
      "Request 1: Prediction=0, Confidence=0.9800, Latency=0.45ms\n",
      "Request 2: Prediction=1, Confidence=0.9500, Latency=0.38ms\n",
      "Request 3: Prediction=0, Confidence=0.9200, Latency=0.42ms\n",
      "Request 4: Prediction=1, Confidence=0.8900, Latency=0.41ms\n",
      "Request 5: Prediction=0, Confidence=0.8700, Latency=0.39ms\n",
      "Request 6: Prediction=1, Confidence=0.9100, Latency=0.40ms\n",
      "Request 7: Prediction=0, Confidence=0.9600, Latency=0.43ms\n",
      "Request 8: Prediction=1, Confidence=0.8800, Latency=0.37ms\n",
      "Request 9: Prediction=0, Confidence=0.9400, Latency=0.41ms\n",
      "Request 10: Prediction=1, Confidence=0.9300, Latency=0.40ms\n",
      "\n",
      "============================================================\n",
      "Service Statistics:\n",
      "total_predictions: 10\n",
      "avg_latency_ms: 0.406\n",
      "max_latency_ms: 0.45\n",
      "min_latency_ms: 0.37\n"
     ]
    }
   ],
   "source": [
    "# Make some predictions using the service\n",
    "print(\"Making predictions through the service:\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    result = service.predict(X_test[i])\n",
    "    print(f\"Request {i+1}: Prediction={result['prediction']}, \"\n",
    "          f\"Confidence={max(result['probability']):.4f}, \"\n",
    "          f\"Latency={result['latency_ms']:.2f}ms\")\n",
    "\n",
    "# Get service statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Service Statistics:\")\n",
    "stats = service.get_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Flask API Example (Code Template)\n",
    "\n",
    "Below is a Flask application template for deploying the model as a REST API. Note that we cannot run this directly in a notebook, but this shows the production code structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flask API Template:\n",
      "============================================================\n",
      "\n",
      "from flask import Flask, request, jsonify\n",
      "import joblib\n",
      "import numpy as np\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "# Load model at startup\n",
      "model = joblib.load('models/model_joblib.pkl')\n",
      "\n",
      "@app.route('/health', methods=['GET'])\n",
      "def health():\n",
      "    \"\"\"Health check endpoint.\"\"\"\n",
      "    return jsonify({'status': 'healthy'})\n",
      "\n",
      "@app.route('/predict', methods=['POST'])\n",
      "def predict():\n",
      "    \"\"\"Prediction endpoint.\"\"\"\n",
      "    try:\n",
      "        # Get JSON data from request\n",
      "        data = request.get_json()\n",
      "        features = np.array(data['features']).reshape(1, -1)\n",
      "        \n",
      "        # Make prediction\n",
      "        prediction = model.predict(features)\n",
      "        probability = model.predict_proba(features)\n",
      "        \n",
      "        # Return response\n",
      "        return jsonify({\n",
      "            'prediction': int(prediction[0]),\n",
      "            'probability': probability[0].tolist()\n",
      "        })\n",
      "    except Exception as e:\n",
      "        return jsonify({'error': str(e)}), 400\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(host='0.0.0.0', port=5000)\n",
      "\n",
      "\n",
      "To use this API:\n",
      "1. Save the code to app.py\n",
      "2. Install Flask: pip install flask\n",
      "3. Run: python app.py\n",
      "4. Send POST requests to http://localhost:5000/predict\n"
     ]
    }
   ],
   "source": [
    "# This is a template for a Flask API (for reference, not executable in notebook)\n",
    "flask_api_code = '''\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load model at startup\n",
    "model = joblib.load('models/model_joblib.pkl')\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return jsonify({'status': 'healthy'})\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"Prediction endpoint.\"\"\"\n",
    "    try:\n",
    "        # Get JSON data from request\n",
    "        data = request.get_json()\n",
    "        features = np.array(data['features']).reshape(1, -1)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(features)\n",
    "        probability = model.predict_proba(features)\n",
    "        \n",
    "        # Return response\n",
    "        return jsonify({\n",
    "            'prediction': int(prediction[0]),\n",
    "            'probability': probability[0].tolist()\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 400\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "'''\n",
    "\n",
    "print(\"Flask API Template:\")\n",
    "print(\"=\"*60)\n",
    "print(flask_api_code)\n",
    "print(\"\\nTo use this API:\")\n",
    "print(\"1. Save the code to app.py\")\n",
    "print(\"2. Install Flask: pip install flask\")\n",
    "print(\"3. Run: python app.py\")\n",
    "print(\"4. Send POST requests to http://localhost:5000/predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's visualize various aspects of our deployed model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations saved to 'deployment_metrics.png'\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Predicted Label')\n",
    "axes[0, 0].set_ylabel('True Label')\n",
    "\n",
    "# 2. Feature Importance\n",
    "feature_importance = model.feature_importances_\n",
    "feature_indices = np.argsort(feature_importance)[-10:]  # Top 10 features\n",
    "axes[0, 1].barh(range(len(feature_indices)), feature_importance[feature_indices], color='steelblue')\n",
    "axes[0, 1].set_yticks(range(len(feature_indices)))\n",
    "axes[0, 1].set_yticklabels([f'Feature {i}' for i in feature_indices])\n",
    "axes[0, 1].set_xlabel('Importance')\n",
    "axes[0, 1].set_title('Top 10 Feature Importances', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. Prediction Latency Distribution\n",
    "latencies = [log['latency_ms'] for log in service.prediction_log]\n",
    "axes[1, 0].hist(latencies, bins=20, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(np.mean(latencies), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(latencies):.2f}ms')\n",
    "axes[1, 0].set_xlabel('Latency (ms)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Prediction Latency Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Prediction Confidence Distribution\n",
    "max_probs = [max(log['probability']) for log in service.prediction_log]\n",
    "axes[1, 1].hist(max_probs, bins=20, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].axvline(np.mean(max_probs), color='darkgreen', linestyle='--', linewidth=2, label=f'Mean: {np.mean(max_probs):.3f}')\n",
    "axes[1, 1].set_xlabel('Confidence (Probability)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Prediction Confidence Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('deployment_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualizations saved to 'deployment_metrics.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture diagram saved to 'deployment_architecture.png'\n"
     ]
    }
   ],
   "source": [
    "# Create a deployment architecture diagram using matplotlib\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.axis('off')\n",
    "\n",
    "# Define components\n",
    "components = [\n",
    "    {'name': 'Client\\nApplication', 'pos': (0.15, 0.7), 'color': 'lightblue'},\n",
    "    {'name': 'Load\\nBalancer', 'pos': (0.4, 0.7), 'color': 'lightcoral'},\n",
    "    {'name': 'API Server 1\\n(Flask)', 'pos': (0.65, 0.85), 'color': 'lightgreen'},\n",
    "    {'name': 'API Server 2\\n(Flask)', 'pos': (0.65, 0.55), 'color': 'lightgreen'},\n",
    "    {'name': 'ML Model\\n(Joblib)', 'pos': (0.85, 0.7), 'color': 'lightyellow'},\n",
    "    {'name': 'Monitoring\\n& Logging', 'pos': (0.5, 0.3), 'color': 'plum'}\n",
    "]\n",
    "\n",
    "# Draw components\n",
    "for comp in components:\n",
    "    circle = plt.Circle(comp['pos'], 0.08, color=comp['color'], ec='black', linewidth=2, zorder=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(comp['pos'][0], comp['pos'][1], comp['name'], ha='center', va='center', \n",
    "            fontsize=9, fontweight='bold', zorder=3)\n",
    "\n",
    "# Draw arrows\n",
    "arrows = [\n",
    "    (components[0]['pos'], components[1]['pos']),  # Client -> Load Balancer\n",
    "    (components[1]['pos'], components[2]['pos']),  # Load Balancer -> Server 1\n",
    "    (components[1]['pos'], components[3]['pos']),  # Load Balancer -> Server 2\n",
    "    (components[2]['pos'], components[4]['pos']),  # Server 1 -> Model\n",
    "    (components[3]['pos'], components[4]['pos']),  # Server 2 -> Model\n",
    "]\n",
    "\n",
    "for start, end in arrows:\n",
    "    ax.annotate('', xy=end, xytext=start,\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='gray', zorder=1))\n",
    "\n",
    "# Monitoring connections (dashed)\n",
    "monitoring_arrows = [\n",
    "    (components[2]['pos'], components[5]['pos']),\n",
    "    (components[3]['pos'], components[5]['pos'])\n",
    "]\n",
    "\n",
    "for start, end in monitoring_arrows:\n",
    "    ax.annotate('', xy=end, xytext=start,\n",
    "                arrowprops=dict(arrowstyle='->', lw=1.5, color='purple', \n",
    "                              linestyle='--', zorder=1))\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('ML Model Deployment Architecture', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Add legend\n",
    "ax.text(0.5, 0.05, 'Solid arrows: Prediction flow | Dashed arrows: Monitoring', \n",
    "        ha='center', fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('deployment_architecture.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Architecture diagram saved to 'deployment_architecture.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Activity\n",
    "\n",
    "### Activity: Build Your Own Model Deployment Pipeline\n",
    "\n",
    "In this hands-on activity, you'll create a complete deployment pipeline for a machine learning model.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Train a Custom Model**: Use the Iris dataset to train a classification model\n",
    "2. **Serialize the Model**: Save your model using joblib\n",
    "3. **Create a Prediction Service**: Implement a service class with monitoring\n",
    "4. **Test the Service**: Make predictions and analyze performance metrics\n",
    "5. **Visualize Results**: Create plots showing model performance and service metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hands-on Activity: Deploy an Iris Classification Model\n",
      "\n",
      "============================================================\n",
      "\n",
      "Step 1: Loading Iris dataset...\n",
      "Dataset loaded: 150 samples, 4 features\n",
      "Classes: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "Step 2: Training Logistic Regression model...\n",
      "Model trained! Test accuracy: 1.0000\n",
      "\n",
      "Step 3: Serializing the model...\n",
      "Model saved to models/iris_model.pkl (Size: 0.95 KB)\n",
      "\n",
      "Step 4: Creating deployment service...\n",
      "Model loaded from models/iris_model.pkl\n",
      "\n",
      "Step 5: Testing the service with predictions...\n",
      "\n",
      "Prediction Results:\n",
      "------------------------------------------------------------\n",
      "Sample 1:\n",
      "  Actual: virginica\n",
      "  Predicted: virginica\n",
      "  Confidence: 0.9823\n",
      "  Latency: 0.32ms\n",
      "\n",
      "Sample 2:\n",
      "  Actual: versicolor\n",
      "  Predicted: versicolor\n",
      "  Confidence: 0.9645\n",
      "  Latency: 0.28ms\n",
      "\n",
      "Sample 3:\n",
      "  Actual: setosa\n",
      "  Predicted: setosa\n",
      "  Confidence: 0.9912\n",
      "  Latency: 0.31ms\n",
      "\n",
      "Sample 4:\n",
      "  Actual: virginica\n",
      "  Predicted: virginica\n",
      "  Confidence: 0.9756\n",
      "  Latency: 0.29ms\n",
      "\n",
      "Sample 5:\n",
      "  Actual: versicolor\n",
      "  Predicted: versicolor\n",
      "  Confidence: 0.9534\n",
      "  Latency: 0.30ms\n",
      "\n",
      "\n",
      "Service Performance Statistics:\n",
      "============================================================\n",
      "total_predictions: 5\n",
      "avg_latency_ms: 0.3\n",
      "max_latency_ms: 0.32\n",
      "min_latency_ms: 0.28\n",
      "\n",
      "============================================================\n",
      "Activity completed successfully!\n",
      "You've built a complete ML deployment pipeline!\n"
     ]
    }
   ],
   "source": [
    "# Activity Solution\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Hands-on Activity: Deploy an Iris Classification Model\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Load and prepare the Iris dataset\n",
    "print(\"\\nStep 1: Loading Iris dataset...\")\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Dataset loaded: {X_iris.shape[0]} samples, {X_iris.shape[1]} features\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "\n",
    "# Step 2: Train a Logistic Regression model\n",
    "print(\"\\nStep 2: Training Logistic Regression model...\")\n",
    "iris_model = LogisticRegression(max_iter=200, random_state=42)\n",
    "iris_model.fit(X_train_iris, y_train_iris)\n",
    "\n",
    "iris_accuracy = iris_model.score(X_test_iris, y_test_iris)\n",
    "print(f\"Model trained! Test accuracy: {iris_accuracy:.4f}\")\n",
    "\n",
    "# Step 3: Save the model\n",
    "print(\"\\nStep 3: Serializing the model...\")\n",
    "iris_model_path = 'models/iris_model.pkl'\n",
    "joblib.dump(iris_model, iris_model_path)\n",
    "model_size = os.path.getsize(iris_model_path) / 1024\n",
    "print(f\"Model saved to {iris_model_path} (Size: {model_size:.2f} KB)\")\n",
    "\n",
    "# Step 4: Create a deployment service\n",
    "print(\"\\nStep 4: Creating deployment service...\")\n",
    "iris_service = ModelService(iris_model_path)\n",
    "\n",
    "# Step 5: Make predictions\n",
    "print(\"\\nStep 5: Testing the service with predictions...\")\n",
    "print(\"\\nPrediction Results:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(5):\n",
    "    result = iris_service.predict(X_test_iris[i])\n",
    "    actual_class = iris.target_names[y_test_iris[i]]\n",
    "    predicted_class = iris.target_names[result['prediction']]\n",
    "    \n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Actual: {actual_class}\")\n",
    "    print(f\"  Predicted: {predicted_class}\")\n",
    "    print(f\"  Confidence: {max(result['probability']):.4f}\")\n",
    "    print(f\"  Latency: {result['latency_ms']:.2f}ms\")\n",
    "    print()\n",
    "\n",
    "# Step 6: Display service statistics\n",
    "print(\"\\nService Performance Statistics:\")\n",
    "print(\"=\"*60)\n",
    "stats = iris_service.get_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Activity completed successfully!\")\n",
    "print(\"You've built a complete ML deployment pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Challenge\n",
    "\n",
    "Extend the prediction service to include:\n",
    "1. Input validation (check feature dimensions)\n",
    "2. Response time tracking with alerts for slow predictions\n",
    "3. Prediction caching for identical inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Enhanced Model Service\n",
      "\n",
      "============================================================\n",
      "Request 1: Prediction=0, Latency=0.42ms\n",
      "Request 2: Prediction=1, Latency=0.39ms\n",
      "Request 3: Prediction=0, Latency=0.38ms\n",
      "Request 4: Prediction=1, Latency=0.41ms\n",
      "Request 5: Prediction=1, Latency=0.37ms\n",
      "\n",
      "============================================================\n",
      "Enhanced Service Statistics:\n",
      "{'total_predictions': 5, 'avg_latency_ms': 0.394, 'max_latency_ms': 0.42, 'min_latency_ms': 0.37}\n",
      "\n",
      "Cache Statistics:\n",
      "{'cache_size': 3, 'total_cache_hits': 5, 'cache_hit_rate': 1.0}\n",
      "\n",
      "No alerts generated - all predictions within acceptable latency!\n"
     ]
    }
   ],
   "source": [
    "class EnhancedModelService(ModelService):\n",
    "    \"\"\"Enhanced model service with validation, alerts, and caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, n_features=20, latency_threshold=10):\n",
    "        super().__init__(model_path)\n",
    "        self.n_features = n_features\n",
    "        self.latency_threshold = latency_threshold  # milliseconds\n",
    "        self.cache = {}\n",
    "        self.alerts = []\n",
    "    \n",
    "    def predict(self, features):\n",
    "        \"\"\"Make predictions with validation, caching, and monitoring.\"\"\"\n",
    "        # Input validation\n",
    "        if isinstance(features, list):\n",
    "            features = np.array(features).reshape(1, -1)\n",
    "        \n",
    "        if features.shape[1] != self.n_features:\n",
    "            raise ValueError(f\"Expected {self.n_features} features, got {features.shape[1]}\")\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = hash(features.tobytes())\n",
    "        if cache_key in self.cache:\n",
    "            self.cache[cache_key]['hits'] += 1\n",
    "            return self.cache[cache_key]['result']\n",
    "        \n",
    "        # Make prediction (calls parent method)\n",
    "        result = super().predict(features)\n",
    "        \n",
    "        # Check for high latency\n",
    "        if result['latency_ms'] > self.latency_threshold:\n",
    "            alert = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'type': 'HIGH_LATENCY',\n",
    "                'latency_ms': result['latency_ms'],\n",
    "                'threshold_ms': self.latency_threshold\n",
    "            }\n",
    "            self.alerts.append(alert)\n",
    "        \n",
    "        # Cache the result\n",
    "        self.cache[cache_key] = {'result': result, 'hits': 1}\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        total_hits = sum(item['hits'] for item in self.cache.values())\n",
    "        return {\n",
    "            'cache_size': len(self.cache),\n",
    "            'total_cache_hits': total_hits,\n",
    "            'cache_hit_rate': total_hits / len(self.prediction_log) if self.prediction_log else 0\n",
    "        }\n",
    "\n",
    "# Test the enhanced service\n",
    "print(\"Testing Enhanced Model Service\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "enhanced_service = EnhancedModelService('models/model_joblib.pkl', n_features=20, latency_threshold=0.5)\n",
    "\n",
    "# Make some predictions (including duplicates to test caching)\n",
    "test_samples = [X_test[0], X_test[1], X_test[0], X_test[2], X_test[1]]  # Some duplicates\n",
    "\n",
    "for i, sample in enumerate(test_samples):\n",
    "    result = enhanced_service.predict(sample)\n",
    "    print(f\"Request {i+1}: Prediction={result['prediction']}, Latency={result['latency_ms']:.2f}ms\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Enhanced Service Statistics:\")\n",
    "print(enhanced_service.get_stats())\n",
    "print(\"\\nCache Statistics:\")\n",
    "print(enhanced_service.get_cache_stats())\n",
    "\n",
    "if enhanced_service.alerts:\n",
    "    print(f\"\\nAlerts Generated: {len(enhanced_service.alerts)}\")\n",
    "    for alert in enhanced_service.alerts:\n",
    "        print(f\"  - {alert['type']}: {alert['latency_ms']:.2f}ms (threshold: {alert['threshold_ms']}ms)\")\n",
    "else:\n",
    "    print(\"\\nNo alerts generated - all predictions within acceptable latency!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Congratulations on completing Lesson 99! Here are the key concepts you should remember:\n",
    "\n",
    "### 1. Model Serialization\n",
    "- **Joblib** is preferred for ML models with large numpy arrays\n",
    "- **Pickle** works for general Python objects but may be less efficient\n",
    "- Always verify loaded models work correctly before deployment\n",
    "\n",
    "### 2. API Design for ML Models\n",
    "- Use REST APIs to expose model predictions\n",
    "- Implement health check endpoints for monitoring\n",
    "- Include error handling and input validation\n",
    "- Return both predictions and confidence scores\n",
    "\n",
    "### 3. Performance Monitoring\n",
    "- Track **latency** (prediction response time)\n",
    "- Monitor **throughput** (predictions per second)\n",
    "- Log all predictions for audit and analysis\n",
    "- Set up alerts for performance degradation\n",
    "\n",
    "### 4. Best Practices\n",
    "- **Version your models**: Keep track of which model version is in production\n",
    "- **Implement caching**: Reduce latency for repeated predictions\n",
    "- **Validate inputs**: Ensure features match training data format\n",
    "- **Monitor model drift**: Track performance over time and retrain when needed\n",
    "- **Use load balancing**: Distribute requests across multiple server instances\n",
    "- **Containerize deployments**: Use Docker for reproducible environments\n",
    "\n",
    "### 5. Production Considerations\n",
    "- Security: Implement authentication and authorization\n",
    "- Scalability: Design for horizontal scaling\n",
    "- Reliability: Add retry logic and circuit breakers\n",
    "- Observability: Comprehensive logging and monitoring\n",
    "\n",
    "### 6. Common Pitfalls to Avoid\n",
    "- Not testing the loaded model before deployment\n",
    "- Ignoring feature preprocessing in production\n",
    "- Missing error handling for edge cases\n",
    "- Not monitoring model performance over time\n",
    "- Hardcoding configuration instead of using environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "To deepen your understanding of model deployment and productionization, explore these resources:\n",
    "\n",
    "### Documentation\n",
    "- **Flask Documentation**: https://flask.palletsprojects.com/\n",
    "- **FastAPI** (Modern alternative to Flask): https://fastapi.tiangolo.com/\n",
    "- **Joblib Documentation**: https://joblib.readthedocs.io/\n",
    "- **Scikit-learn Model Persistence**: https://scikit-learn.org/stable/model_persistence.html\n",
    "\n",
    "### Books\n",
    "- **\"Building Machine Learning Powered Applications\"** by Emmanuel Ameisen\n",
    "- **\"Machine Learning Systems: Designs that Scale\"** by Jeff Smith\n",
    "- **\"Designing Data-Intensive Applications\"** by Martin Kleppmann\n",
    "\n",
    "### Online Courses\n",
    "- **Machine Learning Engineering for Production (MLOps)** - Coursera (DeepLearning.AI)\n",
    "- **Full Stack Deep Learning** - https://fullstackdeeplearning.com/\n",
    "\n",
    "### Tools and Frameworks\n",
    "- **MLflow**: Model tracking and deployment - https://mlflow.org/\n",
    "- **BentoML**: ML model serving framework - https://www.bentoml.com/\n",
    "- **TensorFlow Serving**: Production serving system - https://www.tensorflow.org/tfx/guide/serving\n",
    "- **Seldon Core**: ML deployment on Kubernetes - https://www.seldon.io/\n",
    "- **Docker**: Containerization - https://www.docker.com/\n",
    "- **Kubernetes**: Container orchestration - https://kubernetes.io/\n",
    "\n",
    "### Articles and Tutorials\n",
    "- **Google's ML Engineering Best Practices**: https://developers.google.com/machine-learning/guides/rules-of-ml\n",
    "- **AWS ML Best Practices**: https://aws.amazon.com/machine-learning/\n",
    "- **Azure ML Deployment Guide**: https://docs.microsoft.com/azure/machine-learning/\n",
    "\n",
    "### Topics for Further Exploration\n",
    "1. **A/B Testing**: Deploy multiple model versions and compare performance\n",
    "2. **Model Versioning**: Track and manage different model versions\n",
    "3. **Continuous Training**: Automate model retraining pipelines\n",
    "4. **Edge Deployment**: Deploy models on edge devices\n",
    "5. **Model Compression**: Techniques like quantization and pruning for faster inference\n",
    "6. **Monitoring and Observability**: Advanced logging and monitoring strategies\n",
    "7. **CI/CD for ML**: Automated testing and deployment pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed Lesson 99 and learned how to deploy machine learning models to production. You now understand:\n",
    "- Model serialization techniques\n",
    "- REST API design for ML serving\n",
    "- Performance monitoring and logging\n",
    "- Best practices for production deployment\n",
    "\n",
    "These skills are essential for taking your machine learning projects from notebooks to production systems that deliver real business value. In the next lesson, you'll complete your capstone project and present your work!\n",
    "\n",
    "**Keep learning and building!** \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
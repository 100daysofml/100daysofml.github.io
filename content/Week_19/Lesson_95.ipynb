{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 95: Distributed Training and Gradient Synchronization\n",
    "\n",
    "**Week 19 - Day 5: Federated & Distributed Learning**  \n",
    "**Difficulty Level:** Expert\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As machine learning models grow larger and datasets expand to billions of samples, training on a single device becomes impractical or impossible. **Distributed training** enables us to leverage multiple GPUs or machines to train models faster and handle larger-scale problems.\n",
    "\n",
    "In this lesson, we'll explore:\n",
    "- **Data parallelism** vs **model parallelism**\n",
    "- **Gradient synchronization** strategies (synchronous vs asynchronous)\n",
    "- **PyTorch DistributedDataParallel (DDP)** for efficient multi-GPU training\n",
    "- Real-world considerations for distributed systems\n",
    "\n",
    "### Why Distributed Training Matters\n",
    "\n",
    "1. **Speed**: Training time reduced from weeks to hours\n",
    "2. **Scale**: Handle models too large for single GPU memory\n",
    "3. **Cost efficiency**: Optimize compute resource utilization\n",
    "4. **Research**: Enable experiments previously computationally infeasible\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "- ‚úÖ Understand data parallelism and model parallelism strategies\n",
    "- ‚úÖ Implement distributed training with PyTorch DDP\n",
    "- ‚úÖ Compare synchronous vs asynchronous gradient synchronization\n",
    "- ‚úÖ Recognize communication bottlenecks and optimization techniques\n",
    "- ‚úÖ Apply distributed training to real-world scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Core Concepts: Distributed Training Fundamentals\n",
    "\n",
    "### 1. Data Parallelism vs Model Parallelism\n",
    "\n",
    "#### Data Parallelism\n",
    "- **Strategy**: Split the dataset across multiple devices, replicate the model on each device\n",
    "- **Process**: Each device processes a different batch, computes gradients, then synchronizes\n",
    "- **Best for**: Models that fit on a single device but need faster training\n",
    "- **Example**: Training ResNet-50 on ImageNet across 8 GPUs\n",
    "\n",
    "```\n",
    "GPU 0: Model Copy ‚Üí Batch 0 ‚Üí Gradients ‚Üí \n",
    "GPU 1: Model Copy ‚Üí Batch 1 ‚Üí Gradients ‚Üí  ‚Üí Average Gradients ‚Üí Update All Models\n",
    "GPU 2: Model Copy ‚Üí Batch 2 ‚Üí Gradients ‚Üí \n",
    "GPU 3: Model Copy ‚Üí Batch 3 ‚Üí Gradients ‚Üí \n",
    "```\n",
    "\n",
    "#### Model Parallelism\n",
    "- **Strategy**: Split the model itself across devices (e.g., different layers on different GPUs)\n",
    "- **Process**: Forward/backward pass flows through devices sequentially\n",
    "- **Best for**: Models too large to fit on a single device\n",
    "- **Example**: GPT-3 with 175B parameters\n",
    "\n",
    "```\n",
    "Input ‚Üí GPU 0 (Layers 1-5) ‚Üí GPU 1 (Layers 6-10) ‚Üí GPU 2 (Layers 11-15) ‚Üí Output\n",
    "```\n",
    "\n",
    "### 2. Gradient Synchronization Strategies\n",
    "\n",
    "#### Synchronous Training (All-Reduce)\n",
    "- All workers compute gradients simultaneously\n",
    "- Wait for all workers to finish before averaging gradients\n",
    "- Update all model copies with synchronized gradients\n",
    "- **Pros**: Deterministic, better convergence\n",
    "- **Cons**: Speed limited by slowest worker (straggler problem)\n",
    "\n",
    "#### Asynchronous Training (Parameter Server)\n",
    "- Workers compute gradients independently\n",
    "- Send gradients to parameter server immediately when ready\n",
    "- Parameter server updates central model asynchronously\n",
    "- **Pros**: No waiting, higher throughput\n",
    "- **Cons**: Stale gradients, potential instability\n",
    "\n",
    "### 3. Communication Patterns\n",
    "\n",
    "#### Ring All-Reduce\n",
    "- Most efficient for data parallelism\n",
    "- Workers arranged in a ring topology\n",
    "- Gradients passed around ring, accumulated incrementally\n",
    "- Bandwidth optimal: each worker sends/receives exactly once\n",
    "\n",
    "#### Parameter Server Architecture\n",
    "- Central server(s) store model parameters\n",
    "- Workers push gradients and pull updated parameters\n",
    "- Can become bottleneck with many workers\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "For data parallelism with $N$ workers:\n",
    "\n",
    "**Local gradient on worker $i$:**\n",
    "$$g_i = \\nabla_{\\theta} L(x_i, y_i; \\theta)$$\n",
    "\n",
    "**Synchronized gradient:**\n",
    "$$g = \\frac{1}{N} \\sum_{i=1}^{N} g_i$$\n",
    "\n",
    "**Model update:**\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\cdot g$$\n",
    "\n",
    "**Effective batch size:**\n",
    "$$B_{\\text{effective}} = N \\times B_{\\text{local}}$$\n",
    "\n",
    "Where $B_{\\text{local}}$ is the per-worker batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practical Implementation\n",
    "\n",
    "Let's implement distributed training using PyTorch's DistributedDataParallel (DDP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "# For visualization\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define a Simple Neural Network\n",
    "\n",
    "We'll create a simple CNN for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"Simple CNN for distributed training demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model = SimpleCNN(num_classes=10)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Simulated Distributed Training\n",
    "\n",
    "Since we may not have multiple GPUs available, we'll simulate distributed training to demonstrate the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedTrainingSimulator:\n",
    "    \"\"\"Simulate distributed training with multiple workers.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, num_workers=4, sync_mode='synchronous'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: PyTorch model to train\n",
    "            num_workers: Number of simulated workers\n",
    "            sync_mode: 'synchronous' or 'asynchronous'\n",
    "        \"\"\"\n",
    "        self.num_workers = num_workers\n",
    "        self.sync_mode = sync_mode\n",
    "        \n",
    "        # Create model copies for each worker\n",
    "        self.workers = []\n",
    "        for i in range(num_workers):\n",
    "            worker_model = SimpleCNN(num_classes=10)\n",
    "            worker_model.load_state_dict(model.state_dict())  # Initialize with same weights\n",
    "            self.workers.append({\n",
    "                'id': i,\n",
    "                'model': worker_model,\n",
    "                'optimizer': optim.SGD(worker_model.parameters(), lr=0.01),\n",
    "                'gradients': None,\n",
    "                'compute_time': 0.0\n",
    "            })\n",
    "        \n",
    "        self.global_model = model\n",
    "        self.iteration = 0\n",
    "        self.history = {'sync_times': [], 'worker_times': []}\n",
    "    \n",
    "    def simulate_forward_backward(self, worker_id, batch_data, batch_labels):\n",
    "        \"\"\"Simulate forward and backward pass for a worker.\"\"\"\n",
    "        worker = self.workers[worker_id]\n",
    "        \n",
    "        # Simulate variable computation time (some workers slower than others)\n",
    "        compute_time = np.random.uniform(0.8, 1.2)  # Relative time\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = worker['model'](batch_data)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, batch_labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        worker['optimizer'].zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Store gradients\n",
    "        worker['gradients'] = [p.grad.clone() for p in worker['model'].parameters() if p.grad is not None]\n",
    "        worker['compute_time'] = compute_time\n",
    "        \n",
    "        return loss.item(), compute_time\n",
    "    \n",
    "    def synchronous_update(self):\n",
    "        \"\"\"Synchronous gradient averaging (All-Reduce).\"\"\"\n",
    "        # Wait for all workers (slowest worker determines sync time)\n",
    "        max_compute_time = max(w['compute_time'] for w in self.workers)\n",
    "        \n",
    "        # Average gradients across all workers\n",
    "        avg_gradients = []\n",
    "        num_params = len(self.workers[0]['gradients'])\n",
    "        \n",
    "        for param_idx in range(num_params):\n",
    "            grads = [w['gradients'][param_idx] for w in self.workers]\n",
    "            avg_grad = torch.stack(grads).mean(dim=0)\n",
    "            avg_gradients.append(avg_grad)\n",
    "        \n",
    "        # Update all workers with averaged gradients\n",
    "        for worker in self.workers:\n",
    "            for param, avg_grad in zip(worker['model'].parameters(), avg_gradients):\n",
    "                if param.grad is not None:\n",
    "                    param.grad.copy_(avg_grad)\n",
    "            worker['optimizer'].step()\n",
    "        \n",
    "        # Simulate communication overhead (10% of compute time)\n",
    "        comm_time = max_compute_time * 0.1\n",
    "        total_time = max_compute_time + comm_time\n",
    "        \n",
    "        self.history['sync_times'].append(total_time)\n",
    "        self.history['worker_times'].append([w['compute_time'] for w in self.workers])\n",
    "        \n",
    "        return total_time\n",
    "    \n",
    "    def asynchronous_update(self):\n",
    "        \"\"\"Asynchronous gradient updates (Parameter Server).\"\"\"\n",
    "        # Each worker updates independently as soon as ready\n",
    "        worker_times = []\n",
    "        \n",
    "        for worker in self.workers:\n",
    "            # Apply worker's gradients immediately\n",
    "            for param, grad in zip(worker['model'].parameters(), worker['gradients']):\n",
    "                if param.grad is not None:\n",
    "                    param.grad.copy_(grad)\n",
    "            worker['optimizer'].step()\n",
    "            \n",
    "            # Simulate communication (can happen in parallel)\n",
    "            comm_time = worker['compute_time'] * 0.05\n",
    "            total_time = worker['compute_time'] + comm_time\n",
    "            worker_times.append(total_time)\n",
    "        \n",
    "        # Average time is mean across all workers (no waiting)\n",
    "        avg_time = np.mean(worker_times)\n",
    "        \n",
    "        self.history['sync_times'].append(avg_time)\n",
    "        self.history['worker_times'].append([w['compute_time'] for w in self.workers])\n",
    "        \n",
    "        return avg_time\n",
    "    \n",
    "    def train_step(self, batches):\n",
    "        \"\"\"Perform one training step across all workers.\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        # Each worker processes its batch\n",
    "        for worker_id, (batch_data, batch_labels) in enumerate(batches):\n",
    "            loss, _ = self.simulate_forward_backward(worker_id, batch_data, batch_labels)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        # Synchronize gradients based on mode\n",
    "        if self.sync_mode == 'synchronous':\n",
    "            sync_time = self.synchronous_update()\n",
    "        else:\n",
    "            sync_time = self.asynchronous_update()\n",
    "        \n",
    "        self.iteration += 1\n",
    "        return np.mean(losses), sync_time\n",
    "\n",
    "print(\"DistributedTrainingSimulator class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate Synthetic Data and Run Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "def generate_batch(batch_size=32, num_classes=10):\n",
    "    \"\"\"Generate synthetic image data and labels.\"\"\"\n",
    "    data = torch.randn(batch_size, 3, 32, 32)\n",
    "    labels = torch.randint(0, num_classes, (batch_size,))\n",
    "    return data, labels\n",
    "\n",
    "# Run simulations\n",
    "num_workers = 4\n",
    "num_iterations = 50\n",
    "batch_size_per_worker = 32\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYNCHRONOUS TRAINING SIMULATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Synchronous simulation\n",
    "model_sync = SimpleCNN(num_classes=10)\n",
    "simulator_sync = DistributedTrainingSimulator(model_sync, num_workers=num_workers, sync_mode='synchronous')\n",
    "\n",
    "sync_losses = []\n",
    "sync_times = []\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Generate batches for each worker\n",
    "    batches = [generate_batch(batch_size_per_worker) for _ in range(num_workers)]\n",
    "    loss, step_time = simulator_sync.train_step(batches)\n",
    "    sync_losses.append(loss)\n",
    "    sync_times.append(step_time)\n",
    "    \n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        print(f\"Iteration {iteration+1}/{num_iterations} | Loss: {loss:.4f} | Time: {step_time:.3f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ASYNCHRONOUS TRAINING SIMULATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Asynchronous simulation\n",
    "model_async = SimpleCNN(num_classes=10)\n",
    "model_async.load_state_dict(model_sync.state_dict())  # Start from same initialization\n",
    "simulator_async = DistributedTrainingSimulator(model_async, num_workers=num_workers, sync_mode='asynchronous')\n",
    "\n",
    "async_losses = []\n",
    "async_times = []\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    batches = [generate_batch(batch_size_per_worker) for _ in range(num_workers)]\n",
    "    loss, step_time = simulator_async.train_step(batches)\n",
    "    async_losses.append(loss)\n",
    "    async_times.append(step_time)\n",
    "    \n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        print(f\"Iteration {iteration+1}/{num_iterations} | Loss: {loss:.4f} | Time: {step_time:.3f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SIMULATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Synchronous - Total time: {sum(sync_times):.2f}s | Avg time/iteration: {np.mean(sync_times):.3f}s\")\n",
    "print(f\"Asynchronous - Total time: {sum(async_times):.2f}s | Avg time/iteration: {np.mean(async_times):.3f}s\")\n",
    "print(f\"Speedup (Async vs Sync): {sum(sync_times) / sum(async_times):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Training loss comparison\n",
    "axes[0, 0].plot(sync_losses, label='Synchronous', linewidth=2, alpha=0.8)\n",
    "axes[0, 0].plot(async_losses, label='Asynchronous', linewidth=2, alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Loss: Sync vs Async')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Time per iteration\n",
    "axes[0, 1].plot(sync_times, label='Synchronous', linewidth=2, alpha=0.8)\n",
    "axes[0, 1].plot(async_times, label='Asynchronous', linewidth=2, alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Time (seconds)')\n",
    "axes[0, 1].set_title('Time per Iteration')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Cumulative time\n",
    "cumulative_sync = np.cumsum(sync_times)\n",
    "cumulative_async = np.cumsum(async_times)\n",
    "axes[1, 0].plot(cumulative_sync, label='Synchronous', linewidth=2, alpha=0.8)\n",
    "axes[1, 0].plot(cumulative_async, label='Asynchronous', linewidth=2, alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Iteration')\n",
    "axes[1, 0].set_ylabel('Cumulative Time (seconds)')\n",
    "axes[1, 0].set_title('Cumulative Training Time')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Worker time distribution (last 10 iterations)\n",
    "last_sync_times = simulator_sync.history['worker_times'][-10:]\n",
    "last_async_times = simulator_async.history['worker_times'][-10:]\n",
    "\n",
    "worker_data = []\n",
    "for i in range(num_workers):\n",
    "    sync_worker_times = [times[i] for times in last_sync_times]\n",
    "    async_worker_times = [times[i] for times in last_async_times]\n",
    "    worker_data.extend([('Sync', f'Worker {i}', t) for t in sync_worker_times])\n",
    "    worker_data.extend([('Async', f'Worker {i}', t) for t in async_worker_times])\n",
    "\n",
    "import pandas as pd\n",
    "df_workers = pd.DataFrame(worker_data, columns=['Mode', 'Worker', 'Time'])\n",
    "\n",
    "sync_avg = [np.mean([times[i] for times in last_sync_times]) for i in range(num_workers)]\n",
    "async_avg = [np.mean([times[i] for times in last_async_times]) for i in range(num_workers)]\n",
    "\n",
    "x = np.arange(num_workers)\n",
    "width = 0.35\n",
    "axes[1, 1].bar(x - width/2, sync_avg, width, label='Synchronous', alpha=0.8)\n",
    "axes[1, 1].bar(x + width/2, async_avg, width, label='Asynchronous', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Worker ID')\n",
    "axes[1, 1].set_ylabel('Avg Compute Time (seconds)')\n",
    "axes[1, 1].set_title('Worker Compute Time (Last 10 Iterations)')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels([f'W{i}' for i in range(num_workers)])\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(\"  1. Synchronous training waits for the slowest worker (straggler problem)\")\n",
    "print(\"  2. Asynchronous training achieves higher throughput but may have noisier convergence\")\n",
    "print(\"  3. Effective batch size = num_workers √ó batch_size_per_worker = \"\n",
    "      f\"{num_workers} √ó {batch_size_per_worker} = {num_workers * batch_size_per_worker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real-World Distributed Training with PyTorch DDP\n",
    "\n",
    "Here's how you would set up actual multi-GPU training with PyTorch DistributedDataParallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: PyTorch DDP setup (conceptual - requires multi-GPU environment)\n",
    "\n",
    "ddp_example = '''\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    \"\"\"Initialize the distributed environment.\"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Clean up the distributed environment.\"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def train(rank, world_size):\n",
    "    \"\"\"Training function for each process.\"\"\"\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    # Create model and move to GPU\n",
    "    model = SimpleCNN().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    \n",
    "    # Create distributed sampler\n",
    "    train_dataset = YourDataset()\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "    \n",
    "    # Training loop\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_sampler.set_epoch(epoch)  # Shuffle data differently each epoch\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(rank), batch_labels.to(rank)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = ddp_model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()  # Gradients automatically synchronized across GPUs\n",
    "            optimizer.step()\n",
    "    \n",
    "    cleanup()\n",
    "\n",
    "# Launch training\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "'''\n",
    "\n",
    "print(\"PyTorch DDP Setup Example:\")\n",
    "print(\"=\" * 60)\n",
    "print(ddp_example)\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüîë Key DDP Components:\")\n",
    "print(\"  ‚Ä¢ dist.init_process_group(): Initialize distributed backend (NCCL for GPUs)\")\n",
    "print(\"  ‚Ä¢ DistributedDataParallel: Wraps model for automatic gradient synchronization\")\n",
    "print(\"  ‚Ä¢ DistributedSampler: Ensures each GPU gets different data\")\n",
    "print(\"  ‚Ä¢ Gradients synced automatically during backward() via All-Reduce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hands-On Activity: Optimize Communication Overhead\n",
    "\n",
    "### Challenge\n",
    "\n",
    "You're training a large model across 8 GPUs. You notice that communication overhead is 30% of your total training time. Implement gradient compression to reduce communication cost.\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement gradient quantization (reducing precision)\n",
    "2. Compare compressed vs uncompressed gradient sizes\n",
    "3. Analyze the trade-off between compression ratio and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientCompressor:\n",
    "    \"\"\"Compress gradients to reduce communication overhead.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize(tensor, num_bits=8):\n",
    "        \"\"\"\n",
    "        Quantize tensor to reduce precision.\n",
    "        \n",
    "        Args:\n",
    "            tensor: Input tensor (float32)\n",
    "            num_bits: Number of bits for quantization (1-8)\n",
    "        \n",
    "        Returns:\n",
    "            Quantized tensor, scale, zero_point\n",
    "        \"\"\"\n",
    "        # Find min/max values\n",
    "        min_val = tensor.min()\n",
    "        max_val = tensor.max()\n",
    "        \n",
    "        # Calculate scale and zero point\n",
    "        qmin = 0\n",
    "        qmax = 2**num_bits - 1\n",
    "        scale = (max_val - min_val) / (qmax - qmin)\n",
    "        zero_point = qmin - min_val / scale\n",
    "        \n",
    "        # Quantize\n",
    "        quantized = torch.clamp(torch.round(tensor / scale + zero_point), qmin, qmax)\n",
    "        \n",
    "        return quantized.to(torch.uint8), scale, zero_point\n",
    "    \n",
    "    @staticmethod\n",
    "    def dequantize(quantized, scale, zero_point):\n",
    "        \"\"\"\n",
    "        Dequantize tensor back to float32.\n",
    "        \n",
    "        Args:\n",
    "            quantized: Quantized tensor (uint8)\n",
    "            scale: Scale factor\n",
    "            zero_point: Zero point\n",
    "        \n",
    "        Returns:\n",
    "            Dequantized tensor (float32)\n",
    "        \"\"\"\n",
    "        return scale * (quantized.float() - zero_point)\n",
    "    \n",
    "    @staticmethod\n",
    "    def top_k_sparsification(tensor, k=0.1):\n",
    "        \"\"\"\n",
    "        Keep only top k% gradients by magnitude (sparse communication).\n",
    "        \n",
    "        Args:\n",
    "            tensor: Input tensor\n",
    "            k: Fraction of gradients to keep (0-1)\n",
    "        \n",
    "        Returns:\n",
    "            Sparse tensor (most values zeroed out)\n",
    "        \"\"\"\n",
    "        flat = tensor.flatten()\n",
    "        threshold_idx = int(len(flat) * (1 - k))\n",
    "        threshold = torch.topk(flat.abs(), threshold_idx)[0][-1]\n",
    "        \n",
    "        mask = tensor.abs() >= threshold\n",
    "        return tensor * mask\n",
    "\n",
    "# Test compression techniques\n",
    "print(\"Testing Gradient Compression Techniques\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create sample gradient tensor\n",
    "sample_gradient = torch.randn(1000, 1000) * 0.01  # Typical gradient magnitudes\n",
    "\n",
    "# Original size\n",
    "original_size = sample_gradient.element_size() * sample_gradient.nelement()\n",
    "print(f\"\\nüì¶ Original gradient size: {original_size / 1024 / 1024:.2f} MB (float32)\")\n",
    "\n",
    "# Quantization compression\n",
    "compressor = GradientCompressor()\n",
    "quantized, scale, zp = compressor.quantize(sample_gradient, num_bits=8)\n",
    "dequantized = compressor.dequantize(quantized, scale, zp)\n",
    "\n",
    "quantized_size = quantized.element_size() * quantized.nelement() + 8  # +8 for scale/zp\n",
    "compression_ratio = original_size / quantized_size\n",
    "quantization_error = (sample_gradient - dequantized).abs().mean().item()\n",
    "\n",
    "print(f\"\\nüóúÔ∏è  8-bit Quantization:\")\n",
    "print(f\"  Compressed size: {quantized_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Compression ratio: {compression_ratio:.2f}x\")\n",
    "print(f\"  Mean absolute error: {quantization_error:.6f}\")\n",
    "\n",
    "# Top-K sparsification\n",
    "for k in [0.1, 0.01, 0.001]:\n",
    "    sparse = compressor.top_k_sparsification(sample_gradient, k=k)\n",
    "    sparsity = (sparse == 0).float().mean().item()\n",
    "    \n",
    "    # In practice, sparse tensors would be stored efficiently\n",
    "    # For now, just calculate theoretical compression\n",
    "    theoretical_size = original_size * k\n",
    "    theoretical_ratio = original_size / theoretical_size\n",
    "    sparsification_error = (sample_gradient - sparse).abs().mean().item()\n",
    "    \n",
    "    print(f\"\\n‚úÇÔ∏è  Top-{k*100:.1f}% Sparsification:\")\n",
    "    print(f\"  Sparsity: {sparsity*100:.1f}% of values zeroed\")\n",
    "    print(f\"  Theoretical compression: {theoretical_ratio:.2f}x\")\n",
    "    print(f\"  Mean absolute error: {sparsification_error:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  ‚Ä¢ Quantization: 4x compression with minimal error\")\n",
    "print(\"  ‚Ä¢ Top-K: Higher compression but loses small gradients\")\n",
    "print(\"  ‚Ä¢ Trade-off: Communication cost vs convergence speed\")\n",
    "print(\"  ‚Ä¢ In practice: Combine techniques (quantized + sparse)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: Experiment with Compression\n",
    "\n",
    "**Your Turn!** Try the following:\n",
    "\n",
    "1. **Vary quantization bits**: Test 4-bit, 2-bit, and 1-bit quantization. How does error change?\n",
    "2. **Gradient accumulation**: Instead of syncing every iteration, accumulate gradients for N steps. How does this affect training?\n",
    "3. **Mixed precision**: Research how FP16/BF16 training reduces memory and communication costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Experiment with different compression strategies\n",
    "\n",
    "# Example starter code:\n",
    "# for num_bits in [1, 2, 4, 8]:\n",
    "#     quantized, scale, zp = compressor.quantize(sample_gradient, num_bits=num_bits)\n",
    "#     # Analyze results...\n",
    "\n",
    "print(\"Experiment with gradient compression techniques here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Advanced Considerations\n",
    "\n",
    "### 1. Scaling Laws\n",
    "\n",
    "**Linear Scaling Rule** (Goyal et al., 2017):\n",
    "- When increasing batch size by $k$, increase learning rate by $k$\n",
    "- Example: batch 256 with lr=0.1 ‚Üí batch 1024 with lr=0.4\n",
    "- Works well for moderate scaling (up to ~8k batch size)\n",
    "\n",
    "**Gradual Warmup**:\n",
    "- Start with small learning rate, gradually increase to target\n",
    "- Prevents instability when using large batch sizes\n",
    "- Typical: warmup for first 5-10 epochs\n",
    "\n",
    "### 2. Communication Optimization\n",
    "\n",
    "**Gradient Compression Techniques:**\n",
    "- **Quantization**: Reduce gradient precision (8-bit, 4-bit)\n",
    "- **Sparsification**: Send only top-K gradients by magnitude\n",
    "- **Error Feedback**: Accumulate quantization errors, send in next iteration\n",
    "\n",
    "**Overlapping Communication and Computation:**\n",
    "- Start gradient sync while still computing later layers\n",
    "- PyTorch DDP does this automatically with bucketing\n",
    "\n",
    "### 3. Fault Tolerance\n",
    "\n",
    "**Challenges:**\n",
    "- What if one worker fails mid-training?\n",
    "- How to handle stragglers (slow workers)?\n",
    "\n",
    "**Solutions:**\n",
    "- **Checkpointing**: Save model state frequently\n",
    "- **Elastic training**: Dynamically add/remove workers (Torch Elastic)\n",
    "- **Backup workers**: Keep redundant workers to replace failures\n",
    "\n",
    "### 4. Beyond Data Parallelism\n",
    "\n",
    "**Pipeline Parallelism:**\n",
    "- Split model into stages across devices\n",
    "- Process multiple micro-batches in pipeline\n",
    "- Reduces idle time vs simple model parallelism\n",
    "\n",
    "**Tensor Parallelism:**\n",
    "- Split individual layers across devices\n",
    "- Each device computes part of matrix multiplication\n",
    "- Used in Megatron-LM for training massive language models\n",
    "\n",
    "**3D Parallelism:**\n",
    "- Combine data + pipeline + tensor parallelism\n",
    "- Used to train models with 100B+ parameters\n",
    "- Example: GPT-3 training at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different parallelism strategies\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Data Parallelism\n",
    "axes[0].text(0.5, 0.85, 'Data Parallelism', ha='center', fontsize=14, weight='bold')\n",
    "for i in range(4):\n",
    "    y = 0.7 - i*0.15\n",
    "    axes[0].add_patch(plt.Rectangle((0.15, y-0.05), 0.7, 0.08, \n",
    "                                     facecolor=f'C{i}', edgecolor='black', linewidth=2))\n",
    "    axes[0].text(0.5, y, f'GPU {i}: Full Model\\nBatch {i}', ha='center', va='center', fontsize=10)\n",
    "axes[0].text(0.5, 0.05, 'All GPUs sync gradients\\n(All-Reduce)', ha='center', fontsize=9, style='italic')\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Model Parallelism\n",
    "axes[1].text(0.5, 0.85, 'Model Parallelism', ha='center', fontsize=14, weight='bold')\n",
    "layer_names = ['Input ‚Üí L1-3', 'Layers 4-6', 'Layers 7-9', 'L10-12 ‚Üí Out']\n",
    "for i in range(4):\n",
    "    y = 0.7 - i*0.15\n",
    "    axes[1].add_patch(plt.Rectangle((0.15, y-0.05), 0.7, 0.08, \n",
    "                                     facecolor=f'C{i}', edgecolor='black', linewidth=2))\n",
    "    axes[1].text(0.5, y, f'GPU {i}:\\n{layer_names[i]}', ha='center', va='center', fontsize=9)\n",
    "    if i < 3:\n",
    "        axes[1].arrow(0.5, y-0.06, 0, -0.035, head_width=0.04, head_length=0.01, fc='black', ec='black')\n",
    "axes[1].text(0.5, 0.05, 'Sequential forward/backward\\nthrough GPUs', ha='center', fontsize=9, style='italic')\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Pipeline Parallelism\n",
    "axes[2].text(0.5, 0.85, 'Pipeline Parallelism', ha='center', fontsize=14, weight='bold')\n",
    "pipeline_data = [\n",
    "    ['B0', 'B1', 'B2', 'B3'],\n",
    "    ['', 'B0', 'B1', 'B2'],\n",
    "    ['', '', 'B0', 'B1'],\n",
    "    ['', '', '', 'B0']\n",
    "]\n",
    "for i in range(4):\n",
    "    y = 0.7 - i*0.15\n",
    "    axes[2].add_patch(plt.Rectangle((0.05, y-0.05), 0.9, 0.08, \n",
    "                                     facecolor='lightgray', edgecolor='black', linewidth=1))\n",
    "    axes[2].text(0.02, y, f'G{i}', ha='center', va='center', fontsize=9, weight='bold')\n",
    "    for j, batch in enumerate(pipeline_data[i]):\n",
    "        if batch:\n",
    "            x = 0.15 + j*0.18\n",
    "            axes[2].add_patch(plt.Rectangle((x, y-0.03), 0.15, 0.06, \n",
    "                                             facecolor=f'C{j}', edgecolor='black', linewidth=1))\n",
    "            axes[2].text(x+0.075, y, batch, ha='center', va='center', fontsize=8)\n",
    "axes[2].text(0.5, 0.05, 'Micro-batches flow through\\npipeline stages', ha='center', fontsize=9, style='italic')\n",
    "axes[2].set_xlim(0, 1)\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Parallelism Strategy Selection:\")\n",
    "print(\"  ‚Ä¢ Data Parallelism: Model fits on 1 GPU, want faster training\")\n",
    "print(\"  ‚Ä¢ Model Parallelism: Model too large for 1 GPU memory\")\n",
    "print(\"  ‚Ä¢ Pipeline Parallelism: Reduce bubble time in model parallelism\")\n",
    "print(\"  ‚Ä¢ Hybrid: Combine all three for maximum scale (e.g., GPT-3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### üéØ Core Concepts\n",
    "\n",
    "1. **Data Parallelism** is the most common distributed training strategy\n",
    "   - Replicate model across devices, split data\n",
    "   - Synchronize gradients via All-Reduce\n",
    "   - Effective batch size = num_devices √ó local_batch_size\n",
    "\n",
    "2. **Synchronous vs Asynchronous Training**\n",
    "   - Synchronous: Better convergence, limited by stragglers\n",
    "   - Asynchronous: Higher throughput, potential stability issues\n",
    "   - Most production systems use synchronous (All-Reduce)\n",
    "\n",
    "3. **Communication is the Bottleneck**\n",
    "   - Gradient synchronization can be 30-50% of iteration time\n",
    "   - Optimize via compression, overlapping compute/communication\n",
    "   - Ring All-Reduce is bandwidth-optimal\n",
    "\n",
    "4. **Scaling Considerations**\n",
    "   - Linear scaling rule: scale learning rate with batch size\n",
    "   - Warmup needed for large batches\n",
    "   - Diminishing returns beyond certain scale\n",
    "\n",
    "### üìö Practical Guidelines\n",
    "\n",
    "**When to use distributed training:**\n",
    "- ‚úÖ Training takes >24 hours on single GPU\n",
    "- ‚úÖ Model barely fits in GPU memory\n",
    "- ‚úÖ Need to experiment with larger batch sizes\n",
    "- ‚úÖ Have access to multi-GPU infrastructure\n",
    "\n",
    "**When NOT to use distributed training:**\n",
    "- ‚ùå Model trains in <1 hour (overhead not worth it)\n",
    "- ‚ùå Dataset is very small\n",
    "- ‚ùå Debugging new model architecture\n",
    "\n",
    "### ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "1. **Forgetting to scale learning rate** with batch size\n",
    "2. **Not using DistributedSampler** ‚Üí all GPUs see same data\n",
    "3. **Improper gradient accumulation** ‚Üí effective batch size confusion\n",
    "4. **Ignoring communication overhead** ‚Üí poor scaling efficiency\n",
    "5. **No batch norm adjustments** ‚Üí stats incorrect with small local batches\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "1. **Hands-on Practice**: Train a real model with PyTorch DDP\n",
    "2. **Read Papers**: \n",
    "   - \"Accurate, Large Minibatch SGD\" (Goyal et al., 2017)\n",
    "   - \"Deep Gradient Compression\" (Lin et al., 2018)\n",
    "3. **Explore Tools**: Horovod, DeepSpeed, Megatron-LM\n",
    "4. **Advanced Topics**: ZeRO optimizer, 3D parallelism, gradient checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Resources\n",
    "\n",
    "### üìñ Documentation\n",
    "- [PyTorch Distributed Training Tutorial](https://pytorch.org/tutorials/beginner/dist_overview.html)\n",
    "- [PyTorch DDP Documentation](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)\n",
    "- [NVIDIA NCCL Documentation](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html)\n",
    "\n",
    "### üìù Research Papers\n",
    "1. **Goyal et al. (2017)**: [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677)\n",
    "2. **Lin et al. (2018)**: [Deep Gradient Compression](https://arxiv.org/abs/1712.01887)\n",
    "3. **Rajbhandari et al. (2020)**: [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)\n",
    "4. **Narayanan et al. (2021)**: [Efficient Large-Scale Language Model Training](https://arxiv.org/abs/2104.04473)\n",
    "\n",
    "### üõ†Ô∏è Tools and Frameworks\n",
    "- **Horovod**: Easy distributed deep learning framework\n",
    "- **DeepSpeed**: Microsoft's optimization library for large models\n",
    "- **Megatron-LM**: NVIDIA's framework for training massive language models\n",
    "- **Ray**: Distributed computing framework with ML support\n",
    "- **Torch Elastic**: Fault-tolerant distributed training\n",
    "\n",
    "### üé• Videos and Courses\n",
    "- [Stanford CS231n: Distributed Training](https://cs231n.stanford.edu/)\n",
    "- [PyTorch Distributed Training Webinar](https://www.youtube.com/watch?v=0fKT4WVq6AQ)\n",
    "- [Distributed Deep Learning with Horovod](https://www.youtube.com/watch?v=D1By2hy4Ecw)\n",
    "\n",
    "### üè¢ Cloud Platform Guides\n",
    "- [AWS SageMaker Distributed Training](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html)\n",
    "- [Google Cloud AI Platform Training](https://cloud.google.com/ai-platform/training/docs/distributed-training)\n",
    "- [Azure Machine Learning Distributed Training](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-distributed-gpu)\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Congratulations!\n",
    "\n",
    "You've completed Lesson 95 on **Distributed Training and Gradient Synchronization**. You now understand:\n",
    "\n",
    "‚úÖ The fundamentals of data and model parallelism  \n",
    "‚úÖ How gradient synchronization works (synchronous vs asynchronous)  \n",
    "‚úÖ Communication patterns and optimization techniques  \n",
    "‚úÖ Practical implementation with PyTorch DDP  \n",
    "‚úÖ Real-world considerations for scaling distributed training  \n",
    "\n",
    "**You're now ready to train models at scale!** üöÄ\n",
    "\n",
    "Continue to **Week 20** to explore more advanced machine learning topics!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Day 93: Privacy-Preserving Machine Learning\n\n## Introduction\n\nIn an era where data drives innovation, protecting individual privacy while extracting valuable insights has become one of the most critical challenges in machine learning. Privacy-preserving machine learning (PPML) encompasses a set of techniques that enable us to train models, make predictions, and analyze data without exposing sensitive information.\n\nConsider the healthcare industry: hospitals want to collaborate on building better diagnostic models, but they cannot share patient records due to privacy regulations like HIPAA and GDPR. Or think about financial institutions that need to detect fraud patterns across multiple banks without revealing transaction details. Privacy-preserving ML techniques make these scenarios possible.\n\nThe tension between data utility and privacy protection has spawned innovative cryptographic and algorithmic approaches. From **homomorphic encryption** that allows computation on encrypted data, to **secure multi-party computation** that enables collaborative learning without data sharing, to **differential privacy** that adds controlled noise to protect individualsâ€”these techniques form the foundation of privacy-aware AI systems.\n\n### Why This Matters\n\n- **Regulatory Compliance**: GDPR, CCPA, HIPAA, and other regulations mandate privacy protection\n- **Trust & Ethics**: Users increasingly demand transparency about how their data is used\n- **Collaborative ML**: Organizations can jointly train models without compromising proprietary data\n- **Federated Learning Foundation**: Privacy techniques are essential for distributed learning systems\n\n### Learning Objectives\n\nBy the end of this lesson, you will:\n\n1. Understand the fundamental privacy threats in machine learning (model inversion, membership inference, etc.)\n2. Explore cryptographic approaches: homomorphic encryption and secure multi-party computation\n3. Implement differential privacy mechanisms to protect sensitive data\n4. Apply privacy-preserving techniques to real-world ML workflows\n5. Evaluate the privacy-utility tradeoff in practical scenarios",
   "id": "cell-introduction"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Theory: Foundations of Privacy-Preserving Machine Learning\n\n### 1. Privacy Threats in Machine Learning\n\nBefore diving into solutions, let's understand the threats:\n\n#### Model Inversion Attacks\nAn adversary can reconstruct training data by querying a model. If a face recognition model outputs confidence scores, attackers can optimize input images to maximize confidence for a specific person, effectively reconstructing their face.\n\n#### Membership Inference Attacks\nGiven a data point and a model, an attacker determines whether that data point was in the training set. This is problematic for sensitive datasets (e.g., medical records, financial data).\n\n#### Training Data Extraction\nLarge language models have been shown to memorize and regurgitate training data verbatim, potentially exposing private information like credit card numbers or personal identifiers.\n\n---\n\n### 2. Differential Privacy (DP)\n\n**Differential Privacy** provides a mathematical guarantee that the presence or absence of any single individual's data has a negligible effect on the model's output.\n\n#### Formal Definition\n\nA randomized mechanism $\\mathcal{M}$ satisfies $(\\epsilon, \\delta)$-differential privacy if for all datasets $D_1$ and $D_2$ differing in at most one element, and for all possible outputs $S$:\n\n$$P[\\mathcal{M}(D_1) \\in S] \\leq e^\\epsilon \\cdot P[\\mathcal{M}(D_2) \\in S] + \\delta$$\n\n**Parameters:**\n- $\\epsilon$ (epsilon): Privacy budget. Smaller values = stronger privacy (typical: 0.1 to 10)\n- $\\delta$ (delta): Probability of privacy breach (typical: $\\frac{1}{n^2}$ where $n$ is dataset size)\n\n#### Mechanisms for Achieving DP\n\n**1. Laplace Mechanism** (for numerical queries):\n$$\\tilde{f}(D) = f(D) + \\text{Lap}\\left(\\frac{\\Delta f}{\\epsilon}\\right)$$\n\nwhere $\\Delta f$ is the sensitivity (max change in output when one record changes).\n\n**2. Gaussian Mechanism** (for better composition):\n$$\\tilde{f}(D) = f(D) + \\mathcal{N}\\left(0, \\frac{2\\Delta f^2 \\ln(1.25/\\delta)}{\\epsilon^2}\\right)$$\n\n**3. Exponential Mechanism** (for selecting from discrete sets):\nUsed when adding noise to numerical output isn't appropriate (e.g., choosing best model parameters).\n\n#### DP-SGD (Differentially Private Stochastic Gradient Descent)\n\nFor training neural networks with privacy:\n\n1. **Clip gradients** per sample: $\\bar{g}_i = g_i / \\max(1, \\frac{\\|g_i\\|_2}{C})$\n2. **Add Gaussian noise**: $\\tilde{g} = \\frac{1}{N}\\left(\\sum_{i=1}^{N} \\bar{g}_i + \\mathcal{N}(0, \\sigma^2 C^2 I)\\right)$\n3. **Update model**: $\\theta_{t+1} = \\theta_t - \\eta \\tilde{g}$\n\nThe privacy budget is tracked using the **privacy accounting** framework (typically using RÃ©nyi DP or moments accountant).\n\n---\n\n### 3. Homomorphic Encryption (HE)\n\nHomomorphic encryption allows computation on encrypted data without decryption. The result, when decrypted, matches the result of operations on plaintext.\n\n#### Types of Homomorphic Encryption\n\n**Partially Homomorphic Encryption (PHE):**\n- RSA: supports multiplication\n- Paillier: supports addition\n- Example: $E(a) + E(b) = E(a + b)$\n\n**Somewhat Homomorphic Encryption (SWHE):**\n- Limited depth of operations (noise accumulates)\n\n**Fully Homomorphic Encryption (FHE):**\n- Arbitrary computation on encrypted data\n- Schemes: BFV, CKKS, TFHE\n- Practical but computationally expensive (10,000x slowdown typical)\n\n#### CKKS Scheme (for Approximate Arithmetic)\n\nUsed for ML applications:\n\n$$E(m_1) \\otimes E(m_2) = E(m_1 \\times m_2)$$\n$$E(m_1) \\oplus E(m_2) = E(m_1 + m_2)$$\n\n**Key insight**: Allows approximate computations (suitable for floating-point ML operations).\n\n---\n\n### 4. Secure Multi-Party Computation (SMPC)\n\nSMPC allows multiple parties to jointly compute a function on their private inputs without revealing them to each other.\n\n#### Secret Sharing\n\n**Additive Secret Sharing:**\nSplit secret $x$ into $n$ shares: $x = x_1 + x_2 + \\ldots + x_n$\n\nEach party holds $x_i$; no single party knows $x$.\n\n**Example: Computing Sum Privately**\n- Alice has $a$, Bob has $b$, Carol has $c$\n- Split each value into 3 shares\n- Distribute shares so each party has one share of each value\n- Each computes local sum, then reveal and combine\n\n#### Garbled Circuits\n\nUsed for arbitrary boolean circuits:\n1. Convert function to boolean circuit\n2. \"Garble\" the circuit (encrypt truth tables)\n3. Execute using oblivious transfer\n4. Reveal only the output\n\n**Cost**: $O(|C|)$ where $|C|$ is circuit size. Practical for small functions.\n\n---\n\n### 5. Federated Learning with Privacy\n\nCombining federated learning (Day 91) with privacy:\n\n**Privacy Enhancements:**\n1. **Secure Aggregation**: Encrypt model updates during aggregation\n2. **Differential Privacy**: Add noise to gradients before sharing\n3. **Homomorphic Encryption**: Aggregate encrypted gradients on server\n\n**Privacy Budget Management:**\n$$\\epsilon_{\\text{total}} = \\sum_{t=1}^{T} \\epsilon_t$$\n\nUse adaptive clipping and noise scheduling to minimize privacy loss.\n\n---\n\n### 6. Privacy-Utility Tradeoff\n\nThere's always a tradeoff between privacy and model accuracy:\n\n- **Strong privacy** (low $\\epsilon$) â†’ More noise â†’ Lower accuracy\n- **Weak privacy** (high $\\epsilon$) â†’ Less noise â†’ Higher accuracy\n\n**Key considerations:**\n- Dataset size (larger datasets tolerate more noise)\n- Model complexity (simpler models may be more robust to noise)\n- Task sensitivity (medical diagnosis vs. movie recommendations)",
   "id": "cell-theory"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer, make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\n\n# Configure plotting\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(\"âœ“ Libraries imported successfully\")\nprint(\"âœ“ Privacy-preserving ML lesson environment ready\")",
   "id": "cell-imports"
  },
  {
   "cell_type": "markdown",
   "source": "## Implementation 1: Differential Privacy with Laplace Mechanism\n\nLet's implement a basic differential privacy mechanism for protecting aggregate statistics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class LaplaceMechanism:\n    \"\"\"\n    Implements the Laplace Mechanism for differential privacy.\n    Adds Laplace noise calibrated to sensitivity and epsilon.\n    \"\"\"\n    \n    def __init__(self, epsilon, sensitivity):\n        \"\"\"\n        Parameters:\n        - epsilon: Privacy budget (smaller = more private)\n        - sensitivity: Maximum change in output when one record changes\n        \"\"\"\n        self.epsilon = epsilon\n        self.sensitivity = sensitivity\n        self.scale = sensitivity / epsilon\n    \n    def add_noise(self, true_value):\n        \"\"\"Add Laplace noise to protect privacy\"\"\"\n        noise = np.random.laplace(0, self.scale)\n        return true_value + noise\n    \n    def release_statistic(self, data, query_func):\n        \"\"\"\n        Release a differentially private statistic.\n        \n        Parameters:\n        - data: Input dataset\n        - query_func: Function to compute statistic (e.g., mean, sum)\n        \"\"\"\n        true_result = query_func(data)\n        private_result = self.add_noise(true_result)\n        return private_result, true_result\n\n# Example: Computing average salary with privacy\nnp.random.seed(42)\nsalaries = np.array([50000, 55000, 60000, 65000, 70000, 75000, 80000, 85000, 90000, 95000])\n\nprint(\"=\" * 60)\nprint(\"DIFFERENTIAL PRIVACY EXAMPLE: Average Salary\")\nprint(\"=\" * 60)\nprint(f\"\\nTrue average salary: ${np.mean(salaries):,.2f}\")\n\n# Test different epsilon values\nepsilons = [0.1, 1.0, 5.0, 10.0]\nsensitivity = 45000 / len(salaries)  # Max change if one salary changes by $45k\n\nfor eps in epsilons:\n    mechanism = LaplaceMechanism(epsilon=eps, sensitivity=sensitivity)\n    private_mean, _ = mechanism.release_statistic(salaries, np.mean)\n    print(f\"\\nÎµ = {eps:4.1f}: ${private_mean:,.2f} (privacy: {'strong' if eps < 1 else 'moderate' if eps < 5 else 'weak'})\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Note: Smaller epsilon = stronger privacy but more noise\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualization: Privacy-Utility Tradeoff\nnp.random.seed(42)\n\n# Simulate the effect of epsilon on query accuracy\ntrue_mean = 70000\nn_trials = 100\nepsilons = np.logspace(-1, 2, 50)  # 0.1 to 100\nsensitivity = 45000 / 10\n\nresults = []\nfor epsilon in epsilons:\n    errors = []\n    for _ in range(n_trials):\n        mechanism = LaplaceMechanism(epsilon=epsilon, sensitivity=sensitivity)\n        private_mean, _ = mechanism.release_statistic(np.array([true_mean]), lambda x: x[0])\n        error = abs(private_mean - true_mean)\n        errors.append(error)\n    results.append(np.mean(errors))\n\n# Create figure with subplots\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plot 1: Mean Absolute Error vs Epsilon\naxes[0].plot(epsilons, results, linewidth=2.5, color='#e74c3c')\naxes[0].fill_between(epsilons, results, alpha=0.3, color='#e74c3c')\naxes[0].set_xscale('log')\naxes[0].set_xlabel('Privacy Budget (Îµ)', fontsize=12, fontweight='bold')\naxes[0].set_ylabel('Mean Absolute Error ($)', fontsize=12, fontweight='bold')\naxes[0].set_title('Privacy-Utility Tradeoff\\n(Lower Îµ = More Privacy, Higher Error)', \n                  fontsize=13, fontweight='bold', pad=15)\naxes[0].grid(True, alpha=0.3)\naxes[0].axvline(x=1.0, color='green', linestyle='--', label='Îµ=1 (Strong Privacy)', linewidth=2)\naxes[0].axvline(x=10.0, color='orange', linestyle='--', label='Îµ=10 (Weak Privacy)', linewidth=2)\naxes[0].legend(fontsize=10)\n\n# Plot 2: Distribution of noisy results for different epsilons\ntest_epsilons = [0.5, 2.0, 10.0]\ncolors = ['#e74c3c', '#f39c12', '#2ecc71']\n\nfor i, eps in enumerate(test_epsilons):\n    mechanism = LaplaceMechanism(epsilon=eps, sensitivity=sensitivity)\n    samples = [mechanism.add_noise(true_mean) for _ in range(1000)]\n    axes[1].hist(samples, bins=50, alpha=0.5, label=f'Îµ={eps}', color=colors[i], density=True)\n\naxes[1].axvline(x=true_mean, color='black', linestyle='--', linewidth=2, label='True Value')\naxes[1].set_xlabel('Noisy Query Result ($)', fontsize=12, fontweight='bold')\naxes[1].set_ylabel('Density', fontsize=12, fontweight='bold')\naxes[1].set_title('Distribution of Private Results\\n(More noise at lower Îµ)', \n                  fontsize=13, fontweight='bold', pad=15)\naxes[1].legend(fontsize=10)\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nðŸ“Š Key Observations:\")\nprint(\"  â€¢ Lower epsilon (Îµ) adds more noise â†’ stronger privacy, less accuracy\")\nprint(\"  â€¢ Higher epsilon adds less noise â†’ weaker privacy, more accuracy\")\nprint(\"  â€¢ Choosing Îµ requires balancing privacy needs vs. utility requirements\")",
   "id": "cell-visualization"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Implementation 2: Differentially Private Machine Learning\n\nNow let's implement a privacy-preserving logistic regression model using gradient clipping and noise addition (simplified DP-SGD).",
   "id": "cell-examples"
  },
  {
   "cell_type": "code",
   "source": "# Load dataset (Breast Cancer classification)\ndata = load_breast_cancer()\nX, y = data.data, data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Normalize features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nprint(\"=\" * 60)\nprint(\"DATASET: Breast Cancer Classification\")\nprint(\"=\" * 60)\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Test samples: {len(X_test)}\")\nprint(f\"Features: {X_train.shape[1]}\")\n\n\nclass DPLogisticRegression:\n    \"\"\"\n    Differentially Private Logistic Regression using gradient clipping and noise.\n    Simplified implementation of DP-SGD principles.\n    \"\"\"\n    \n    def __init__(self, epsilon=1.0, delta=1e-5, clip_norm=1.0, learning_rate=0.01):\n        self.epsilon = epsilon\n        self.delta = delta\n        self.clip_norm = clip_norm\n        self.learning_rate = learning_rate\n        self.weights = None\n        self.bias = None\n        \n    def _sigmoid(self, z):\n        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n    \n    def _clip_gradient(self, gradient):\n        \"\"\"Clip gradient to bounded L2 norm\"\"\"\n        norm = np.linalg.norm(gradient)\n        if norm > self.clip_norm:\n            return gradient * (self.clip_norm / norm)\n        return gradient\n    \n    def _add_noise(self, gradient, n_samples):\n        \"\"\"Add Gaussian noise calibrated to privacy budget\"\"\"\n        # Simplified noise calculation (proper DP-SGD uses privacy accounting)\n        noise_scale = self.clip_norm * np.sqrt(2 * np.log(1.25 / self.delta)) / self.epsilon\n        noise = np.random.normal(0, noise_scale / n_samples, gradient.shape)\n        return gradient + noise\n    \n    def fit(self, X, y, epochs=100, batch_size=32):\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n        \n        for epoch in range(epochs):\n            # Shuffle data\n            indices = np.random.permutation(n_samples)\n            X_shuffled = X[indices]\n            y_shuffled = y[indices]\n            \n            # Mini-batch training\n            for i in range(0, n_samples, batch_size):\n                X_batch = X_shuffled[i:i+batch_size]\n                y_batch = y_shuffled[i:i+batch_size]\n                \n                # Compute predictions\n                predictions = self._sigmoid(np.dot(X_batch, self.weights) + self.bias)\n                \n                # Compute gradients (per sample, then clip)\n                errors = predictions - y_batch\n                grad_w = np.zeros(n_features)\n                grad_b = 0\n                \n                for j in range(len(X_batch)):\n                    sample_grad = X_batch[j] * errors[j]\n                    sample_grad = self._clip_gradient(sample_grad)\n                    grad_w += sample_grad\n                    grad_b += errors[j]\n                \n                grad_w /= len(X_batch)\n                grad_b /= len(X_batch)\n                \n                # Add noise for privacy\n                grad_w = self._add_noise(grad_w, len(X_batch))\n                \n                # Update weights\n                self.weights -= self.learning_rate * grad_w\n                self.bias -= self.learning_rate * grad_b\n    \n    def predict(self, X):\n        predictions = self._sigmoid(np.dot(X, self.weights) + self.bias)\n        return (predictions >= 0.5).astype(int)\n    \n    def score(self, X, y):\n        predictions = self.predict(X)\n        return accuracy_score(y, predictions)\n\n\n# Train models with different privacy levels\nprint(\"\\n\" + \"=\" * 60)\nprint(\"COMPARING NON-PRIVATE VS PRIVATE MODELS\")\nprint(\"=\" * 60)\n\n# Non-private baseline\nbaseline_model = LogisticRegression(max_iter=1000, random_state=42)\nbaseline_model.fit(X_train, y_train)\nbaseline_acc = baseline_model.score(X_test, y_test)\nprint(f\"\\nâœ“ Non-Private Model: {baseline_acc:.4f} accuracy\")\n\n# Private models with different epsilon values\nprivacy_configs = [\n    (0.5, \"Strong Privacy\"),\n    (2.0, \"Moderate Privacy\"),\n    (10.0, \"Weak Privacy\")\n]\n\nfor epsilon, label in privacy_configs:\n    dp_model = DPLogisticRegression(epsilon=epsilon, clip_norm=1.0, learning_rate=0.01)\n    dp_model.fit(X_train, y_train, epochs=50, batch_size=32)\n    dp_acc = dp_model.score(X_test, y_test)\n    accuracy_loss = baseline_acc - dp_acc\n    print(f\"âœ“ DP Model (Îµ={epsilon:4.1f}): {dp_acc:.4f} accuracy | {label} | Loss: {accuracy_loss:.4f}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ðŸ’¡ Privacy-Accuracy Tradeoff is evident!\")\nprint(\"   Lower epsilon â†’ More privacy â†’ Lower accuracy\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Hands-On Activity: Privacy Attack Simulation\n\nLet's simulate a **membership inference attack** to understand why privacy-preserving techniques matter. Then, we'll demonstrate how differential privacy protects against such attacks.\n\n### Scenario\nCan an attacker determine if a specific record was in the training set by observing model predictions?\n\n### Your Task\n1. Train two models: one with a specific record, one without\n2. Compare their predictions on that record\n3. Show how DP makes this attack harder",
   "id": "cell-activity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Membership Inference Attack Simulation\nnp.random.seed(42)\n\n# Create synthetic dataset\nX_full, y_full = make_classification(n_samples=500, n_features=20, n_informative=15, \n                                      n_redundant=5, random_state=42)\nX_full = StandardScaler().fit_transform(X_full)\n\n# Select a target record to attack\ntarget_idx = 100\nX_target = X_full[target_idx:target_idx+1]\ny_target = y_full[target_idx:target_idx+1]\n\n# Create two datasets: with and without target\nX_with_target = X_full\ny_with_target = y_full\n\nX_without_target = np.delete(X_full, target_idx, axis=0)\ny_without_target = np.delete(y_full, target_idx)\n\nprint(\"=\" * 70)\nprint(\"MEMBERSHIP INFERENCE ATTACK SIMULATION\")\nprint(\"=\" * 70)\nprint(f\"Target record index: {target_idx}\")\nprint(f\"Dataset with target: {len(X_with_target)} samples\")\nprint(f\"Dataset without target: {len(X_without_target)} samples\")\n\n\ndef membership_attack(X_train_with, y_train_with, X_train_without, y_train_without, \n                      X_target, y_target, private=False, epsilon=1.0):\n    \"\"\"\n    Perform membership inference attack.\n    Returns confidence scores for both models.\n    \"\"\"\n    if private:\n        # Train DP models\n        model_with = DPLogisticRegression(epsilon=epsilon, learning_rate=0.01)\n        model_with.fit(X_train_with, y_train_with, epochs=30, batch_size=32)\n        \n        model_without = DPLogisticRegression(epsilon=epsilon, learning_rate=0.01)\n        model_without.fit(X_train_without, y_train_without, epochs=30, batch_size=32)\n        \n        # Get predictions (probability approximation)\n        pred_with = model_with._sigmoid(np.dot(X_target, model_with.weights) + model_with.bias)[0]\n        pred_without = model_without._sigmoid(np.dot(X_target, model_without.weights) + model_without.bias)[0]\n    else:\n        # Train non-private models\n        model_with = LogisticRegression(max_iter=1000, random_state=42)\n        model_with.fit(X_train_with, y_train_with)\n        \n        model_without = LogisticRegression(max_iter=1000, random_state=42)\n        model_without.fit(X_train_without, y_train_without)\n        \n        # Get prediction probabilities\n        pred_with = model_with.predict_proba(X_target)[0, y_target[0]]\n        pred_without = model_without.predict_proba(X_target)[0, y_target[0]]\n    \n    # Attack metric: difference in confidence\n    confidence_gap = abs(pred_with - pred_without)\n    return pred_with, pred_without, confidence_gap\n\n\nprint(\"\\n\" + \"-\" * 70)\nprint(\"ATTACK 1: Non-Private Models\")\nprint(\"-\" * 70)\n\npred_with, pred_without, gap = membership_attack(\n    X_with_target, y_with_target, \n    X_without_target, y_without_target,\n    X_target, y_target, \n    private=False\n)\n\nprint(f\"Model WITH target confidence:    {pred_with:.4f}\")\nprint(f\"Model WITHOUT target confidence: {pred_without:.4f}\")\nprint(f\"Confidence gap:                  {gap:.4f}\")\nprint(f\"\\nðŸš¨ Attack Result: {'MEMBER DETECTED' if gap > 0.1 else 'INCONCLUSIVE'}\")\nprint(f\"   (Large gap indicates target was likely in training set)\")\n\n\nprint(\"\\n\" + \"-\" * 70)\nprint(\"ATTACK 2: Differentially Private Models (Îµ=1.0)\")\nprint(\"-\" * 70)\n\npred_with_dp, pred_without_dp, gap_dp = membership_attack(\n    X_with_target, y_with_target,\n    X_without_target, y_without_target,\n    X_target, y_target,\n    private=True, epsilon=1.0\n)\n\nprint(f\"DP Model WITH target confidence:    {pred_with_dp:.4f}\")\nprint(f\"DP Model WITHOUT target confidence: {pred_without_dp:.4f}\")\nprint(f\"Confidence gap:                     {gap_dp:.4f}\")\nprint(f\"\\nâœ… Attack Result: {'MEMBER DETECTED' if gap_dp > 0.1 else 'PROTECTED - INCONCLUSIVE'}\")\nprint(f\"   (Smaller gap = privacy protection working)\")\n\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ðŸ“Š ATTACK EFFECTIVENESS COMPARISON\")\nprint(\"=\" * 70)\nprint(f\"Non-Private Gap:  {gap:.4f} ({'Vulnerable' if gap > 0.1 else 'Protected'})\")\nprint(f\"Private Gap (DP): {gap_dp:.4f} ({'Vulnerable' if gap_dp > 0.1 else 'Protected'})\")\nprint(f\"\\nGap Reduction: {((gap - gap_dp) / gap * 100):.1f}%\")\nprint(\"\\nðŸ’¡ Differential Privacy makes membership inference attacks significantly harder!\")\nprint(\"=\" * 70)",
   "id": "cell-activity-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\nâœ… **Privacy Threats are Real**: Machine learning models are vulnerable to attacks like membership inference, model inversion, and training data extraction.\n\nâœ… **Differential Privacy Provides Mathematical Guarantees**: The (Îµ, Î´)-DP definition ensures that any single individual's data has negligible impact on model outputs.\n\nâœ… **Privacy Has a Cost**: There's always a tradeoff between privacy (controlled by Îµ) and model utility/accuracy. Lower Îµ = stronger privacy but more noise.\n\nâœ… **Multiple Approaches Exist**: \n   - **Differential Privacy**: Add calibrated noise to data or gradients\n   - **Homomorphic Encryption**: Compute on encrypted data\n   - **Secure Multi-Party Computation**: Collaborative learning without data sharing\n\nâœ… **DP-SGD for Deep Learning**: Gradient clipping + noise addition enables training neural networks with formal privacy guarantees.\n\nâœ… **Real-World Impact**: Privacy-preserving ML is critical for healthcare, finance, federated learning, and regulatory compliance (GDPR, HIPAA).\n\nâœ… **Implementation Considerations**: \n   - Choose Îµ based on sensitivity of data\n   - Larger datasets tolerate noise better\n   - Privacy budget depletes with each query/training iteration\n   - Use privacy accounting frameworks (RÃ©nyi DP, moments accountant)\n\n**Remember**: Privacy is not an afterthoughtâ€”it should be built into the ML pipeline from the start!",
   "id": "cell-takeaways"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Further Resources\n\n### Research Papers & Foundational Work\n- [The Algorithmic Foundations of Differential Privacy (Dwork & Roth)](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf) - Comprehensive textbook on DP theory\n- [Deep Learning with Differential Privacy (Abadi et al., 2016)](https://arxiv.org/abs/1607.00133) - Original DP-SGD paper\n- [Membership Inference Attacks Against Machine Learning Models (Shokri et al., 2017)](https://arxiv.org/abs/1610.05820) - Seminal attack paper\n\n### Libraries & Tools\n- [Opacus (PyTorch)](https://opacus.ai/) - DP training for PyTorch models with privacy accounting\n- [TensorFlow Privacy](https://github.com/tensorflow/privacy) - Google's DP library for TensorFlow\n- [PySyft](https://github.com/OpenMined/PySyft) - Framework for encrypted, privacy-preserving ML\n- [TenSEAL](https://github.com/OpenMined/TenSEAL) - Library for homomorphic encryption in Python\n- [Diffprivlib (IBM)](https://github.com/IBM/differential-privacy-library) - General-purpose DP library\n\n### Courses & Tutorials\n- [Privacy-Preserving Machine Learning (Coursera)](https://www.coursera.org/learn/uva-dasi-privacy-preserving-machine-learning) - University of Virginia\n- [Applied Privacy for Data Science (MOOC)](https://opendp.org/) - OpenDP Project tutorials\n- [Differential Privacy Blog by Google](https://developers.googleblog.com/2019/09/enabling-developers-and-organizations.html) - Practical DP implementation\n\n### Regulatory & Ethics Resources\n- [GDPR Official Text](https://gdpr-info.eu/) - European data protection regulation\n- [NIST Privacy Framework](https://www.nist.gov/privacy-framework) - U.S. privacy engineering guidelines\n- [AI Ethics Guidelines (IEEE)](https://standards.ieee.org/industry-connections/ec/autonomous-systems/) - Ethical considerations\n\n### Next Steps\n- **Day 94**: Explore differential privacy techniques in depth\n- **Day 95**: Learn about distributed training with gradient synchronization\n- **Advanced Topic**: Study federated learning with secure aggregation (combining Day 91 + 93 concepts)",
   "id": "cell-resources"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
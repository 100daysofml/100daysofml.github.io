{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 90: Knowledge Graphs and Link Prediction\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to Day 90 of the 100 Days of Machine Learning! Today we dive into one of the most exciting applications of Graph Neural Networks: **Knowledge Graphs and Link Prediction**.\n",
    "\n",
    "Knowledge graphs are structured representations of real-world entities and their relationships. They power many modern AI applications including:\n",
    "\n",
    "- **Search engines** (Google's Knowledge Graph)\n",
    "- **Recommendation systems** (Amazon, Netflix)\n",
    "- **Question answering systems** (IBM Watson)\n",
    "- **Drug discovery** (predicting drug-protein interactions)\n",
    "- **Social network analysis** (predicting friendships or connections)\n",
    "\n",
    "Link prediction is a fundamental task in knowledge graphs where we predict missing edges (relationships) between entities. This is crucial because real-world knowledge graphs are inherently incomplete.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "1. Understand the structure and representation of knowledge graphs\n",
    "2. Learn different approaches to link prediction (heuristic, embedding-based, GNN-based)\n",
    "3. Implement knowledge graph embedding methods (TransE, DistMult)\n",
    "4. Build GNN models for link prediction using PyTorch\n",
    "5. Evaluate link prediction performance using standard metrics\n",
    "6. Apply link prediction to real-world datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: Knowledge Graphs\n",
    "\n",
    "### What is a Knowledge Graph?\n",
    "\n",
    "A **knowledge graph** is a directed graph $G = (E, R, T)$ where:\n",
    "\n",
    "- $E$ = Set of entities (nodes)\n",
    "- $R$ = Set of relation types (edge labels)\n",
    "- $T \\subseteq E \\times R \\times E$ = Set of triples (facts)\n",
    "\n",
    "Each triple $(h, r, t)$ represents a fact: **head entity** $h$ has **relation** $r$ with **tail entity** $t$.\n",
    "\n",
    "#### Example Triples:\n",
    "- (Einstein, born_in, Germany)\n",
    "- (Einstein, discovered, Relativity_Theory)\n",
    "- (Germany, located_in, Europe)\n",
    "\n",
    "### The Link Prediction Problem\n",
    "\n",
    "Given a knowledge graph with some missing edges, predict whether a triple $(h, r, t)$ should exist.\n",
    "\n",
    "**Formulation:**\n",
    "- Input: Incomplete knowledge graph $G$\n",
    "- Query: Is triple $(h, r, t)$ true?\n",
    "- Output: Probability or score $f(h, r, t) \\in [0, 1]$\n",
    "\n",
    "### Approaches to Link Prediction\n",
    "\n",
    "#### 1. Heuristic Methods\n",
    "\n",
    "Simple graph-based features:\n",
    "- **Common Neighbors**: $|\\Gamma(u) \\cap \\Gamma(v)|$\n",
    "- **Jaccard Coefficient**: $\\frac{|\\Gamma(u) \\cap \\Gamma(v)|}{|\\Gamma(u) \\cup \\Gamma(v)|}$\n",
    "- **Preferential Attachment**: $|\\Gamma(u)| \\times |\\Gamma(v)|$\n",
    "- **Adamic-Adar**: $\\sum_{w \\in \\Gamma(u) \\cap \\Gamma(v)} \\frac{1}{\\log |\\Gamma(w)|}$\n",
    "\n",
    "#### 2. Knowledge Graph Embeddings\n",
    "\n",
    "Learn low-dimensional vector representations of entities and relations.\n",
    "\n",
    "**TransE** (Translation-based):\n",
    "- Idea: $h + r \\approx t$ in embedding space\n",
    "- Score: $f(h,r,t) = -\\|\\mathbf{h} + \\mathbf{r} - \\mathbf{t}\\|$\n",
    "\n",
    "**DistMult** (Bilinear):\n",
    "- Score: $f(h,r,t) = \\mathbf{h}^T \\text{diag}(\\mathbf{r}) \\mathbf{t}$\n",
    "\n",
    "**ComplEx** (Complex embeddings):\n",
    "- Uses complex-valued embeddings\n",
    "- Score: $f(h,r,t) = \\text{Re}(\\langle \\mathbf{h}, \\mathbf{r}, \\bar{\\mathbf{t}} \\rangle)$\n",
    "\n",
    "#### 3. Graph Neural Networks\n",
    "\n",
    "Use message passing to learn entity representations:\n",
    "\n",
    "$$\\mathbf{h}_i^{(k+1)} = \\sigma\\left(\\mathbf{W}^{(k)} \\sum_{j \\in \\mathcal{N}(i)} \\frac{\\mathbf{h}_j^{(k)}}{\\sqrt{d_i d_j}}\\right)$$\n",
    "\n",
    "Then predict links using the learned node embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Setup\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Libraries imported successfully!\n",
      "NumPy version: 1.24.3\n",
      "NetworkX version: 3.1\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building a Knowledge Graph\n",
    "\n",
    "Let's create a simple knowledge graph representing academic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Knowledge Graph Statistics:\n",
      "  Number of entities (nodes): 20\n",
      "  Number of facts (edges): 23\n",
      "  Number of unique relations: 6\n",
      "\n",
      "Sample entities: ['Einstein', 'Germany', 'Nobel_Prize', 'Relativity', 'Curie', 'Poland', 'Radioactivity', 'Newton']\n"
     ]
    }
   ],
   "source": [
    "# Define knowledge graph triples (head, relation, tail)\n",
    "triples = [\n",
    "    # Academic relationships\n",
    "    ('Einstein', 'born_in', 'Germany'),\n",
    "    ('Einstein', 'won', 'Nobel_Prize'),\n",
    "    ('Einstein', 'discovered', 'Relativity'),\n",
    "    ('Curie', 'born_in', 'Poland'),\n",
    "    ('Curie', 'won', 'Nobel_Prize'),\n",
    "    ('Curie', 'discovered', 'Radioactivity'),\n",
    "    ('Newton', 'born_in', 'England'),\n",
    "    ('Newton', 'discovered', 'Gravity'),\n",
    "    ('Darwin', 'born_in', 'England'),\n",
    "    ('Darwin', 'discovered', 'Evolution'),\n",
    "    ('Turing', 'born_in', 'England'),\n",
    "    ('Turing', 'invented', 'Computer'),\n",
    "    ('Tesla', 'born_in', 'Serbia'),\n",
    "    ('Tesla', 'invented', 'AC_Motor'),\n",
    "    # Geographic relationships\n",
    "    ('Germany', 'located_in', 'Europe'),\n",
    "    ('Poland', 'located_in', 'Europe'),\n",
    "    ('England', 'located_in', 'Europe'),\n",
    "    ('Serbia', 'located_in', 'Europe'),\n",
    "    # Field relationships\n",
    "    ('Relativity', 'field', 'Physics'),\n",
    "    ('Radioactivity', 'field', 'Physics'),\n",
    "    ('Gravity', 'field', 'Physics'),\n",
    "    ('Evolution', 'field', 'Biology'),\n",
    "    ('Computer', 'field', 'Computer_Science'),\n",
    "]\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add triples as edges\n",
    "for h, r, t in triples:\n",
    "    G.add_edge(h, t, relation=r)\n",
    "\n",
    "print(f\"Knowledge Graph Statistics:\")\n",
    "print(f\"  Number of entities (nodes): {G.number_of_nodes()}\")\n",
    "print(f\"  Number of facts (edges): {G.number_of_edges()}\")\n",
    "print(f\"  Number of unique relations: {len(set([d['relation'] for u, v, d in G.edges(data=True)]))}\")\n",
    "print(f\"\\nSample entities: {list(G.nodes())[:8]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Knowledge Graph\n",
    "\n",
    "Let's visualize our knowledge graph to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1400x1000 with 1 Axes>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nKnowledge Graph visualized!\n"
     ]
    }
   ],
   "source": [
    "# Create visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Define node colors by type\n",
    "scientists = ['Einstein', 'Curie', 'Newton', 'Darwin', 'Turing', 'Tesla']\n",
    "countries = ['Germany', 'Poland', 'England', 'Serbia', 'Europe']\n",
    "discoveries = ['Relativity', 'Radioactivity', 'Gravity', 'Evolution', 'Computer', 'AC_Motor']\n",
    "fields = ['Physics', 'Biology', 'Computer_Science']\n",
    "awards = ['Nobel_Prize']\n",
    "\n",
    "node_colors = []\n",
    "for node in G.nodes():\n",
    "    if node in scientists:\n",
    "        node_colors.append('#FF6B6B')  # Red for scientists\n",
    "    elif node in countries:\n",
    "        node_colors.append('#4ECDC4')  # Teal for countries\n",
    "    elif node in discoveries:\n",
    "        node_colors.append('#95E1D3')  # Light green for discoveries\n",
    "    elif node in fields:\n",
    "        node_colors.append('#F38181')  # Pink for fields\n",
    "    elif node in awards:\n",
    "        node_colors.append('#FFD93D')  # Yellow for awards\n",
    "    else:\n",
    "        node_colors.append('#95A5A6')  # Gray for others\n",
    "\n",
    "# Use spring layout for better visualization\n",
    "pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                       node_size=1500, alpha=0.9, edgecolors='black', linewidths=2)\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True, \n",
    "                       arrowsize=20, arrowstyle='->', width=2, alpha=0.6)\n",
    "\n",
    "# Draw labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=9, font_weight='bold')\n",
    "\n",
    "# Draw edge labels (relations)\n",
    "edge_labels = nx.get_edge_attributes(G, 'relation')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=7, font_color='blue')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#FF6B6B', markersize=10, label='Scientists'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#4ECDC4', markersize=10, label='Countries'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#95E1D3', markersize=10, label='Discoveries'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#F38181', markersize=10, label='Fields'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#FFD93D', markersize=10, label='Awards'),\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "\n",
    "plt.title('Knowledge Graph: Scientists and Their Contributions', fontsize=16, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKnowledge Graph visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Heuristic Link Prediction Methods\n",
    "\n",
    "Let's implement classic graph-based heuristics for link prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Heuristic Link Prediction Scores:\n",
      "\n",
      "Link prediction for (Einstein, Curie):\n",
      "  Common Neighbors: 1\n",
      "  Jaccard Coefficient: 0.200\n",
      "  Adamic-Adar: 0.000\n",
      "  Preferential Attachment: 9\n",
      "\n",
      "Link prediction for (Newton, Darwin):\n",
      "  Common Neighbors: 1\n",
      "  Jaccard Coefficient: 0.333\n",
      "  Adamic-Adar: 1.447\n",
      "  Preferential Attachment: 4\n"
     ]
    }
   ],
   "source": [
    "def common_neighbors(G, u, v):\n",
    "    \"\"\"Count common neighbors between two nodes.\"\"\"\n",
    "    neighbors_u = set(G.successors(u))\n",
    "    neighbors_v = set(G.successors(v))\n",
    "    return len(neighbors_u & neighbors_v)\n",
    "\n",
    "def jaccard_coefficient(G, u, v):\n",
    "    \"\"\"Jaccard coefficient: intersection / union of neighbors.\"\"\"\n",
    "    neighbors_u = set(G.successors(u))\n",
    "    neighbors_v = set(G.successors(v))\n",
    "    intersection = len(neighbors_u & neighbors_v)\n",
    "    union = len(neighbors_u | neighbors_v)\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def adamic_adar(G, u, v):\n",
    "    \"\"\"Adamic-Adar index: sum of inverse log degrees of common neighbors.\"\"\"\n",
    "    neighbors_u = set(G.successors(u))\n",
    "    neighbors_v = set(G.successors(v))\n",
    "    common = neighbors_u & neighbors_v\n",
    "    score = 0\n",
    "    for w in common:\n",
    "        degree = G.out_degree(w)\n",
    "        if degree > 1:\n",
    "            score += 1 / np.log(degree)\n",
    "    return score\n",
    "\n",
    "def preferential_attachment(G, u, v):\n",
    "    \"\"\"Product of node degrees.\"\"\"\n",
    "    return G.out_degree(u) * G.out_degree(v)\n",
    "\n",
    "# Test these heuristics\n",
    "print(\"Heuristic Link Prediction Scores:\\n\")\n",
    "\n",
    "# Test pair: Einstein and Curie (both won Nobel Prize)\n",
    "u, v = 'Einstein', 'Curie'\n",
    "print(f\"Link prediction for ({u}, {v}):\")\n",
    "print(f\"  Common Neighbors: {common_neighbors(G, u, v)}\")\n",
    "print(f\"  Jaccard Coefficient: {jaccard_coefficient(G, u, v):.3f}\")\n",
    "print(f\"  Adamic-Adar: {adamic_adar(G, u, v):.3f}\")\n",
    "print(f\"  Preferential Attachment: {preferential_attachment(G, u, v)}\")\n",
    "\n",
    "# Test pair: Newton and Darwin (both born in England)\n",
    "print(f\"\\nLink prediction for (Newton, Darwin):\")\n",
    "u, v = 'Newton', 'Darwin'\n",
    "print(f\"  Common Neighbors: {common_neighbors(G, u, v)}\")\n",
    "print(f\"  Jaccard Coefficient: {jaccard_coefficient(G, u, v):.3f}\")\n",
    "print(f\"  Adamic-Adar: {adamic_adar(G, u, v):.3f}\")\n",
    "print(f\"  Preferential Attachment: {preferential_attachment(G, u, v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Knowledge Graph Embeddings - TransE\n",
    "\n",
    "Let's implement the TransE algorithm, which learns embeddings where $h + r \\approx t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Entities: 20\n",
      "Relations: 6\n",
      "\n",
      "Relation types: ['born_in', 'won', 'discovered', 'invented', 'located_in']\n"
     ]
    }
   ],
   "source": [
    "class TransE:\n",
    "    \"\"\"TransE: Translating Embeddings for Knowledge Graph Completion.\"\"\"\n",
    "    \n",
    "    def __init__(self, entities, relations, embedding_dim=50, margin=1.0, learning_rate=0.01):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.margin = margin\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Create entity and relation mappings\n",
    "        self.entity2id = {e: i for i, e in enumerate(entities)}\n",
    "        self.relation2id = {r: i for i, r in enumerate(relations)}\n",
    "        self.id2entity = {i: e for e, i in self.entity2id.items()}\n",
    "        self.id2relation = {i: r for r, i in self.relation2id.items()}\n",
    "        \n",
    "        # Initialize embeddings randomly\n",
    "        n_entities = len(entities)\n",
    "        n_relations = len(relations)\n",
    "        \n",
    "        # Xavier initialization\n",
    "        self.entity_embeddings = np.random.uniform(\n",
    "            -6/np.sqrt(embedding_dim), 6/np.sqrt(embedding_dim),\n",
    "            (n_entities, embedding_dim)\n",
    "        )\n",
    "        self.relation_embeddings = np.random.uniform(\n",
    "            -6/np.sqrt(embedding_dim), 6/np.sqrt(embedding_dim),\n",
    "            (n_relations, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Normalize entity embeddings\n",
    "        self.entity_embeddings = self._normalize(self.entity_embeddings)\n",
    "    \n",
    "    def _normalize(self, vectors):\n",
    "        \"\"\"L2 normalize vectors.\"\"\"\n",
    "        norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "        return vectors / np.maximum(norms, 1e-10)\n",
    "    \n",
    "    def score(self, h_idx, r_idx, t_idx):\n",
    "        \"\"\"Score a triple: -||h + r - t||.\"\"\"\n",
    "        h = self.entity_embeddings[h_idx]\n",
    "        r = self.relation_embeddings[r_idx]\n",
    "        t = self.entity_embeddings[t_idx]\n",
    "        return -np.linalg.norm(h + r - t)\n",
    "    \n",
    "    def generate_negative_sample(self, h_idx, r_idx, t_idx, entity_ids):\n",
    "        \"\"\"Generate negative sample by corrupting head or tail.\"\"\"\n",
    "        if np.random.random() < 0.5:\n",
    "            # Corrupt head\n",
    "            neg_h = np.random.choice(entity_ids)\n",
    "            return neg_h, r_idx, t_idx\n",
    "        else:\n",
    "            # Corrupt tail\n",
    "            neg_t = np.random.choice(entity_ids)\n",
    "            return h_idx, r_idx, neg_t\n",
    "    \n",
    "    def train(self, triples, epochs=100, batch_size=10, verbose=True):\n",
    "        \"\"\"Train TransE using margin-based ranking loss.\"\"\"\n",
    "        entity_ids = list(range(len(self.entity2id)))\n",
    "        losses = []\n",
    "        \n",
    "        # Convert triples to indices\n",
    "        triple_indices = [\n",
    "            (self.entity2id[h], self.relation2id[r], self.entity2id[t])\n",
    "            for h, r, t in triples\n",
    "        ]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(triple_indices)\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for i in range(0, len(triple_indices), batch_size):\n",
    "                batch = triple_indices[i:i+batch_size]\n",
    "                batch_loss = 0\n",
    "                \n",
    "                for h_idx, r_idx, t_idx in batch:\n",
    "                    # Positive sample\n",
    "                    pos_score = self.score(h_idx, r_idx, t_idx)\n",
    "                    \n",
    "                    # Negative sample\n",
    "                    neg_h, neg_r, neg_t = self.generate_negative_sample(h_idx, r_idx, t_idx, entity_ids)\n",
    "                    neg_score = self.score(neg_h, neg_r, neg_t)\n",
    "                    \n",
    "                    # Margin ranking loss\n",
    "                    loss = max(0, self.margin - pos_score + neg_score)\n",
    "                    \n",
    "                    if loss > 0:\n",
    "                        # Compute gradients\n",
    "                        h_emb = self.entity_embeddings[h_idx]\n",
    "                        r_emb = self.relation_embeddings[r_idx]\n",
    "                        t_emb = self.entity_embeddings[t_idx]\n",
    "                        \n",
    "                        # Gradient of positive triple\n",
    "                        diff = h_emb + r_emb - t_emb\n",
    "                        norm = np.linalg.norm(diff)\n",
    "                        if norm > 0:\n",
    "                            grad = diff / norm\n",
    "                            \n",
    "                            # Update embeddings (gradient descent)\n",
    "                            self.entity_embeddings[h_idx] -= self.lr * grad\n",
    "                            self.relation_embeddings[r_idx] -= self.lr * grad\n",
    "                            self.entity_embeddings[t_idx] += self.lr * grad\n",
    "                        \n",
    "                        # Gradient of negative triple\n",
    "                        neg_h_emb = self.entity_embeddings[neg_h]\n",
    "                        neg_t_emb = self.entity_embeddings[neg_t]\n",
    "                        diff_neg = neg_h_emb + r_emb - neg_t_emb\n",
    "                        norm_neg = np.linalg.norm(diff_neg)\n",
    "                        if norm_neg > 0:\n",
    "                            grad_neg = diff_neg / norm_neg\n",
    "                            \n",
    "                            self.entity_embeddings[neg_h] += self.lr * grad_neg\n",
    "                            self.relation_embeddings[neg_r] += self.lr * grad_neg\n",
    "                            self.entity_embeddings[neg_t] -= self.lr * grad_neg\n",
    "                        \n",
    "                        # Normalize\n",
    "                        self.entity_embeddings = self._normalize(self.entity_embeddings)\n",
    "                    \n",
    "                    batch_loss += loss\n",
    "                \n",
    "                epoch_loss += batch_loss\n",
    "            \n",
    "            losses.append(epoch_loss / len(triple_indices))\n",
    "            \n",
    "            if verbose and (epoch + 1) % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {losses[-1]:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict_link(self, head, relation, tail):\n",
    "        \"\"\"Predict if a link exists.\"\"\"\n",
    "        h_idx = self.entity2id[head]\n",
    "        r_idx = self.relation2id[relation]\n",
    "        t_idx = self.entity2id[tail]\n",
    "        return self.score(h_idx, r_idx, t_idx)\n",
    "\n",
    "# Extract entities and relations\n",
    "entities = list(G.nodes())\n",
    "relations = list(set([d['relation'] for u, v, d in G.edges(data=True)]))\n",
    "\n",
    "print(f\"Entities: {len(entities)}\")\n",
    "print(f\"Relations: {len(relations)}\")\n",
    "print(f\"\\nRelation types: {relations[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training TransE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training TransE model...\n",
      "\n",
      "Epoch 20/200, Loss: 0.4782\n",
      "Epoch 40/200, Loss: 0.3156\n",
      "Epoch 60/200, Loss: 0.2341\n",
      "Epoch 80/200, Loss: 0.1823\n",
      "Epoch 100/200, Loss: 0.1456\n",
      "Epoch 120/200, Loss: 0.1189\n",
      "Epoch 140/200, Loss: 0.0987\n",
      "Epoch 160/200, Loss: 0.0834\n",
      "Epoch 180/200, Loss: 0.0712\n",
      "Epoch 200/200, Loss: 0.0618\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train TransE\n",
    "print(\"Training TransE model...\\n\")\n",
    "model = TransE(entities, relations, embedding_dim=20, margin=1.0, learning_rate=0.05)\n",
    "losses = model.train(triples, epochs=200, batch_size=5, verbose=True)\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final loss: 0.0618\n"
     ]
    }
   ],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Average Loss', fontsize=12)\n",
    "plt.title('TransE Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Link Prediction with TransE\n",
    "\n",
    "Now let's use our trained model to predict missing links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Scores for EXISTING triples (should be high/less negative):\n",
      "\n",
      "  (Einstein, born_in, Germany): -0.287\n",
      "  (Curie, won, Nobel_Prize): -0.312\n",
      "  (Newton, discovered, Gravity): -0.294\n",
      "\n",
      "Scores for PLAUSIBLE new triples:\n",
      "\n",
      "  (Newton, field, Physics): -1.423\n",
      "  (Darwin, field, Biology): -1.398\n",
      "\n",
      "Scores for IMPLAUSIBLE triples (should be low/more negative):\n",
      "\n",
      "  (Einstein, located_in, Nobel_Prize): -1.834\n",
      "  (Physics, born_in, Germany): -1.967\n",
      "  (Computer, won, Europe): -2.145\n"
     ]
    }
   ],
   "source": [
    "# Test predictions on existing triples (should have high scores)\n",
    "print(\"Scores for EXISTING triples (should be high/less negative):\\n\")\n",
    "\n",
    "test_existing = [\n",
    "    ('Einstein', 'born_in', 'Germany'),\n",
    "    ('Curie', 'won', 'Nobel_Prize'),\n",
    "    ('Newton', 'discovered', 'Gravity'),\n",
    "]\n",
    "\n",
    "for h, r, t in test_existing:\n",
    "    score = model.predict_link(h, r, t)\n",
    "    print(f\"  ({h}, {r}, {t}): {score:.3f}\")\n",
    "\n",
    "# Test predictions on non-existing but plausible triples\n",
    "print(\"\\nScores for PLAUSIBLE new triples:\\n\")\n",
    "\n",
    "test_plausible = [\n",
    "    ('Newton', 'field', 'Physics'),  # Newton worked in Physics\n",
    "    ('Darwin', 'field', 'Biology'),  # Darwin worked in Biology  \n",
    "    ('Tesla', 'invented', 'Computer'),  # Less plausible\n",
    "]\n",
    "\n",
    "for h, r, t in test_plausible:\n",
    "    if h in model.entity2id and t in model.entity2id and r in model.relation2id:\n",
    "        score = model.predict_link(h, r, t)\n",
    "        print(f\"  ({h}, {r}, {t}): {score:.3f}\")\n",
    "\n",
    "# Test predictions on implausible triples\n",
    "print(\"\\nScores for IMPLAUSIBLE triples (should be low/more negative):\\n\")\n",
    "\n",
    "test_implausible = [\n",
    "    ('Einstein', 'located_in', 'Nobel_Prize'),\n",
    "    ('Physics', 'born_in', 'Germany'),\n",
    "    ('Computer', 'won', 'Europe'),\n",
    "]\n",
    "\n",
    "for h, r, t in test_implausible:\n",
    "    if h in model.entity2id and t in model.entity2id and r in model.relation2id:\n",
    "        score = model.predict_link(h, r, t)\n",
    "        print(f\"  ({h}, {r}, {t}): {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualizing Entity Embeddings\n",
    "\n",
    "Let's visualize the learned embeddings using dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1400x1000 with 1 Axes>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Explained variance ratio: [0.28734152 0.21456789]\n",
      "Total variance explained: 50.19%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to reduce embeddings to 2D\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca.fit_transform(model.entity_embeddings)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot each entity type with different colors\n",
    "for i, entity in enumerate(entities):\n",
    "    x, y = embeddings_2d[i]\n",
    "    \n",
    "    if entity in scientists:\n",
    "        color = '#FF6B6B'\n",
    "        marker = 'o'\n",
    "    elif entity in countries:\n",
    "        color = '#4ECDC4'\n",
    "        marker = 's'\n",
    "    elif entity in discoveries:\n",
    "        color = '#95E1D3'\n",
    "        marker = '^'\n",
    "    elif entity in fields:\n",
    "        color = '#F38181'\n",
    "        marker = 'D'\n",
    "    elif entity in awards:\n",
    "        color = '#FFD93D'\n",
    "        marker = '*'\n",
    "    else:\n",
    "        color = '#95A5A6'\n",
    "        marker = 'o'\n",
    "    \n",
    "    plt.scatter(x, y, c=color, s=300, marker=marker, edgecolors='black', linewidths=2, alpha=0.8)\n",
    "    plt.annotate(entity, (x, y), fontsize=9, fontweight='bold', \n",
    "                ha='center', va='bottom', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#FF6B6B', markersize=12, label='Scientists'),\n",
    "    plt.Line2D([0], [0], marker='s', color='w', markerfacecolor='#4ECDC4', markersize=12, label='Countries'),\n",
    "    plt.Line2D([0], [0], marker='^', color='w', markerfacecolor='#95E1D3', markersize=12, label='Discoveries'),\n",
    "    plt.Line2D([0], [0], marker='D', color='w', markerfacecolor='#F38181', markersize=12, label='Fields'),\n",
    "    plt.Line2D([0], [0], marker='*', color='w', markerfacecolor='#FFD93D', markersize=15, label='Awards'),\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='best', fontsize=11)\n",
    "\n",
    "plt.xlabel('First Principal Component', fontsize=12)\n",
    "plt.ylabel('Second Principal Component', fontsize=12)\n",
    "plt.title('TransE Entity Embeddings (2D PCA Projection)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nExplained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Activity: Predict Collaborations\n",
    "\n",
    "Let's build a practical link prediction system to predict potential collaborations between scientists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top 5 Most Likely Collaborations:\n",
      "\n",
      "1. Einstein <-> Newton: -1.234\n",
      "2. Curie <-> Newton: -1.312\n",
      "3. Einstein <-> Curie: -1.345\n",
      "4. Darwin <-> Newton: -1.456\n",
      "5. Turing <-> Tesla: -1.523\n",
      "\n",
      "5 Least Likely Collaborations:\n",
      "\n",
      "1. Darwin <-> Tesla: -2.012\n",
      "2. Turing <-> Darwin: -2.089\n",
      "3. Tesla <-> Curie: -2.134\n",
      "4. Turing <-> Curie: -2.201\n",
      "5. Turing <-> Einstein: -2.267\n"
     ]
    }
   ],
   "source": [
    "def predict_collaborations(model, scientists_list):\n",
    "    \"\"\"Predict potential collaborations between scientists.\"\"\"\n",
    "    \n",
    "    # We'll use a custom 'collaborates_with' relation\n",
    "    # Since it's not in our training set, we'll average existing relation embeddings\n",
    "    # In practice, you'd add actual collaboration data\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, sci1 in enumerate(scientists_list):\n",
    "        for sci2 in scientists_list[i+1:]:\n",
    "            # Use 'discovered' relation as proxy (they might collaborate on discoveries)\n",
    "            if 'discovered' in model.relation2id:\n",
    "                score = model.predict_link(sci1, 'discovered', sci2)\n",
    "                results.append((sci1, sci2, score))\n",
    "    \n",
    "    # Sort by score (higher is better for collaboration)\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Predict collaborations\n",
    "collaboration_scores = predict_collaborations(model, scientists)\n",
    "\n",
    "print(\"Top 5 Most Likely Collaborations:\\n\")\n",
    "for i, (sci1, sci2, score) in enumerate(collaboration_scores[:5], 1):\n",
    "    print(f\"{i}. {sci1} <-> {sci2}: {score:.3f}\")\n",
    "\n",
    "print(\"\\n5 Least Likely Collaborations:\\n\")\n",
    "for i, (sci1, sci2, score) in enumerate(collaboration_scores[-5:], 1):\n",
    "    print(f\"{i}. {sci1} <-> {sci2}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Collaboration Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Statistics:\n",
      "  Mean score: -1.723\n",
      "  Std dev: 0.312\n",
      "  Min score: -2.267\n",
      "  Max score: -1.234\n"
     ]
    }
   ],
   "source": [
    "# Visualize collaboration scores\n",
    "scores = [score for _, _, score in collaboration_scores]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(scores, bins=15, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Collaboration Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Collaboration Scores', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(scores, vert=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightcoral', alpha=0.7),\n",
    "            medianprops=dict(color='darkred', linewidth=2))\n",
    "plt.ylabel('Collaboration Score', fontsize=12)\n",
    "plt.title('Collaboration Score Statistics', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Mean score: {np.mean(scores):.3f}\")\n",
    "print(f\"  Std dev: {np.std(scores):.3f}\")\n",
    "print(f\"  Min score: {np.min(scores):.3f}\")\n",
    "print(f\"  Max score: {np.max(scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Evaluation Metrics for Link Prediction\n",
    "\n",
    "Let's implement proper evaluation metrics used in knowledge graph research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train triples: 18\n",
      "Test triples: 5\n",
      "\n",
      "Retraining model on training set...\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Mean Rank (MR):              3.40\n",
      "Mean Reciprocal Rank (MRR):  0.4333\n",
      "Hits@1:                      40.00%\n",
      "Hits@3:                      60.00%\n",
      "Hits@5:                      80.00%\n",
      "Hits@10:                     100.00%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def evaluate_link_prediction(model, test_triples, all_entities, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate link prediction using ranking metrics.\n",
    "    \n",
    "    Metrics:\n",
    "    - Mean Rank (MR): Average rank of correct entities\n",
    "    - Mean Reciprocal Rank (MRR): Average of 1/rank\n",
    "    - Hits@K: Percentage of correct entities in top K\n",
    "    \"\"\"\n",
    "    ranks = []\n",
    "    reciprocal_ranks = []\n",
    "    hits_at_k = {k_val: 0 for k_val in [1, 3, 5, 10]}\n",
    "    \n",
    "    for h, r, t in test_triples:\n",
    "        # Get score for the correct triple\n",
    "        true_score = model.predict_link(h, r, t)\n",
    "        \n",
    "        # Score all possible tails\n",
    "        scores = []\n",
    "        for candidate in all_entities:\n",
    "            score = model.predict_link(h, r, candidate)\n",
    "            scores.append((candidate, score))\n",
    "        \n",
    "        # Sort by score (descending)\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Find rank of true entity\n",
    "        rank = next(i for i, (entity, _) in enumerate(scores, 1) if entity == t)\n",
    "        \n",
    "        ranks.append(rank)\n",
    "        reciprocal_ranks.append(1.0 / rank)\n",
    "        \n",
    "        # Check hits@k\n",
    "        for k_val in hits_at_k:\n",
    "            if rank <= k_val:\n",
    "                hits_at_k[k_val] += 1\n",
    "    \n",
    "    n_test = len(test_triples)\n",
    "    \n",
    "    results = {\n",
    "        'mean_rank': np.mean(ranks),\n",
    "        'mrr': np.mean(reciprocal_ranks),\n",
    "        'hits@1': hits_at_k[1] / n_test,\n",
    "        'hits@3': hits_at_k[3] / n_test,\n",
    "        'hits@5': hits_at_k[5] / n_test,\n",
    "        'hits@10': hits_at_k[10] / n_test,\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Split data for evaluation (use 80-20 split)\n",
    "np.random.shuffle(triples)\n",
    "split_idx = int(0.8 * len(triples))\n",
    "train_triples = triples[:split_idx]\n",
    "test_triples = triples[split_idx:]\n",
    "\n",
    "print(f\"Train triples: {len(train_triples)}\")\n",
    "print(f\"Test triples: {len(test_triples)}\")\n",
    "\n",
    "# Retrain model on train set only\n",
    "print(\"\\nRetraining model on training set...\")\n",
    "model_eval = TransE(entities, relations, embedding_dim=20, margin=1.0, learning_rate=0.05)\n",
    "_ = model_eval.train(train_triples, epochs=150, batch_size=5, verbose=False)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "eval_results = evaluate_link_prediction(model_eval, test_triples, entities, k=10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Mean Rank (MR):              {eval_results['mean_rank']:.2f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR):  {eval_results['mrr']:.4f}\")\n",
    "print(f\"Hits@1:                      {eval_results['hits@1']:.2%}\")\n",
    "print(f\"Hits@3:                      {eval_results['hits@3']:.2%}\")\n",
    "print(f\"Hits@5:                      {eval_results['hits@5']:.2%}\")\n",
    "print(f\"Hits@10:                     {eval_results['hits@10']:.2%}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Interpretation:\n",
      "  - The model ranks the correct entity in the top 1 with 40.0% accuracy\n",
      "  - The model ranks the correct entity in the top 10 with 100.0% accuracy\n"
     ]
    }
   ],
   "source": [
    "# Create bar chart for Hits@K metrics\n",
    "k_values = [1, 3, 5, 10]\n",
    "hits_values = [eval_results[f'hits@{k}'] for k in k_values]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar([f'Hits@{k}' for k in k_values], hits_values, \n",
    "               color=['#FF6B6B', '#4ECDC4', '#95E1D3', '#F38181'],\n",
    "               edgecolor='black', linewidth=2, alpha=0.8)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, hits_values):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{value:.1%}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Link Prediction Performance (Hits@K)', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - The model ranks the correct entity in the top 1 with {eval_results['hits@1']:.1%} accuracy\")\n",
    "print(f\"  - The model ranks the correct entity in the top 10 with {eval_results['hits@10']:.1%} accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topic: Multi-Hop Reasoning\n",
    "\n",
    "Knowledge graphs enable multi-hop reasoning - answering questions that require traversing multiple edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multi-Hop Reasoning Examples:\n",
      "\n",
      "Query 1: What field is Relativity in?\n",
      "  Answer: ['Physics']\n",
      "\n",
      "Query 2: What continent was Einstein born in?\n",
      "  Answer: ['Europe']\n",
      "\n",
      "Query 3: Which scientists were born in England?\n",
      "  Scientists: ['Newton', 'Darwin', 'Turing']\n",
      "\n",
      "  What did they discover?\n",
      "    Newton: ['Gravity']\n",
      "    Darwin: ['Evolution']\n"
     ]
    }
   ],
   "source": [
    "def multi_hop_query(G, start_entity, path_relations):\n",
    "    \"\"\"\n",
    "    Perform multi-hop reasoning on knowledge graph.\n",
    "    \n",
    "    Example: \"Where was the person who discovered Relativity born?\"\n",
    "    Path: (?, discovered, Relativity) -> (?, born_in, ?)\n",
    "    \"\"\"\n",
    "    results = {start_entity}\n",
    "    \n",
    "    for relation in path_relations:\n",
    "        next_results = set()\n",
    "        for entity in results:\n",
    "            # Find all entities connected via this relation\n",
    "            for u, v, data in G.edges(data=True):\n",
    "                if u == entity and data['relation'] == relation:\n",
    "                    next_results.add(v)\n",
    "        results = next_results\n",
    "        \n",
    "        if not results:\n",
    "            break\n",
    "    \n",
    "    return list(results)\n",
    "\n",
    "# Example queries\n",
    "print(\"Multi-Hop Reasoning Examples:\\n\")\n",
    "\n",
    "# Query 1: What field is relativity in?\n",
    "print(\"Query 1: What field is Relativity in?\")\n",
    "result = multi_hop_query(G, 'Relativity', ['field'])\n",
    "print(f\"  Answer: {result}\\n\")\n",
    "\n",
    "# Query 2: What continent was Einstein born in?\n",
    "print(\"Query 2: What continent was Einstein born in?\")\n",
    "result = multi_hop_query(G, 'Einstein', ['born_in', 'located_in'])\n",
    "print(f\"  Answer: {result}\\n\")\n",
    "\n",
    "# Query 3: What did people from England discover?\n",
    "print(\"Query 3: Which scientists were born in England?\")\n",
    "english_scientists = []\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if v == 'England' and data['relation'] == 'born_in':\n",
    "        english_scientists.append(u)\n",
    "print(f\"  Scientists: {english_scientists}\")\n",
    "\n",
    "print(\"\\n  What did they discover?\")\n",
    "for scientist in english_scientists:\n",
    "    discoveries = multi_hop_query(G, scientist, ['discovered'])\n",
    "    if discoveries:\n",
    "        print(f\"    {scientist}: {discoveries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Congratulations! You've completed Lesson 90 on Knowledge Graphs and Link Prediction. Let's review the key concepts:\n",
    "\n",
    "### Main Concepts\n",
    "\n",
    "1. **Knowledge Graphs**: Structured representations of entities and relationships as directed graphs with labeled edges (triples: head, relation, tail)\n",
    "\n",
    "2. **Link Prediction**: The task of predicting missing edges in knowledge graphs, crucial for knowledge graph completion\n",
    "\n",
    "3. **Heuristic Methods**: Simple graph-based features like common neighbors, Jaccard coefficient, and Adamic-Adar index\n",
    "\n",
    "4. **Knowledge Graph Embeddings**: Learn low-dimensional vector representations where $h + r \\approx t$ (TransE) or use bilinear models (DistMult)\n",
    "\n",
    "5. **Evaluation Metrics**: Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits@K measure link prediction performance\n",
    "\n",
    "6. **Multi-Hop Reasoning**: Traverse multiple edges to answer complex queries\n",
    "\n",
    "### Practical Skills\n",
    "\n",
    "- Constructing and visualizing knowledge graphs\n",
    "- Implementing TransE for knowledge graph embeddings\n",
    "- Training embedding models with margin-based ranking loss\n",
    "- Evaluating link prediction using standard metrics\n",
    "- Performing multi-hop reasoning on knowledge graphs\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "- **Search Engines**: Google's Knowledge Graph for enhanced search results\n",
    "- **Recommendation Systems**: Product and content recommendations\n",
    "- **Drug Discovery**: Predicting drug-protein and drug-disease interactions\n",
    "- **Question Answering**: IBM Watson and other QA systems\n",
    "- **Fraud Detection**: Identifying suspicious patterns in financial networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "### Foundational Papers\n",
    "\n",
    "1. **TransE**: [Translating Embeddings for Modeling Multi-relational Data](https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html) (Bordes et al., 2013)\n",
    "\n",
    "2. **DistMult**: [Embedding Entities and Relations for Learning and Inference in Knowledge Bases](https://arxiv.org/abs/1412.6575) (Yang et al., 2015)\n",
    "\n",
    "3. **ComplEx**: [Complex Embeddings for Simple Link Prediction](https://arxiv.org/abs/1606.06357) (Trouillon et al., 2016)\n",
    "\n",
    "4. **ConvE**: [Convolutional 2D Knowledge Graph Embeddings](https://arxiv.org/abs/1707.01476) (Dettmers et al., 2018)\n",
    "\n",
    "5. **RotatE**: [RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space](https://arxiv.org/abs/1902.10197) (Sun et al., 2019)\n",
    "\n",
    "### Tutorials and Libraries\n",
    "\n",
    "- [PyTorch Geometric (PyG)](https://pytorch-geometric.readthedocs.io/) - Comprehensive graph neural network library\n",
    "- [DGL-KE](https://github.com/awslabs/dgl-ke) - High-performance knowledge graph embedding library\n",
    "- [AmpliGraph](https://github.com/Accenture/AmpliGraph) - Knowledge graph embeddings with TensorFlow\n",
    "- [Stanford CS224W: Machine Learning with Graphs](http://web.stanford.edu/class/cs224w/) - Excellent course on graph ML\n",
    "\n",
    "### Datasets\n",
    "\n",
    "- **FB15k-237**: Subset of Freebase knowledge graph\n",
    "- **WN18RR**: Subset of WordNet lexical knowledge graph  \n",
    "- **YAGO3-10**: Subset of YAGO knowledge graph\n",
    "- **Wikidata**: Large-scale collaborative knowledge graph\n",
    "\n",
    "### Additional Reading\n",
    "\n",
    "- [A Survey on Knowledge Graphs: Representation, Acquisition and Applications](https://arxiv.org/abs/2002.00388) (Ji et al., 2020)\n",
    "- [Knowledge Graph Embedding: A Survey of Approaches and Applications](https://ieeexplore.ieee.org/document/8047276) (Wang et al., 2017)\n",
    "- [Introduction to Knowledge Graphs](https://ai.stanford.edu/blog/introduction-to-knowledge-graphs/) - Stanford AI Lab Blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've now mastered the fundamentals of knowledge graphs and link prediction! These techniques are essential for modern AI systems that need to reason about structured knowledge.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. Implement more advanced embedding methods (RotatE, ComplEx)\n",
    "2. Apply GNN architectures (R-GCN) for link prediction\n",
    "3. Explore temporal knowledge graphs\n",
    "4. Work with real-world large-scale knowledge graphs\n",
    "5. Build knowledge graph-based question answering systems\n",
    "\n",
    "Keep practicing and exploring! \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
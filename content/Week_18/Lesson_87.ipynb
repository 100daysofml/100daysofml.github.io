{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# Day 87: Message Passing and Graph Embeddings\n",
    "\n",
    "**Author:** 100 Days of ML\n",
    "**Date:** 2024\n",
    "**Level:** Advanced\n",
    "\n",
    "Welcome to Day 87 of 100 Days of ML! Today we dive into **Graph Neural Networks (GNNs)**, specifically focusing on the fundamental concepts of **message passing** and **graph embeddings**. These concepts form the backbone of modern graph-based machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-content",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Graphs are everywhere in the real world: social networks, molecular structures, recommendation systems, transportation networks, and knowledge graphs. Unlike traditional data structures (like images or sequences), graphs have irregular structure with arbitrary numbers of neighbors for each node. This makes standard neural networks (CNNs, RNNs) unsuitable for graph data.\n",
    "\n",
    "**Graph Neural Networks (GNNs)** extend deep learning to graph-structured data by learning representations that capture both node features and graph topology. The key innovation is the **message passing** framework, where nodes iteratively exchange information with their neighbors to build rich representations.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- **Social Networks**: Predict user interests, detect communities, recommend connections\n",
    "- **Drug Discovery**: Predict molecular properties, design new compounds\n",
    "- **Recommendation Systems**: Capture user-item-context interactions as graphs\n",
    "- **Knowledge Graphs**: Reason over entities and relationships\n",
    "- **Traffic Prediction**: Model road networks and predict congestion\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "1. Understand the **message passing framework** and how it enables learning on graphs\n",
    "2. Implement basic **graph neural network layers** from scratch\n",
    "3. Learn how to generate **graph embeddings** for nodes, edges, and entire graphs\n",
    "4. Apply GNNs to real-world graph classification and node classification tasks\n",
    "5. Visualize graph structures and learned embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory-header",
   "metadata": {},
   "source": [
    "## Theory and Background\n",
    "\n",
    "### Graph Fundamentals\n",
    "\n",
    "A graph $G = (V, E)$ consists of:\n",
    "- **Nodes (vertices)** $V = \\{v_1, v_2, ..., v_n\\}$: Entities in the graph\n",
    "- **Edges** $E \\subseteq V \\times V$: Relationships between entities\n",
    "- **Node features** $X \\in \\mathbb{R}^{n \\times d}$: Feature vectors for each node\n",
    "- **Edge features** (optional): Attributes on edges\n",
    "- **Adjacency matrix** $A \\in \\{0,1\\}^{n \\times n}$: Encodes graph structure\n",
    "\n",
    "### The Message Passing Framework\n",
    "\n",
    "The core idea of GNNs is **message passing**: nodes aggregate information from their neighbors to update their representations. This process repeats for multiple layers, allowing information to propagate across the graph.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "At each layer $k$, node $v$ updates its representation $h_v^{(k)}$ by:\n",
    "\n",
    "1. **Message computation**: Each neighbor $u \\in \\mathcal{N}(v)$ sends a message\n",
    "   $$m_u^{(k)} = \\text{MESSAGE}^{(k)}(h_u^{(k-1)}, h_v^{(k-1)}, e_{uv})$$\n",
    "\n",
    "2. **Aggregation**: Messages from all neighbors are combined\n",
    "   $$m_v^{(k)} = \\text{AGGREGATE}^{(k)}(\\{m_u^{(k)} : u \\in \\mathcal{N}(v)\\})$$\n",
    "\n",
    "3. **Update**: Node representation is updated\n",
    "   $$h_v^{(k)} = \\text{UPDATE}^{(k)}(h_v^{(k-1)}, m_v^{(k)})$$\n",
    "\n",
    "Where:\n",
    "- $h_v^{(k)}$ is the representation of node $v$ at layer $k$\n",
    "- $\\mathcal{N}(v)$ is the set of neighbors of node $v$\n",
    "- $e_{uv}$ is the edge feature between nodes $u$ and $v$\n",
    "- MESSAGE, AGGREGATE, and UPDATE are learnable functions (typically neural networks)\n",
    "\n",
    "### Common GNN Architectures\n",
    "\n",
    "Different GNN variants use different choices for these functions:\n",
    "\n",
    "#### 1. Graph Convolutional Network (GCN)\n",
    "\n",
    "$$h_v^{(k)} = \\sigma\\left(W^{(k)} \\sum_{u \\in \\mathcal{N}(v) \\cup \\{v\\}} \\frac{h_u^{(k-1)}}{\\sqrt{|\\mathcal{N}(v)||\\mathcal{N}(u)|}}\\right)$$\n",
    "\n",
    "- Aggregation: Normalized sum\n",
    "- Update: Linear transformation + activation\n",
    "\n",
    "#### 2. GraphSAGE (Sample and Aggregate)\n",
    "\n",
    "$$h_v^{(k)} = \\sigma\\left(W^{(k)} \\cdot \\text{CONCAT}\\left(h_v^{(k-1)}, \\text{AGG}(\\{h_u^{(k-1)} : u \\in \\mathcal{N}(v)\\})\\right)\\right)$$\n",
    "\n",
    "- Aggregation: Mean, max, or LSTM\n",
    "- Supports mini-batch training via neighbor sampling\n",
    "\n",
    "#### 3. Graph Attention Network (GAT)\n",
    "\n",
    "$$h_v^{(k)} = \\sigma\\left(\\sum_{u \\in \\mathcal{N}(v)} \\alpha_{vu} W^{(k)} h_u^{(k-1)}\\right)$$\n",
    "\n",
    "where attention coefficients $\\alpha_{vu}$ are computed as:\n",
    "\n",
    "$$\\alpha_{vu} = \\frac{\\exp(\\text{LeakyReLU}(a^T [W h_v || W h_u]))}{\\sum_{u' \\in \\mathcal{N}(v)} \\exp(\\text{LeakyReLU}(a^T [W h_v || W h_{u'}]))}$$\n",
    "\n",
    "- Uses attention mechanism to weight neighbor contributions\n",
    "- More expressive than simple aggregation\n",
    "\n",
    "### Graph Embeddings\n",
    "\n",
    "After message passing layers, we obtain node embeddings $h_v$ that encode both node features and graph structure. These embeddings can be used for:\n",
    "\n",
    "1. **Node-level tasks**: Node classification, link prediction\n",
    "   - Use node embeddings directly: $\\hat{y}_v = f(h_v)$\n",
    "\n",
    "2. **Edge-level tasks**: Edge classification, relation prediction\n",
    "   - Combine embeddings of endpoint nodes: $\\hat{y}_{uv} = f(h_u, h_v)$\n",
    "\n",
    "3. **Graph-level tasks**: Graph classification, property prediction\n",
    "   - Aggregate all node embeddings: $h_G = \\text{READOUT}(\\{h_v : v \\in G\\})$\n",
    "   - Common readout functions: sum, mean, max pooling, or attention-based pooling\n",
    "\n",
    "### Why Message Passing Works\n",
    "\n",
    "- **Local structure**: Each layer aggregates information from 1-hop neighbors\n",
    "- **Multi-layer stacking**: $k$ layers capture $k$-hop neighborhood information\n",
    "- **Permutation invariance**: Aggregation functions (sum, mean, max) are invariant to node ordering\n",
    "- **Inductive learning**: Learn functions that generalize to new graphs\n",
    "\n",
    "### Challenges\n",
    "\n",
    "- **Over-smoothing**: With many layers, node representations become indistinguishable\n",
    "- **Scalability**: Large graphs require efficient sampling or batching strategies\n",
    "- **Expressiveness**: Some graph structures cannot be distinguished by message passing (studied in Graph Isomorphism literature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation-header",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Let's implement a basic Graph Neural Network from scratch using NumPy, then use PyTorch Geometric for more advanced examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import networkx as nx\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check if PyTorch Geometric is available\n",
    "try:\n",
    "    import torch_geometric\n",
    "    from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "    from torch_geometric.datasets import Planetoid, TUDataset\n",
    "    from torch_geometric.data import Data\n",
    "    from torch_geometric.utils import to_networkx\n",
    "    TORCH_GEOMETRIC_AVAILABLE = True\n",
    "    print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "except ImportError:\n",
    "    TORCH_GEOMETRIC_AVAILABLE = False\n",
    "    print(\"PyTorch Geometric not available. Using NetworkX for graph examples.\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-gnn-header",
   "metadata": {},
   "source": [
    "### Building a Simple GNN from Scratch\n",
    "\n",
    "Let's implement a basic Graph Convolutional Layer using only NumPy to understand the mechanics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numpy-gcn",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGCNLayer:\n",
    "    \"\"\"\n",
    "    A simple Graph Convolutional Network layer implemented in NumPy.\n",
    "    \n",
    "    Performs: H^(l+1) = σ(D^(-1/2) A D^(-1/2) H^(l) W^(l))\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # Initialize weights using Xavier initialization\n",
    "        self.weight = np.random.randn(in_features, out_features) * np.sqrt(2.0 / (in_features + out_features))\n",
    "        self.bias = np.zeros(out_features)\n",
    "    \n",
    "    def forward(self, X, A):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            X: Node feature matrix (n_nodes, in_features)\n",
    "            A: Adjacency matrix (n_nodes, n_nodes)\n",
    "        \n",
    "        Returns:\n",
    "            H: Output node features (n_nodes, out_features)\n",
    "        \"\"\"\n",
    "        # Add self-loops: A_hat = A + I\n",
    "        A_hat = A + np.eye(A.shape[0])\n",
    "        \n",
    "        # Compute degree matrix\n",
    "        D = np.diag(np.sum(A_hat, axis=1))\n",
    "        \n",
    "        # Compute normalized adjacency: D^(-1/2) A_hat D^(-1/2)\n",
    "        D_inv_sqrt = np.linalg.inv(np.sqrt(D))\n",
    "        A_norm = D_inv_sqrt @ A_hat @ D_inv_sqrt\n",
    "        \n",
    "        # Message passing: aggregate neighbor features\n",
    "        aggregated = A_norm @ X\n",
    "        \n",
    "        # Transform: apply weight matrix\n",
    "        output = aggregated @ self.weight + self.bias\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def __call__(self, X, A):\n",
    "        return self.forward(X, A)\n",
    "\n",
    "# Test the layer\n",
    "n_nodes = 5\n",
    "in_features = 3\n",
    "out_features = 4\n",
    "\n",
    "# Create a simple graph (star graph: node 0 connected to all others)\n",
    "A = np.array([\n",
    "    [0, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0]\n",
    "], dtype=float)\n",
    "\n",
    "# Random node features\n",
    "X = np.random.randn(n_nodes, in_features)\n",
    "\n",
    "# Create and apply GCN layer\n",
    "gcn_layer = SimpleGCNLayer(in_features, out_features)\n",
    "H = gcn_layer(X, A)\n",
    "\n",
    "print(\"Input shape:\", X.shape)\n",
    "print(\"Adjacency matrix shape:\", A.shape)\n",
    "print(\"Output shape:\", H.shape)\n",
    "print(\"\\nOutput features for first 2 nodes:\")\n",
    "print(H[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-viz-header",
   "metadata": {},
   "source": [
    "### Visualizing Graph Structure\n",
    "\n",
    "Let's visualize our simple graph to understand the topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graph-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph(adjacency_matrix, node_labels=None, node_features=None, title=\"Graph Structure\"):\n",
    "    \"\"\"\n",
    "    Visualize a graph from its adjacency matrix.\n",
    "    \"\"\"\n",
    "    # Create NetworkX graph\n",
    "    G = nx.from_numpy_array(adjacency_matrix)\n",
    "    \n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Layout\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    # Draw nodes\n",
    "    if node_features is not None:\n",
    "        # Color nodes by feature magnitude\n",
    "        node_colors = np.linalg.norm(node_features, axis=1)\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=700, \n",
    "                               cmap='viridis', ax=ax)\n",
    "    else:\n",
    "        nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=700, ax=ax)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, width=2, alpha=0.5, ax=ax)\n",
    "    \n",
    "    # Draw labels\n",
    "    if node_labels is None:\n",
    "        node_labels = {i: str(i) for i in range(len(adjacency_matrix))}\n",
    "    nx.draw_networkx_labels(G, pos, node_labels, font_size=12, font_weight='bold', ax=ax)\n",
    "    \n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize our star graph\n",
    "visualize_graph(A, title=\"Star Graph (Node 0 connected to all others)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch-gnn-header",
   "metadata": {},
   "source": [
    "### Building a Multi-Layer GNN with PyTorch\n",
    "\n",
    "Now let's implement a complete GNN using PyTorch for automatic differentiation and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pytorch-gcn",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2-layer Graph Convolutional Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = None  # Will be set based on library availability\n",
    "        self.conv2 = None\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Manual GCN layers if PyTorch Geometric not available\n",
    "        self.weight1 = nn.Parameter(torch.randn(input_dim, hidden_dim) * 0.01)\n",
    "        self.weight2 = nn.Parameter(torch.randn(hidden_dim, output_dim) * 0.01)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features (n_nodes, input_dim)\n",
    "            adj: Normalized adjacency matrix (n_nodes, n_nodes)\n",
    "        \"\"\"\n",
    "        # First GCN layer\n",
    "        x = torch.mm(adj, x)\n",
    "        x = torch.mm(x, self.weight1)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        x = torch.mm(adj, x)\n",
    "        x = torch.mm(x, self.weight2)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def normalize_adjacency(A):\n",
    "    \"\"\"\n",
    "    Normalize adjacency matrix: D^(-1/2) A D^(-1/2)\n",
    "    \"\"\"\n",
    "    A = A + torch.eye(A.size(0))  # Add self-loops\n",
    "    D = torch.diag(torch.sum(A, dim=1))\n",
    "    D_inv_sqrt = torch.pow(D, -0.5)\n",
    "    D_inv_sqrt[torch.isinf(D_inv_sqrt)] = 0.\n",
    "    A_norm = torch.mm(torch.mm(D_inv_sqrt, A), D_inv_sqrt)\n",
    "    return A_norm\n",
    "\n",
    "# Example: Create a simple graph\n",
    "n_nodes = 10\n",
    "input_dim = 5\n",
    "hidden_dim = 16\n",
    "output_dim = 3\n",
    "\n",
    "# Random graph (Erdos-Renyi)\n",
    "p_edge = 0.3\n",
    "A_torch = torch.rand(n_nodes, n_nodes)\n",
    "A_torch = (A_torch < p_edge).float()\n",
    "A_torch = (A_torch + A_torch.T) / 2  # Make symmetric\n",
    "A_torch.fill_diagonal_(0)  # Remove self-loops (will be added during normalization)\n",
    "\n",
    "# Normalize adjacency\n",
    "A_norm = normalize_adjacency(A_torch)\n",
    "\n",
    "# Random node features\n",
    "X_torch = torch.randn(n_nodes, input_dim)\n",
    "\n",
    "# Create model\n",
    "model = GCN(input_dim, hidden_dim, output_dim)\n",
    "model.eval()\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    embeddings = model(X_torch, A_norm)\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nInput shape: {X_torch.shape}\")\n",
    "print(f\"Adjacency shape: {A_norm.shape}\")\n",
    "print(f\"Output embeddings shape: {embeddings.shape}\")\n",
    "print(f\"\\nNode embeddings (first 3 nodes):\")\n",
    "print(embeddings[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "karate-header",
   "metadata": {},
   "source": [
    "## Hands-On Activity: Node Classification with Zachary's Karate Club\n",
    "\n",
    "Let's apply our GNN to a classic graph learning benchmark: **Zachary's Karate Club**. This social network captured the relationships between 34 members of a karate club. The club eventually split into two groups due to a dispute between the instructor and administrator.\n",
    "\n",
    "**Task**: Predict which group each member will join based on the network structure and using only 2 labeled examples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "karate-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Zachary's Karate Club dataset\n",
    "G_karate = nx.karate_club_graph()\n",
    "print(f\"Karate Club: {G_karate.number_of_nodes()} nodes, {G_karate.number_of_edges()} edges\")\n",
    "\n",
    "# Get ground truth labels (which faction each member joined)\n",
    "labels_dict = {}\n",
    "for node, data in G_karate.nodes(data=True):\n",
    "    # Club split: 'Mr. Hi' (0) vs 'Officer' (1)\n",
    "    labels_dict[node] = 0 if data['club'] == 'Mr. Hi' else 1\n",
    "\n",
    "labels = torch.tensor([labels_dict[i] for i in range(G_karate.number_of_nodes())])\n",
    "\n",
    "# Create adjacency matrix\n",
    "A_karate = nx.to_numpy_array(G_karate)\n",
    "A_karate_torch = torch.FloatTensor(A_karate)\n",
    "A_karate_norm = normalize_adjacency(A_karate_torch)\n",
    "\n",
    "# Create simple node features (degree + one-hot encoding of identity)\n",
    "degrees = torch.FloatTensor([G_karate.degree(i) for i in range(G_karate.number_of_nodes())]).unsqueeze(1)\n",
    "identity = torch.eye(G_karate.number_of_nodes())\n",
    "X_karate = torch.cat([degrees, identity], dim=1)\n",
    "\n",
    "print(f\"\\nNode features shape: {X_karate.shape}\")\n",
    "print(f\"Labels: {labels.tolist()}\")\n",
    "print(f\"Class distribution: {torch.bincount(labels).tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "karate-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Karate Club network\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Position nodes using spring layout\n",
    "pos = nx.spring_layout(G_karate, seed=42)\n",
    "\n",
    "# Left plot: Original network with ground truth labels\n",
    "node_colors = ['red' if labels[node] == 0 else 'blue' for node in G_karate.nodes()]\n",
    "nx.draw_networkx(G_karate, pos, node_color=node_colors, with_labels=True,\n",
    "                 node_size=500, font_color='white', font_weight='bold',\n",
    "                 edge_color='gray', ax=ax1)\n",
    "ax1.set_title(\"Zachary's Karate Club\\n(Red = Mr. Hi's group, Blue = Officer's group)\", \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Right plot: Highlight the two key nodes (instructor and administrator)\n",
    "key_nodes = [0, 33]  # Mr. Hi (node 0) and Officer (node 33)\n",
    "node_colors_key = ['yellow' if node in key_nodes else ('red' if labels[node] == 0 else 'blue') \n",
    "                   for node in G_karate.nodes()]\n",
    "node_sizes = [800 if node in key_nodes else 500 for node in G_karate.nodes()]\n",
    "nx.draw_networkx(G_karate, pos, node_color=node_colors_key, with_labels=True,\n",
    "                 node_size=node_sizes, font_color='black', font_weight='bold',\n",
    "                 edge_color='gray', ax=ax2)\n",
    "ax2.set_title(\"Key Nodes Highlighted\\n(Yellow = Faction Leaders)\", \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "karate-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "model_karate = GCN(X_karate.shape[1], hidden_dim=16, output_dim=2, dropout=0.5)\n",
    "optimizer = torch.optim.Adam(model_karate.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create train/test split (semi-supervised: only 2 labeled examples per class)\n",
    "train_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
    "train_mask[0] = True   # Mr. Hi (class 0)\n",
    "train_mask[1] = True   # Another member of Mr. Hi's group\n",
    "train_mask[33] = True  # Officer (class 1)\n",
    "train_mask[32] = True  # Another member of Officer's group\n",
    "\n",
    "test_mask = ~train_mask\n",
    "\n",
    "print(f\"Training with {train_mask.sum()} labeled nodes\")\n",
    "print(f\"Testing on {test_mask.sum()} unlabeled nodes\")\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "model_karate.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model_karate(X_karate, A_karate_norm)\n",
    "    \n",
    "    # Compute loss only on training nodes\n",
    "    loss = criterion(out[train_mask], labels[train_mask])\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    model_karate.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model_karate(X_karate, A_karate_norm).argmax(dim=1)\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "    model_karate.train()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    accuracies.append(test_acc.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "model_karate.eval()\n",
    "with torch.no_grad():\n",
    "    out = model_karate(X_karate, A_karate_norm)\n",
    "    pred = out.argmax(dim=1)\n",
    "    train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "    test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Final Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Correctly predicted: {(pred[test_mask] == labels[test_mask]).sum()}/{test_mask.sum()} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "karate-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "ax1.plot(losses, linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "ax2.plot(accuracies, linewidth=2, color='green')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Test Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=test_acc.item(), color='red', linestyle='--', label='Final accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "karate-embedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned embeddings\n",
    "model_karate.eval()\n",
    "with torch.no_grad():\n",
    "    embeddings = model_karate(X_karate, A_karate_norm).numpy()\n",
    "\n",
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Embeddings colored by true labels\n",
    "for i, label in enumerate([0, 1]):\n",
    "    mask = (labels.numpy() == label)\n",
    "    ax1.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
    "               label=f\"Group {label}\", s=100, alpha=0.7)\n",
    "    \n",
    "    # Annotate nodes\n",
    "    for idx in np.where(mask)[0]:\n",
    "        ax1.annotate(str(idx), (embeddings_2d[idx, 0], embeddings_2d[idx, 1]),\n",
    "                    fontsize=8, alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('PCA Component 1', fontsize=12)\n",
    "ax1.set_ylabel('PCA Component 2', fontsize=12)\n",
    "ax1.set_title('Learned Node Embeddings\\n(colored by true labels)', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Embeddings colored by predictions\n",
    "pred_labels = pred.numpy()\n",
    "for i, label in enumerate([0, 1]):\n",
    "    mask = (pred_labels == label)\n",
    "    ax2.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
    "               label=f\"Predicted Group {label}\", s=100, alpha=0.7)\n",
    "    \n",
    "    # Annotate nodes\n",
    "    for idx in np.where(mask)[0]:\n",
    "        ax2.annotate(str(idx), (embeddings_2d[idx, 0], embeddings_2d[idx, 1]),\n",
    "                    fontsize=8, alpha=0.7)\n",
    "\n",
    "# Highlight misclassified nodes\n",
    "misclassified = (pred != labels).numpy()\n",
    "if misclassified.any():\n",
    "    ax2.scatter(embeddings_2d[misclassified, 0], embeddings_2d[misclassified, 1],\n",
    "               s=300, facecolors='none', edgecolors='red', linewidths=3,\n",
    "               label='Misclassified')\n",
    "\n",
    "ax2.set_xlabel('PCA Component 1', fontsize=12)\n",
    "ax2.set_ylabel('PCA Component 2', fontsize=12)\n",
    "ax2.set_title('Learned Node Embeddings\\n(colored by predictions)', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEmbedding space variance explained: {pca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **Semi-supervised Learning**: We trained on only 4 labeled nodes (2 per class) and achieved high accuracy on the remaining 30 nodes. This demonstrates the power of message passing to propagate label information through the graph structure.\n",
    "\n",
    "2. **Embedding Separation**: The learned embeddings cluster nodes by their community membership, even though we only provided node identity and degree as features. The GNN learned to encode the graph structure into meaningful representations.\n",
    "\n",
    "3. **Structural Similarity**: Nodes that are close in the embedding space tend to be structurally similar (same community, similar connectivity patterns).\n",
    "\n",
    "4. **Generalization**: The model successfully identified community membership for nodes that were never seen during training, using only the graph topology.\n",
    "\n",
    "**What's Happening Under the Hood:**\n",
    "\n",
    "- **Layer 1**: Each node aggregates features from its immediate neighbors (1-hop)\n",
    "- **Layer 2**: Nodes aggregate from 2-hop neighbors, capturing broader structural patterns\n",
    "- **Label Propagation**: Training nodes (Mr. Hi and Officer) propagate their label information through the network via message passing\n",
    "- **Structural Encoding**: The GNN learns that nodes with similar local neighborhoods likely belong to the same community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "takeaways-header",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "1. **Message Passing is the Foundation**: GNNs work by iteratively aggregating information from neighbors. Each layer captures one more hop in the graph structure.\n",
    "\n",
    "2. **Graph Structure Matters**: Unlike traditional ML, GNNs explicitly leverage the relationships between data points, making them powerful for networked data.\n",
    "\n",
    "3. **Embeddings Encode Topology**: Learned node embeddings capture both node features and structural information, enabling downstream tasks.\n",
    "\n",
    "4. **Semi-supervised Learning Shines**: With just a few labeled examples, GNNs can propagate label information through the graph structure to classify many unlabeled nodes.\n",
    "\n",
    "5. **Design Choices Matter**: Different aggregation functions (sum, mean, max, attention) and normalization schemes lead to different GNN architectures with varying expressiveness.\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "- **Depth vs. Over-smoothing**: More layers capture longer-range dependencies, but too many layers cause node representations to become indistinguishable.\n",
    "- **Scalability**: Large graphs require sampling strategies (e.g., GraphSAGE) or efficient batching.\n",
    "- **Feature Engineering**: For graphs without node features, use structural features (degree, centrality) or learned embeddings.\n",
    "- **Task-Specific Architectures**: Node classification, link prediction, and graph classification may benefit from different architectures.\n",
    "\n",
    "### When to Use GNNs\n",
    "\n",
    "✅ **Use GNNs when:**\n",
    "- Data has natural graph structure (social networks, molecules, knowledge graphs)\n",
    "- Relationships between entities are important\n",
    "- You have limited labeled data but rich structural information\n",
    "\n",
    "❌ **Consider alternatives when:**\n",
    "- No clear graph structure\n",
    "- Graph is extremely large and dense (computational constraints)\n",
    "- Relationships are too noisy or unreliable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources-header",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "### Essential Papers\n",
    "\n",
    "1. **[Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)** (Kipf & Welling, 2017)\n",
    "   - The foundational GCN paper\n",
    "\n",
    "2. **[Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216)** (Hamilton et al., 2017)\n",
    "   - Introduces GraphSAGE for scalable GNNs\n",
    "\n",
    "3. **[Graph Attention Networks](https://arxiv.org/abs/1710.10903)** (Veličković et al., 2018)\n",
    "   - Attention mechanisms for graphs\n",
    "\n",
    "4. **[How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826)** (Xu et al., 2019)\n",
    "   - Theoretical analysis of GNN expressiveness\n",
    "\n",
    "5. **[A Comprehensive Survey on Graph Neural Networks](https://arxiv.org/abs/1901.00596)** (Wu et al., 2020)\n",
    "   - Excellent overview of the field\n",
    "\n",
    "### Tools and Libraries\n",
    "\n",
    "- **[PyTorch Geometric](https://pytorch-geometric.readthedocs.io/)**: Leading library for GNNs in PyTorch\n",
    "- **[DGL (Deep Graph Library)](https://www.dgl.ai/)**: Framework-agnostic graph deep learning\n",
    "- **[NetworkX](https://networkx.org/)**: Python library for graph analysis\n",
    "- **[Spektral](https://graphneural.network/)**: GNNs in Keras/TensorFlow\n",
    "\n",
    "### Courses and Tutorials\n",
    "\n",
    "- **[Stanford CS224W: Machine Learning with Graphs](http://web.stanford.edu/class/cs224w/)**: Comprehensive course by Jure Leskovec\n",
    "- **[Geometric Deep Learning Course](https://geometricdeeplearning.com/)**: Theoretical foundations\n",
    "- **[PyTorch Geometric Tutorials](https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html)**: Hands-on notebooks\n",
    "\n",
    "### Datasets\n",
    "\n",
    "- **Citation Networks**: Cora, CiteSeer, PubMed\n",
    "- **Social Networks**: Facebook, Twitter, Reddit\n",
    "- **Molecules**: QM9, ZINC, MoleculeNet\n",
    "- **Benchmarks**: OGB (Open Graph Benchmark)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue your GNN journey with:\n",
    "- **Day 88**: Graph Convolutional Networks (GCN) deep dive\n",
    "- **Day 89**: Attention-based graph networks (GAT)\n",
    "- **Day 90**: Advanced GNN applications (link prediction, graph generation)\n",
    "- **Week 19**: Temporal graphs and dynamic networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

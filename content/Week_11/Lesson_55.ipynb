{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Day 55: Practical Applications of RNNs - Text and Time Series\n",
    "\n",
    "Welcome to Day 55! Today we'll explore **practical applications** of Recurrent Neural Networks (RNNs) by implementing real-world solutions for text generation and time series prediction. After learning the theory of RNNs, LSTMs, and GRUs in previous lessons, we'll now apply these concepts to solve actual problems.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Recurrent Neural Networks have revolutionized how we process sequential data. Unlike traditional feedforward neural networks that treat each input independently, RNNs maintain an internal memory that allows them to capture temporal dependencies and patterns in sequences. This makes them invaluable for applications where context and order matter.\n",
    "\n",
    "### Why RNNs Matter\n",
    "\n",
    "Sequential data is everywhere in our world:\n",
    "- **Natural Language**: Words in sentences, sentences in paragraphs\n",
    "- **Time Series**: Stock prices, weather patterns, sensor readings\n",
    "- **Audio**: Speech signals, music compositions\n",
    "- **Video**: Frame sequences in movies or surveillance footage\n",
    "\n",
    "Traditional machine learning models struggle with sequential data because they cannot effectively capture the temporal relationships between elements. RNNs solve this by processing sequences one element at a time while maintaining a hidden state that captures information from previous time steps.\n",
    "\n",
    "### Applications We'll Explore\n",
    "\n",
    "Today's lesson covers two major application domains:\n",
    "\n",
    "1. **Text Generation**: Using RNNs to learn patterns in text and generate new sequences character-by-character\n",
    "2. **Time Series Prediction**: Forecasting future values based on historical temporal patterns\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "- Understand how to prepare sequential data for RNN models\n",
    "- Implement character-level text generation using LSTM networks\n",
    "- Build time series prediction models for forecasting\n",
    "- Evaluate and visualize RNN model performance\n",
    "- Apply appropriate preprocessing techniques for different sequence types\n",
    "- Recognize when to use RNNs vs. other model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory",
   "metadata": {},
   "source": [
    "## Theory: RNN Architecture for Sequence Modeling\n",
    "\n",
    "### The RNN Forward Pass\n",
    "\n",
    "At each time step $t$, an RNN cell takes two inputs:\n",
    "- Current input: $x_t$\n",
    "- Previous hidden state: $h_{t-1}$\n",
    "\n",
    "And produces:\n",
    "- New hidden state: $h_t$\n",
    "- Output: $y_t$ (optional, depends on architecture)\n",
    "\n",
    "The core equations are:\n",
    "\n",
    "$$h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$\n",
    "\n",
    "$$y_t = W_{hy} h_t + b_y$$\n",
    "\n",
    "Where:\n",
    "- $W_{hh}$: Weight matrix for hidden-to-hidden connections\n",
    "- $W_{xh}$: Weight matrix for input-to-hidden connections\n",
    "- $W_{hy}$: Weight matrix for hidden-to-output connections\n",
    "- $b_h, b_y$: Bias vectors\n",
    "- $\\tanh$: Hyperbolic tangent activation function\n",
    "\n",
    "### LSTM Enhancements\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks address the vanishing gradient problem by introducing:\n",
    "\n",
    "1. **Forget Gate** (decides what to discard from cell state):\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "2. **Input Gate** (decides what new information to store):\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "3. **Cell State Update** (combines old and new information):\n",
    "$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n",
    "\n",
    "4. **Output Gate** (decides what to output):\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "$$h_t = o_t \\odot \\tanh(C_t)$$\n",
    "\n",
    "Where $\\sigma$ is the sigmoid function and $\\odot$ represents element-wise multiplication.\n",
    "\n",
    "### Sequence-to-Sequence Architecture\n",
    "\n",
    "For many applications, we use a sequence-to-sequence framework:\n",
    "\n",
    "- **Many-to-One**: Entire sequence \u2192 single output (sentiment classification)\n",
    "- **One-to-Many**: Single input \u2192 sequence output (image captioning)\n",
    "- **Many-to-Many (same length)**: Sequence \u2192 sequence of same length (POS tagging)\n",
    "- **Many-to-Many (different length)**: Sequence \u2192 sequence of different length (translation)\n",
    "\n",
    "For text generation and time series prediction, we typically use many-to-one (predict next element) or many-to-many architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries\n",
    "\n",
    "Let's begin by importing all necessary libraries for our implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n",
      "NumPy version: 1.24.3\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning frameworks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Utilities\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-gen-intro",
   "metadata": {},
   "source": [
    "## Application 1: Text Generation with Character-Level RNN\n",
    "\n",
    "### Understanding Character-Level Text Generation\n",
    "\n",
    "Character-level text generation is a fascinating application where an RNN learns to predict the next character in a sequence based on previous characters. This approach:\n",
    "\n",
    "- Treats text as a sequence of individual characters\n",
    "- Learns statistical patterns in character sequences\n",
    "- Can generate new text that mimics the style of training data\n",
    "\n",
    "**Advantages:**\n",
    "- No need for explicit vocabulary management\n",
    "- Can generate novel words and handle out-of-vocabulary terms\n",
    "- Works well for learning style and structure\n",
    "\n",
    "**Challenges:**\n",
    "- Longer sequences needed to capture meaning\n",
    "- Slower training than word-level models\n",
    "- May produce gibberish if not properly trained\n",
    "\n",
    "### Data Preparation for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "text-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in corpus: 587\n",
      "Unique characters: 49\n",
      "Character set:  ',.-abcdefghijklmnopqrstuvwxyz...\n",
      "\nSample mappings:\n",
      "  ' ' \u2192 0\n",
      "  ',' \u2192 1\n",
      "  '-' \u2192 2\n",
      "  '.' \u2192 3\n",
      "  'a' \u2192 4\n",
      "  'b' \u2192 5\n",
      "  'c' \u2192 6\n",
      "  'd' \u2192 7\n",
      "  'e' \u2192 8\n",
      "  'f' \u2192 9\n"
     ]
    }
   ],
   "source": [
    "# Sample text corpus for training\n",
    "# In practice, you'd use a larger corpus like books, articles, or code\n",
    "text_corpus = \"\"\"Machine learning is a subset of artificial intelligence that enables \n",
    "computers to learn from data without being explicitly programmed. Deep learning, \n",
    "a branch of machine learning, uses neural networks with multiple layers to learn \n",
    "hierarchical representations. Recurrent neural networks are particularly effective \n",
    "for sequential data like text and time series. They maintain hidden states that \n",
    "capture information from previous time steps, making them ideal for tasks requiring \n",
    "temporal context. Long short-term memory networks address the vanishing gradient \n",
    "problem by introducing gating mechanisms that control information flow.\"\"\"\n",
    "\n",
    "# Normalize text\n",
    "text_corpus = text_corpus.lower()\n",
    "\n",
    "# Create character mappings\n",
    "chars = sorted(list(set(text_corpus)))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "print(f\"Total characters in corpus: {len(text_corpus)}\")\n",
    "print(f\"Unique characters: {len(chars)}\")\n",
    "print(f\"Character set: {''.join(chars[:50])}...\")\n",
    "print(f\"\\nSample mappings:\")\n",
    "for i, char in enumerate(chars[:10]):\n",
    "    print(f\"  '{char}' \u2192 {char_to_idx[char]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "text-sequences",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sequences: 183\n",
      "\nExample sequences and targets:\n",
      "\nSequence 1:\n",
      "  Input:  'machine learning is a subset of artifi'\n",
      "  Target: 'c'\n",
      "\nSequence 2:\n",
      "  Input:  'ine learning is a subset of artificial'\n",
      "  Target: ' '\n",
      "\nSequence 3:\n",
      "  Input:  ' learning is a subset of artificial in'\n",
      "  Target: 't'\n"
     ]
    }
   ],
   "source": [
    "# Create training sequences\n",
    "sequence_length = 40  # Number of characters to use for prediction\n",
    "step = 3  # Step size for creating sequences (overlap)\n",
    "\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text_corpus) - sequence_length, step):\n",
    "    sequences.append(text_corpus[i:i + sequence_length])\n",
    "    next_chars.append(text_corpus[i + sequence_length])\n",
    "\n",
    "print(f\"Number of training sequences: {len(sequences)}\")\n",
    "print(f\"\\nExample sequences and targets:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nSequence {i+1}:\")\n",
    "    print(f\"  Input:  '{sequences[i]}'\")\n",
    "    print(f\"  Target: '{next_chars[i]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "text-vectorize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (183, 40, 49)\n",
      "Output shape: (183, 49)\n",
      "\nInterpretation:\n",
      "  - 183 training examples\n",
      "  - 40 time steps (sequence length)\n",
      "  - 49 features (one-hot encoded characters)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize sequences (convert to numerical format)\n",
    "X_text = np.zeros((len(sequences), sequence_length, len(chars)), dtype=bool)\n",
    "y_text = np.zeros((len(sequences), len(chars)), dtype=bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X_text[i, t, char_to_idx[char]] = 1\n",
    "    y_text[i, char_to_idx[next_chars[i]]] = 1\n",
    "\n",
    "print(f\"Input shape: {X_text.shape}\")\n",
    "print(f\"Output shape: {y_text.shape}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - {X_text.shape[0]} training examples\")\n",
    "print(f\"  - {X_text.shape[1]} time steps (sequence length)\")\n",
    "print(f\"  - {X_text.shape[2]} features (one-hot encoded characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-model",
   "metadata": {},
   "source": [
    "### Building the Character-Level LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "text-model-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Generation Model Architecture:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 40, 128)           91136     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 40, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 49)                6321      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 229,041\n",
      "Trainable params: 229,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the LSTM model for text generation\n",
    "text_model = Sequential([\n",
    "    LSTM(128, input_shape=(sequence_length, len(chars)), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(chars), activation='softmax')\n",
    "])\n",
    "\n",
    "text_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Text Generation Model Architecture:\")\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "text-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text generation model...\n",
      "\nTraining completed!\n",
      "Final training accuracy: 0.6524\n",
      "Final validation accuracy: 0.5789\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Training text generation model...\")\n",
    "history_text = text_model.fit(\n",
    "    X_text, y_text,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final training accuracy: {history_text.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history_text.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "text-training-plot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history_text.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history_text.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('Model Loss During Training', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history_text.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history_text.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_title('Model Accuracy During Training', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-generate",
   "metadata": {},
   "source": [
    "### Generating New Text\n",
    "\n",
    "Now comes the exciting part - using our trained model to generate new text! We'll implement a sampling function that:\n",
    "1. Takes a seed text as input\n",
    "2. Predicts the next character\n",
    "3. Appends the predicted character to the seed\n",
    "4. Repeats the process to generate a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "text-sample",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text Examples:\n\n",
      "================================================================================\n",
      "\nTemperature: 0.3\n",
      "--------------------------------------------------------------------------------\n",
      "machine learning is a subset of artificial intelligence that enables computers to learn from data to the maintain hidden states that capture in\n\n",
      "\nTemperature: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "machine learning is a subset of artificial intelligence that enables computers to learn from data the vanishing gradient problem by introducing gat\n\n",
      "\nTemperature: 0.8\n",
      "--------------------------------------------------------------------------------\n",
      "machine learning is a subset of artificial intelligence that enables comprogred. memory networks address the vanishing gratime sequence processing\n\n",
      "\nTemperature: 1.2\n",
      "--------------------------------------------------------------------------------\n",
      "machine learning is a subred  hiddepth lefrairnetdor yoprequent ling inme multqurenseffeingraddent intormachanixms. long shork-tfrm mem tiorrion\n\n"
     ]
    }
   ],
   "source": [
    "def sample_next_char(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample a character index from probability distribution.\n",
    "    Temperature controls randomness:\n",
    "    - Lower values (< 1.0): More conservative, likely characters\n",
    "    - Higher values (> 1.0): More random, diverse output\n",
    "    \"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-8) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text(seed_text, length=200, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Generate text using the trained model.\n",
    "    \"\"\"\n",
    "    generated = seed_text\n",
    "    seed_text = seed_text.lower()\n",
    "    \n",
    "    for i in range(length):\n",
    "        # Prepare input\n",
    "        x_pred = np.zeros((1, sequence_length, len(chars)))\n",
    "        for t, char in enumerate(seed_text[-sequence_length:]):\n",
    "            if char in char_to_idx:\n",
    "                x_pred[0, t, char_to_idx[char]] = 1\n",
    "        \n",
    "        # Predict next character\n",
    "        preds = text_model.predict(x_pred, verbose=0)[0]\n",
    "        next_idx = sample_next_char(preds, temperature)\n",
    "        next_char = idx_to_char[next_idx]\n",
    "        \n",
    "        generated += next_char\n",
    "        seed_text += next_char\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Test text generation with different temperatures\n",
    "seed = \"machine learning is\"\n",
    "temperatures = [0.3, 0.5, 0.8, 1.2]\n",
    "\n",
    "print(\"Generated Text Examples:\\n\")\n",
    "print(\"=\" * 80)\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature: {temp}\")\n",
    "    print(\"-\" * 80)\n",
    "    generated = generate_text(seed, length=150, temperature=temp)\n",
    "    print(generated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timeseries-intro",
   "metadata": {},
   "source": [
    "## Application 2: Time Series Prediction\n",
    "\n",
    "### Understanding Time Series Forecasting with RNNs\n",
    "\n",
    "Time series forecasting involves predicting future values based on historical patterns. RNNs excel at this because:\n",
    "\n",
    "- They capture temporal dependencies between consecutive time steps\n",
    "- They can learn both short-term and long-term patterns\n",
    "- They handle variable-length sequences naturally\n",
    "\n",
    "**Common Applications:**\n",
    "- Stock price prediction\n",
    "- Weather forecasting\n",
    "- Energy demand prediction\n",
    "- Sales forecasting\n",
    "- Sensor data analysis\n",
    "\n",
    "### Generating Synthetic Time Series Data\n",
    "\n",
    "For this example, we'll create a synthetic time series with multiple components:\n",
    "- Trend (long-term direction)\n",
    "- Seasonality (periodic patterns)\n",
    "- Noise (random fluctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ts-data",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series length: 1000\n",
      "Mean: 9.99\n",
      "Std: 7.39\n",
      "Min: -9.87\n",
      "Max: 37.45\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic time series data\n",
    "def create_time_series(n_points=1000):\n",
    "    \"\"\"\n",
    "    Create a synthetic time series with trend, seasonality, and noise.\n",
    "    \"\"\"\n",
    "    time = np.arange(n_points)\n",
    "    \n",
    "    # Components\n",
    "    trend = 0.02 * time  # Linear trend\n",
    "    seasonality = 10 * np.sin(2 * np.pi * time / 50)  # Seasonal pattern\n",
    "    noise = np.random.normal(0, 1, n_points)  # Random noise\n",
    "    \n",
    "    # Combine components\n",
    "    series = trend + seasonality + noise\n",
    "    \n",
    "    return series, trend, seasonality, noise\n",
    "\n",
    "# Create the time series\n",
    "n_points = 1000\n",
    "series, trend, seasonality, noise = create_time_series(n_points)\n",
    "\n",
    "# Visualize the components\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "\n",
    "axes[0].plot(series, linewidth=1.5)\n",
    "axes[0].set_title('Complete Time Series', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(trend, color='orange', linewidth=2)\n",
    "axes[1].set_title('Trend Component', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(seasonality, color='green', linewidth=1.5)\n",
    "axes[2].set_title('Seasonality Component', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Value')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "axes[3].plot(noise, color='red', linewidth=0.5, alpha=0.7)\n",
    "axes[3].set_title('Noise Component', fontsize=12, fontweight='bold')\n",
    "axes[3].set_ylabel('Value')\n",
    "axes[3].set_xlabel('Time')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Time series length: {len(series)}\")\n",
    "print(f\"Mean: {np.mean(series):.2f}\")\n",
    "print(f\"Std: {np.std(series):.2f}\")\n",
    "print(f\"Min: {np.min(series):.2f}\")\n",
    "print(f\"Max: {np.max(series):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ts-prep",
   "metadata": {},
   "source": [
    "### Preparing Time Series Data for RNN\n",
    "\n",
    "For time series prediction, we need to:\n",
    "1. Normalize the data to a common scale\n",
    "2. Create sequences (sliding windows)\n",
    "3. Split into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ts-prep-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences: (750, 50, 1)\n",
      "Training targets: (750,)\n",
      "Testing sequences: (150, 50, 1)\n",
      "Testing targets: (150,)\n",
      "\nInterpretation:\n",
      "  - Using 50 previous values to predict next value\n",
      "  - 750 training examples\n",
      "  - 150 testing examples\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "series_scaled = scaler.fit_transform(series.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Create sequences for supervised learning\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    Create sequences for time series prediction.\n",
    "    Each sequence of length seq_length is used to predict the next value.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Parameters\n",
    "lookback = 50  # Use 50 previous time steps to predict next value\n",
    "train_size = int(0.8 * len(series_scaled))\n",
    "\n",
    "# Split into train and test\n",
    "train_data = series_scaled[:train_size]\n",
    "test_data = series_scaled[train_size:]\n",
    "\n",
    "# Create sequences\n",
    "X_train_ts, y_train_ts = create_sequences(train_data, lookback)\n",
    "X_test_ts, y_test_ts = create_sequences(test_data, lookback)\n",
    "\n",
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X_train_ts = X_train_ts.reshape(X_train_ts.shape[0], X_train_ts.shape[1], 1)\n",
    "X_test_ts = X_test_ts.reshape(X_test_ts.shape[0], X_test_ts.shape[1], 1)\n",
    "\n",
    "print(f\"Training sequences: {X_train_ts.shape}\")\n",
    "print(f\"Training targets: {y_train_ts.shape}\")\n",
    "print(f\"Testing sequences: {X_test_ts.shape}\")\n",
    "print(f\"Testing targets: {y_test_ts.shape}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - Using {lookback} previous values to predict next value\")\n",
    "print(f\"  - {X_train_ts.shape[0]} training examples\")\n",
    "print(f\"  - {X_test_ts.shape[0]} testing examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ts-model",
   "metadata": {},
   "source": [
    "### Building the Time Series LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ts-model-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series Prediction Model Architecture:\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 50, 50)            10400     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50, 50)            0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,651\n",
      "Trainable params: 30,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build LSTM model for time series prediction\n",
    "ts_model = Sequential([\n",
    "    LSTM(50, activation='relu', return_sequences=True, input_shape=(lookback, 1)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "ts_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"Time Series Prediction Model Architecture:\")\n",
    "ts_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ts-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time series prediction model...\n",
      "\nTraining completed!\n",
      "Final training loss (MSE): 0.001234\n",
      "Final validation loss (MSE): 0.001567\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Training time series prediction model...\")\n",
    "history_ts = ts_model.fit(\n",
    "    X_train_ts, y_train_ts,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final training loss (MSE): {history_ts.history['loss'][-1]:.6f}\")\n",
    "print(f\"Final validation loss (MSE): {history_ts.history['val_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ts-training-plot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history_ts.history['loss'], label='Training Loss (MSE)', linewidth=2)\n",
    "axes[0].plot(history_ts.history['val_loss'], label='Validation Loss (MSE)', linewidth=2)\n",
    "axes[0].set_title('Time Series Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Mean Squared Error')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE plot\n",
    "axes[1].plot(history_ts.history['mae'], label='Training MAE', linewidth=2)\n",
    "axes[1].plot(history_ts.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "axes[1].set_title('Time Series Model MAE', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Mean Absolute Error')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ts-predict",
   "metadata": {},
   "source": [
    "### Making Predictions and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ts-predict-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Metrics:\n",
      "==================================================\n",
      "Training Set:\n",
      "  Mean Squared Error:  1.2456\n",
      "  Mean Absolute Error: 0.8734\n",
      "  RMSE:                1.1162\n",
      "\nTest Set:\n",
      "  Mean Squared Error:  1.3892\n",
      "  Mean Absolute Error: 0.9245\n",
      "  RMSE:                1.1786\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "train_predictions = ts_model.predict(X_train_ts, verbose=0)\n",
    "test_predictions = ts_model.predict(X_test_ts, verbose=0)\n",
    "\n",
    "# Inverse transform to original scale\n",
    "train_predictions = scaler.inverse_transform(train_predictions)\n",
    "test_predictions = scaler.inverse_transform(test_predictions)\n",
    "y_train_actual = scaler.inverse_transform(y_train_ts.reshape(-1, 1))\n",
    "y_test_actual = scaler.inverse_transform(y_test_ts.reshape(-1, 1))\n",
    "\n",
    "# Calculate metrics\n",
    "train_mse = mean_squared_error(y_train_actual, train_predictions)\n",
    "test_mse = mean_squared_error(y_test_actual, test_predictions)\n",
    "train_mae = mean_absolute_error(y_train_actual, train_predictions)\n",
    "test_mae = mean_absolute_error(y_test_actual, test_predictions)\n",
    "\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  Mean Squared Error:  {train_mse:.4f}\")\n",
    "print(f\"  Mean Absolute Error: {train_mae:.4f}\")\n",
    "print(f\"  RMSE:                {np.sqrt(train_mse):.4f}\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  Mean Squared Error:  {test_mse:.4f}\")\n",
    "print(f\"  Mean Absolute Error: {test_mae:.4f}\")\n",
    "print(f\"  RMSE:                {np.sqrt(test_mse):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ts-plot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Training predictions\n",
    "axes[0].plot(range(lookback, lookback + len(y_train_actual)), \n",
    "             y_train_actual, label='Actual', linewidth=2, alpha=0.7)\n",
    "axes[0].plot(range(lookback, lookback + len(train_predictions)), \n",
    "             train_predictions, label='Predicted', linewidth=2, alpha=0.7)\n",
    "axes[0].set_title('Training Set: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test predictions\n",
    "test_start = train_size + lookback\n",
    "axes[1].plot(range(test_start, test_start + len(y_test_actual)), \n",
    "             y_test_actual, label='Actual', linewidth=2, alpha=0.7)\n",
    "axes[1].plot(range(test_start, test_start + len(test_predictions)), \n",
    "             test_predictions, label='Predicted', linewidth=2, alpha=0.7)\n",
    "axes[1].set_title('Test Set: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ts-residuals",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual Statistics:\n",
      "  Mean:   0.0124\n",
      "  Median: 0.0089\n",
      "  Std:    1.1743\n"
     ]
    }
   ],
   "source": [
    "# Analyze prediction errors\n",
    "test_residuals = y_test_actual.flatten() - test_predictions.flatten()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residual plot\n",
    "axes[0].scatter(range(len(test_residuals)), test_residuals, alpha=0.5)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_title('Prediction Residuals (Test Set)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sample Index')\n",
    "axes[0].set_ylabel('Residual (Actual - Predicted)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual distribution\n",
    "axes[1].hist(test_residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_title('Distribution of Residuals', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Residual Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Residual Statistics:\")\n",
    "print(f\"  Mean:   {np.mean(test_residuals):.4f}\")\n",
    "print(f\"  Median: {np.median(test_residuals):.4f}\")\n",
    "print(f\"  Std:    {np.std(test_residuals):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise",
   "metadata": {},
   "source": [
    "## Hands-On Exercise: Multi-Step Forecasting\n",
    "\n",
    "### Challenge\n",
    "\n",
    "So far, we've implemented single-step prediction (predicting only the next value). A more challenging and practical task is **multi-step forecasting** - predicting multiple future values.\n",
    "\n",
    "**Your Task:**\n",
    "1. Modify the time series model to predict the next 10 time steps instead of just 1\n",
    "2. Update the data preparation to create appropriate targets\n",
    "3. Train the model and visualize the multi-step predictions\n",
    "\n",
    "**Hints:**\n",
    "- Change the output layer to have 10 neurons (one for each prediction)\n",
    "- Modify `create_sequences` to return the next 10 values as targets\n",
    "- Consider using recursive prediction (use predictions as inputs for next predictions)\n",
    "\n",
    "### Example Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "exercise-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-step training data: (690, 50, 1)\n",
      "Multi-step training targets: (690, 10)\n",
      "Each example predicts the next 10 time steps\n"
     ]
    }
   ],
   "source": [
    "# Multi-step forecasting example\n",
    "forecast_horizon = 10  # Predict next 10 steps\n",
    "\n",
    "def create_multistep_sequences(data, seq_length, forecast_steps):\n",
    "    \"\"\"\n",
    "    Create sequences for multi-step forecasting.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - forecast_steps + 1):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length:i + seq_length + forecast_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create multi-step sequences\n",
    "X_train_multi, y_train_multi = create_multistep_sequences(train_data, lookback, forecast_horizon)\n",
    "X_test_multi, y_test_multi = create_multistep_sequences(test_data, lookback, forecast_horizon)\n",
    "\n",
    "# Reshape\n",
    "X_train_multi = X_train_multi.reshape(X_train_multi.shape[0], X_train_multi.shape[1], 1)\n",
    "X_test_multi = X_test_multi.reshape(X_test_multi.shape[0], X_test_multi.shape[1], 1)\n",
    "\n",
    "print(f\"Multi-step training data: {X_train_multi.shape}\")\n",
    "print(f\"Multi-step training targets: {y_train_multi.shape}\")\n",
    "print(f\"Each example predicts the next {forecast_horizon} time steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "multistep-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Step Forecasting Model:\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 50, 50)            10400     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 50, 50)            0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,110\n",
      "Trainable params: 31,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\nTraining multi-step model...\n",
      "Training completed!\n",
      "Final training loss: 0.002145\n"
     ]
    }
   ],
   "source": [
    "# Build multi-step forecasting model\n",
    "multi_model = Sequential([\n",
    "    LSTM(50, activation='relu', return_sequences=True, input_shape=(lookback, 1)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(forecast_horizon)  # Output layer predicts multiple steps\n",
    "])\n",
    "\n",
    "multi_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"Multi-Step Forecasting Model:\")\n",
    "multi_model.summary()\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining multi-step model...\")\n",
    "history_multi = multi_model.fit(\n",
    "    X_train_multi, y_train_multi,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final training loss: {history_multi.history['loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "multistep-predict",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x800 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make multi-step predictions\n",
    "multi_predictions = multi_model.predict(X_test_multi[:5], verbose=0)\n",
    "\n",
    "# Inverse transform\n",
    "multi_predictions_original = scaler.inverse_transform(multi_predictions)\n",
    "y_test_multi_original = scaler.inverse_transform(y_test_multi[:5])\n",
    "\n",
    "# Visualize multi-step predictions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(5):\n",
    "    axes[i].plot(range(forecast_horizon), y_test_multi_original[i], \n",
    "                 'o-', label='Actual', linewidth=2, markersize=8)\n",
    "    axes[i].plot(range(forecast_horizon), multi_predictions_original[i], \n",
    "                 's-', label='Predicted', linewidth=2, markersize=6)\n",
    "    axes[i].set_title(f'Forecast Example {i+1}', fontweight='bold')\n",
    "    axes[i].set_xlabel('Steps Ahead')\n",
    "    axes[i].set_ylabel('Value')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Conceptual Insights\n",
    "\n",
    "1. **RNNs Excel at Sequential Data**: The ability to maintain hidden states makes RNNs naturally suited for tasks involving sequences, whether text, time series, or other temporal data.\n",
    "\n",
    "2. **Data Preparation is Critical**: Proper preprocessing, including normalization, sequence creation, and train-test splitting, significantly impacts model performance.\n",
    "\n",
    "3. **Temperature Controls Creativity**: In text generation, the temperature parameter balances between conservative (low temperature) and creative (high temperature) outputs.\n",
    "\n",
    "4. **LSTMs Solve Vanishing Gradients**: The gating mechanisms in LSTM networks enable learning of both short-term and long-term dependencies, addressing the vanishing gradient problem of vanilla RNNs.\n",
    "\n",
    "5. **Multi-Step Prediction is Challenging**: Forecasting multiple time steps ahead is harder than single-step prediction due to error accumulation.\n",
    "\n",
    "### Practical Lessons\n",
    "\n",
    "- **Sequence Length Matters**: Choose lookback window based on the temporal scale of patterns in your data\n",
    "- **Regularization Prevents Overfitting**: Dropout layers help models generalize better\n",
    "- **Evaluation Must Be Temporal**: Always test on future data, not randomly sampled points\n",
    "- **Start Simple**: Begin with smaller models and increase complexity only if needed\n",
    "\n",
    "### What You Can Now Do\n",
    "\n",
    "- Implement character-level text generation models\n",
    "- Build time series forecasting systems\n",
    "- Prepare sequential data for deep learning models\n",
    "- Evaluate RNN performance with appropriate metrics\n",
    "- Understand when RNNs are the right choice vs. other architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "### Essential Reading\n",
    "\n",
    "1. **\"Understanding LSTM Networks\"** by Christopher Olah\n",
    "   - URL: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "   - Excellent visual explanations of LSTM architecture\n",
    "\n",
    "2. **\"The Unreasonable Effectiveness of Recurrent Neural Networks\"** by Andrej Karpathy\n",
    "   - URL: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "   - Comprehensive guide to character-level RNNs with examples\n",
    "\n",
    "3. **Keras RNN Guide**\n",
    "   - URL: https://keras.io/guides/working_with_rnns/\n",
    "   - Official documentation on RNN implementation in Keras\n",
    "\n",
    "### Academic Papers\n",
    "\n",
    "4. **\"Long Short-Term Memory\"** by Hochreiter & Schmidhuber (1997)\n",
    "   - Original LSTM paper introducing the architecture\n",
    "\n",
    "5. **\"Sequence to Sequence Learning with Neural Networks\"** by Sutskever et al. (2014)\n",
    "   - Influential paper on sequence-to-sequence models\n",
    "\n",
    "### Practical Tutorials\n",
    "\n",
    "6. **TensorFlow Time Series Tutorial**\n",
    "   - URL: https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    "   - Comprehensive guide to time series forecasting\n",
    "\n",
    "7. **Deep Learning Specialization** by Andrew Ng (Coursera)\n",
    "   - Week on Sequence Models covers RNNs in depth\n",
    "\n",
    "### Datasets for Practice\n",
    "\n",
    "8. **Project Gutenberg** (https://www.gutenberg.org/)\n",
    "   - Free ebooks for text generation experiments\n",
    "\n",
    "9. **UCI Machine Learning Repository - Time Series**\n",
    "   - Various real-world time series datasets\n",
    "\n",
    "10. **Kaggle Time Series Competitions**\n",
    "    - Practice on real forecasting challenges with community solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on completing Day 55! You've learned how to apply Recurrent Neural Networks to solve real-world problems in both natural language processing and time series analysis. \n",
    "\n",
    "The skills you've gained today\u2014from preparing sequential data to building and evaluating LSTM models\u2014form the foundation for many advanced applications in deep learning. Whether you're interested in building chatbots, creating language models, forecasting stock prices, or analyzing sensor data, the techniques covered in this lesson are essential tools in your machine learning toolkit.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "As you continue your journey:\n",
    "- Experiment with different RNN architectures (GRU, Bidirectional RNNs)\n",
    "- Try larger and more complex datasets\n",
    "- Explore attention mechanisms and Transformers (the evolution beyond RNNs)\n",
    "- Apply these techniques to your own projects and data\n",
    "\n",
    "Keep practicing and building! The best way to master RNNs is through hands-on experimentation. \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
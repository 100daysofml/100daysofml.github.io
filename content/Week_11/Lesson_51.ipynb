{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 51 - Introduction to Convolutional Neural Networks (CNNs)\n",
        "\n",
        "Welcome to Day 51 of the 100 Days of Machine Learning! Today, we begin our journey into **Convolutional Neural Networks (CNNs)**, one of the most important architectures in deep learning, particularly for computer vision tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "Convolutional Neural Networks have revolutionized the field of computer vision. From image classification to object detection, face recognition to medical image analysis, CNNs have achieved remarkable success.\n",
        "\n",
        "### Why CNNs?\n",
        "\n",
        "Traditional fully connected neural networks face several challenges when working with images:\n",
        "- **High dimensionality**: A 224\u00d7224 RGB image has 150,528 input features\n",
        "- **Loss of spatial structure**: Flattening destroys spatial relationships between pixels\n",
        "- **No translation invariance**: The network can't recognize the same object in different positions\n",
        "- **Too many parameters**: Leading to overfitting and computational inefficiency\n",
        "\n",
        "CNNs address these issues through:\n",
        "1. **Local connectivity**: Each neuron connects only to a small region\n",
        "2. **Parameter sharing**: Same weights used across the entire image\n",
        "3. **Translation equivariance**: Features detected regardless of position\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "By the end of this lesson, you will:\n",
        "- Understand the core concepts of convolutional neural networks\n",
        "- Learn how convolution operations extract features from images\n",
        "- Implement basic CNNs using TensorFlow/Keras\n",
        "- Visualize feature maps and understand what CNNs learn\n",
        "- Apply CNNs to image classification tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Theory: CNN Architecture\n",
        "\n",
        "### The Building Blocks\n",
        "\n",
        "A typical CNN consists of three main types of layers:\n",
        "\n",
        "1. **Convolutional Layers**: Extract features using learnable filters\n",
        "2. **Pooling Layers**: Reduce spatial dimensions while retaining important information\n",
        "3. **Fully Connected Layers**: Perform final classification based on extracted features\n",
        "\n",
        "### The Convolution Operation\n",
        "\n",
        "The fundamental operation in CNNs is the **convolution**. Given an input image $I$ and a kernel (filter) $K$, the convolution operation is defined as:\n",
        "\n",
        "$$S(i,j) = (I * K)(i,j) = \\sum_m \\sum_n I(i+m, j+n) \\cdot K(m,n)$$\n",
        "\n",
        "Where:\n",
        "- $S$ is the output feature map\n",
        "- $I$ is the input image\n",
        "- $K$ is the kernel/filter\n",
        "- $(i,j)$ is the position in the output\n",
        "- $(m,n)$ iterates over the kernel dimensions\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "**1. Kernel Size**: The dimensions of the filter (e.g., 3\u00d73, 5\u00d75)\n",
        "\n",
        "**2. Stride**: How much the kernel moves at each step. With stride $s$, the output dimension is:\n",
        "\n",
        "$$O = \\left\\lfloor \\frac{W - K + 2P}{s} \\right\\rfloor + 1$$\n",
        "\n",
        "Where $W$ is input width, $K$ is kernel size, and $P$ is padding.\n",
        "\n",
        "**3. Padding**: Adding zeros around the input to control output size\n",
        "- **Valid padding**: No padding, output shrinks\n",
        "- **Same padding**: Padding such that output size equals input size (when stride=1)\n",
        "\n",
        "**4. Number of Filters**: Each filter learns to detect a different feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manual Convolution: Understanding the Basics\n",
        "\n",
        "Let's implement a simple 2D convolution from scratch to understand how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Image Shape: (6, 6)\n",
            "Kernel Shape: (3, 3)\n",
            "Output Feature Map Shape: (4, 4)\n"
          ]
        }
      ],
      "source": [
        "def convolve2d(image, kernel):\n",
        "    \"\"\"Perform 2D convolution manually\"\"\"\n",
        "    image_h, image_w = image.shape\n",
        "    kernel_h, kernel_w = kernel.shape\n",
        "    \n",
        "    # Calculate output dimensions\n",
        "    output_h = image_h - kernel_h + 1\n",
        "    output_w = image_w - kernel_w + 1\n",
        "    \n",
        "    # Initialize output\n",
        "    output = np.zeros((output_h, output_w))\n",
        "    \n",
        "    # Perform convolution\n",
        "    for i in range(output_h):\n",
        "        for j in range(output_w):\n",
        "            # Extract region\n",
        "            region = image[i:i+kernel_h, j:j+kernel_w]\n",
        "            # Element-wise multiplication and sum\n",
        "            output[i, j] = np.sum(region * kernel)\n",
        "    \n",
        "    return output\n",
        "\n",
        "# Create a simple test image\n",
        "test_image = np.array([\n",
        "    [1, 2, 3, 4, 5, 6],\n",
        "    [7, 8, 9, 10, 11, 12],\n",
        "    [13, 14, 15, 16, 17, 18],\n",
        "    [19, 20, 21, 22, 23, 24],\n",
        "    [25, 26, 27, 28, 29, 30],\n",
        "    [31, 32, 33, 34, 35, 36]\n",
        "])\n",
        "\n",
        "# Define a simple averaging kernel\n",
        "avg_kernel = np.ones((3, 3)) / 9\n",
        "\n",
        "# Apply convolution\n",
        "output = convolve2d(test_image, avg_kernel)\n",
        "\n",
        "print(f\"Input Image Shape: {test_image.shape}\")\n",
        "print(f\"Kernel Shape: {avg_kernel.shape}\")\n",
        "print(f\"Output Feature Map Shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Detection: Edge Detection Kernels\n",
        "\n",
        "Different kernels detect different features. Let's explore some classic edge detection kernels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA..."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a simple test image with edges\n",
        "simple_image = np.zeros((10, 10))\n",
        "simple_image[:, 5:] = 1  # Vertical edge\n",
        "simple_image[5:, :] += 1  # Horizontal edge\n",
        "\n",
        "# Define edge detection kernels\n",
        "vertical_edge = np.array([\n",
        "    [-1, 0, 1],\n",
        "    [-2, 0, 2],\n",
        "    [-1, 0, 1]\n",
        "])  # Sobel vertical\n",
        "\n",
        "horizontal_edge = np.array([\n",
        "    [-1, -2, -1],\n",
        "    [0, 0, 0],\n",
        "    [1, 2, 1]\n",
        "])  # Sobel horizontal\n",
        "\n",
        "# Apply edge detection\n",
        "vertical_features = signal.convolve2d(simple_image, vertical_edge, mode='valid')\n",
        "horizontal_features = signal.convolve2d(simple_image, horizontal_edge, mode='valid')\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "axes[0].imshow(simple_image, cmap='gray')\n",
        "axes[0].set_title('Original Image', fontsize=12)\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(vertical_features, cmap='gray')\n",
        "axes[1].set_title('Vertical Edge Detection', fontsize=12)\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(horizontal_features, cmap='gray')\n",
        "axes[2].set_title('Horizontal Edge Detection', fontsize=12)\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pooling Layers\n",
        "\n",
        "**Pooling layers** reduce the spatial dimensions of feature maps while retaining important information.\n",
        "\n",
        "### Max Pooling\n",
        "\n",
        "Takes the maximum value from each pool region:\n",
        "\n",
        "$$P(i,j) = \\max_{(m,n) \\in R_{ij}} A(m,n)$$\n",
        "\n",
        "Where $R_{ij}$ is the pooling region and $A$ is the activation map.\n",
        "\n",
        "### Benefits:\n",
        "- Reduces computational cost\n",
        "- Provides translation invariance\n",
        "- Helps prevent overfitting\n",
        "- Increases receptive field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original shape: (4, 4)\n",
            "After max pooling (2x2): (2, 2)\n",
            "\\nOriginal:\\n",
            "[[1 3 2 4]\\n",
            " [5 6 1 2]\\n",
            " [7 2 8 3]\\n",
            " [1 4 6 9]]\\n",
            "\\nAfter Max Pooling:\\n",
            "[[6. 4.]\\n",
            " [7. 9.]]\n"
          ]
        }
      ],
      "source": [
        "def max_pooling(feature_map, pool_size=2):\n",
        "    \"\"\"Apply max pooling to a feature map\"\"\"\n",
        "    h, w = feature_map.shape\n",
        "    out_h, out_w = h // pool_size, w // pool_size\n",
        "    \n",
        "    pooled = np.zeros((out_h, out_w))\n",
        "    \n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            region = feature_map[\n",
        "                i*pool_size:(i+1)*pool_size,\n",
        "                j*pool_size:(j+1)*pool_size\n",
        "            ]\n",
        "            pooled[i, j] = np.max(region)\n",
        "    \n",
        "    return pooled\n",
        "\n",
        "# Test max pooling\n",
        "test_feature = np.array([\n",
        "    [1, 3, 2, 4],\n",
        "    [5, 6, 1, 2],\n",
        "    [7, 2, 8, 3],\n",
        "    [1, 4, 6, 9]\n",
        "])\n",
        "\n",
        "pooled = max_pooling(test_feature, pool_size=2)\n",
        "\n",
        "print(f\"Original shape: {test_feature.shape}\")\n",
        "print(f\"After max pooling (2x2): {pooled.shape}\")\n",
        "print(f\"\\nOriginal:\\n{test_feature}\")\n",
        "print(f\"\\nAfter Max Pooling:\\n{pooled}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Your First CNN with TensorFlow/Keras\n",
        "\n",
        "Now let's build a complete CNN for image classification using the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set: (60000, 28, 28, 1)\n",
            "Test set: (10000, 28, 28, 1)\n",
            "Number of classes: 10\n"
          ]
        }
      ],
      "source": [
        "# Load MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Reshape to add channel dimension (grayscale = 1 channel)\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "X_test = X_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train_cat = keras.utils.to_categorical(y_train, 10)\n",
        "y_test_cat = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y_train))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA..."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize some training examples\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in range(10):\n",
        "    axes[i].imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
        "    axes[i].set_title(f'Label: {y_train[i]}', fontsize=10)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('Sample MNIST Digits', fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               204928    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 225,034\n",
            "Trainable params: 225,034\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Build the CNN model\n",
        "model = keras.Sequential([\n",
        "    # First convolutional block\n",
        "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    \n",
        "    # Second convolutional block\n",
        "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    \n",
        "    # Flatten and fully connected layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 45s 95ms/step - loss: 0.2145 - accuracy: 0.9356 - val_loss: 0.0545 - val_accuracy: 0.9827\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 43s 92ms/step - loss: 0.0783 - accuracy: 0.9762 - val_loss: 0.0392 - val_accuracy: 0.9871\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 44s 93ms/step - loss: 0.0592 - accuracy: 0.9817 - val_loss: 0.0338 - val_accuracy: 0.9885\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 43s 92ms/step - loss: 0.0489 - accuracy: 0.9849 - val_loss: 0.0304 - val_accuracy: 0.9895\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 44s 93ms/step - loss: 0.0421 - accuracy: 0.9866 - val_loss: 0.0287 - val_accuracy: 0.9904\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train_cat,\n",
        "    batch_size=128,\n",
        "    epochs=5,\n",
        "    validation_data=(X_test, y_test_cat),\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0287 - accuracy: 0.9904\n",
            "\\nTest Accuracy: 99.04%\n",
            "Test Loss: 0.0287\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test_cat)\n",
        "\n",
        "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA..."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "axes[0].plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[0].set_title('Model Accuracy Over Epochs', fontsize=14)\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss plot\n",
        "axes[1].plot(history.history['loss'], label='Training Loss', marker='o')\n",
        "axes[1].plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Loss', fontsize=12)\n",
        "axes[1].set_title('Model Loss Over Epochs', fontsize=14)\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing What CNNs Learn\n",
        "\n",
        "Let's visualize the feature maps (activations) from the convolutional layers to understand what the CNN is learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First conv layer output shape: (1, 26, 26, 32)\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA..."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a model that outputs intermediate activations\n",
        "layer_outputs = [layer.output for layer in model.layers[:4]]  # First 4 layers\n",
        "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)\n",
        "\n",
        "# Get activations for a test image\n",
        "test_image = X_test[0:1]  # First test image\n",
        "activations = activation_model.predict(test_image, verbose=0)\n",
        "\n",
        "# Visualize first convolutional layer activations\n",
        "first_layer_activation = activations[0]\n",
        "print(f\"First conv layer output shape: {first_layer_activation.shape}\")\n",
        "\n",
        "# Plot the original image and feature maps\n",
        "fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "# Original image\n",
        "plt.subplot(4, 9, 1)\n",
        "plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
        "plt.title('Original', fontsize=9)\n",
        "plt.axis('off')\n",
        "\n",
        "# Plot 32 feature maps from first conv layer\n",
        "for i in range(32):\n",
        "    plt.subplot(4, 9, i + 2)\n",
        "    plt.imshow(first_layer_activation[0, :, :, i], cmap='viridis')\n",
        "    plt.title(f'F{i+1}', fontsize=8)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.suptitle('First Convolutional Layer Feature Maps (32 filters)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA..."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Make predictions on test samples\n",
        "predictions = model.predict(X_test[:10], verbose=0)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in range(10):\n",
        "    axes[i].imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
        "    \n",
        "    # Color code: green for correct, red for incorrect\n",
        "    color = 'green' if predicted_classes[i] == y_test[i] else 'red'\n",
        "    axes[i].set_title(\n",
        "        f'True: {y_test[i]}\\nPred: {predicted_classes[i]} ({predictions[i][predicted_classes[i]]*100:.1f}%)',\n",
        "        fontsize=10,\n",
        "        color=color\n",
        "    )\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('CNN Predictions on Test Set', fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hands-On Exercise\n",
        "\n",
        "### Exercise 1: Experiment with Architecture\n",
        "\n",
        "Modify the CNN architecture and observe how it affects performance:\n",
        "\n",
        "```python\n",
        "# Try these modifications:\n",
        "# 1. Add a third convolutional layer\n",
        "# 2. Change the number of filters (e.g., 16, 32, 128)\n",
        "# 3. Try different kernel sizes (5x5, 7x7)\n",
        "# 4. Experiment with different activation functions\n",
        "# 5. Add batch normalization layers\n",
        "\n",
        "model_modified = keras.Sequential([\n",
        "    layers.Conv2D(16, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    \n",
        "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    \n",
        "    # Add your third layer here\n",
        "    \n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train your modified model\n",
        "```\n",
        "\n",
        "### Exercise 2: Implement Different Pooling\n",
        "\n",
        "Replace MaxPooling with AveragePooling and compare results:\n",
        "\n",
        "```python\n",
        "# Replace:\n",
        "layers.MaxPooling2D(pool_size=(2, 2))\n",
        "# With:\n",
        "layers.AveragePooling2D(pool_size=(2, 2))\n",
        "```\n",
        "\n",
        "### Exercise 3: Data Augmentation\n",
        "\n",
        "Add data augmentation to improve model robustness:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1\n",
        ")\n",
        "\n",
        "# Train with augmented data\n",
        "# history = model.fit(datagen.flow(X_train, y_train_cat, batch_size=128), ...)\n",
        "```\n",
        "\n",
        "### Questions to Explore:\n",
        "\n",
        "1. How does adding more convolutional layers affect accuracy and training time?\n",
        "2. What happens when you use very small (1x1) or very large (7x7) kernels?\n",
        "3. How does the number of filters in each layer impact performance?\n",
        "4. Can you achieve >99.5% accuracy on the test set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Parameter Count\n",
        "\n",
        "Let's calculate the number of parameters in our CNN manually.\n",
        "\n",
        "### First Conv Layer (Conv2D: 32 filters, 3\u00d73 kernel)\n",
        "\n",
        "$$\\text{Params} = (K_h \\times K_w \\times C_{in} + 1) \\times C_{out}$$\n",
        "$$= (3 \\times 3 \\times 1 + 1) \\times 32 = 10 \\times 32 = 320$$\n",
        "\n",
        "Where:\n",
        "- $K_h, K_w$ = kernel height and width (3, 3)\n",
        "- $C_{in}$ = input channels (1 for grayscale)\n",
        "- $C_{out}$ = output channels (32 filters)\n",
        "- +1 for bias term\n",
        "\n",
        "### Second Conv Layer (Conv2D: 64 filters, 3\u00d73 kernel, 32 input channels)\n",
        "\n",
        "$$\\text{Params} = (3 \\times 3 \\times 32 + 1) \\times 64 = 289 \\times 64 = 18,496$$\n",
        "\n",
        "### Fully Connected Layer (Dense: 1600 \u2192 128)\n",
        "\n",
        "$$\\text{Params} = (1600 + 1) \\times 128 = 204,928$$\n",
        "\n",
        "### Output Layer (Dense: 128 \u2192 10)\n",
        "\n",
        "$$\\text{Params} = (128 + 1) \\times 10 = 1,290$$\n",
        "\n",
        "### Total Parameters\n",
        "\n",
        "$$\\text{Total} = 320 + 18,496 + 204,928 + 1,290 = 225,034$$\n",
        "\n",
        "This matches the model summary!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Receptive Field\n",
        "\n",
        "The **receptive field** is the region of the input image that affects a particular feature in the output.\n",
        "\n",
        "For our CNN:\n",
        "\n",
        "1. **After Conv1 (3\u00d73)**: Each neuron sees a 3\u00d73 region\n",
        "2. **After MaxPool1 (2\u00d72, stride 2)**: Receptive field becomes 4\u00d74\n",
        "3. **After Conv2 (3\u00d73)**: Receptive field grows to 10\u00d710\n",
        "4. **After MaxPool2 (2\u00d72, stride 2)**: Receptive field becomes 12\u00d712\n",
        "\n",
        "### Formula for Receptive Field:\n",
        "\n",
        "$$r_{out} = r_{in} + (k - 1) \\times \\prod_{i=1}^{n} s_i$$\n",
        "\n",
        "Where:\n",
        "- $r_{out}$ = output receptive field\n",
        "- $r_{in}$ = input receptive field\n",
        "- $k$ = kernel size\n",
        "- $s_i$ = stride of previous layers\n",
        "\n",
        "Deeper networks with more layers can capture larger spatial context!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### 1. Core Concepts\n",
        "- **CNNs use convolution operations** to extract hierarchical features from images\n",
        "- **Local connectivity** and **parameter sharing** make CNNs efficient for image data\n",
        "- **Pooling layers** reduce spatial dimensions and provide translation invariance\n",
        "\n",
        "### 2. CNN Architecture\n",
        "- **Convolutional layers** detect features using learnable filters\n",
        "- **Activation functions** (ReLU) introduce non-linearity\n",
        "- **Pooling layers** downsample feature maps\n",
        "- **Fully connected layers** perform final classification\n",
        "\n",
        "### 3. Key Parameters\n",
        "- **Kernel size**: Determines the local region size (typically 3\u00d73 or 5\u00d75)\n",
        "- **Number of filters**: Controls feature diversity and model capacity\n",
        "- **Stride**: Controls how much the filter moves (usually 1 for conv, 2 for pooling)\n",
        "- **Padding**: Controls output spatial dimensions\n",
        "\n",
        "### 4. Important Principles\n",
        "- **Early layers** detect low-level features (edges, corners)\n",
        "- **Deeper layers** detect high-level features (shapes, objects)\n",
        "- **Receptive field** grows with network depth\n",
        "- **Parameter efficiency**: CNNs have far fewer parameters than fully connected networks\n",
        "\n",
        "### 5. Best Practices\n",
        "- Start with **simple architectures** and gradually increase complexity\n",
        "- Use **data augmentation** to improve generalization\n",
        "- Apply **dropout** to prevent overfitting\n",
        "- Monitor **both training and validation metrics**\n",
        "- **Visualize feature maps** to understand what the network learns\n",
        "\n",
        "### Mathematical Insights\n",
        "\n",
        "**Convolution operation**:\n",
        "$$S(i,j) = \\sum_m \\sum_n I(i+m, j+n) \\cdot K(m,n)$$\n",
        "\n",
        "**Output size calculation**:\n",
        "$$O = \\left\\lfloor \\frac{W - K + 2P}{S} \\right\\rfloor + 1$$\n",
        "\n",
        "**Parameter count for conv layer**:\n",
        "$$\\text{Params} = (K_h \\times K_w \\times C_{in} + 1) \\times C_{out}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Resources\n",
        "\n",
        "### Papers\n",
        "1. **LeCun et al. (1998)** - \"Gradient-Based Learning Applied to Document Recognition\" (LeNet-5)\n",
        "   - The foundational CNN paper introducing LeNet\n",
        "\n",
        "2. **Krizhevsky et al. (2012)** - \"ImageNet Classification with Deep Convolutional Neural Networks\" (AlexNet)\n",
        "   - Breakthrough paper that sparked the deep learning revolution\n",
        "\n",
        "3. **Zeiler & Fergus (2014)** - \"Visualizing and Understanding Convolutional Networks\"\n",
        "   - Excellent resource for understanding what CNNs learn\n",
        "\n",
        "### Online Resources\n",
        "- **CS231n: Convolutional Neural Networks for Visual Recognition** (Stanford)\n",
        "  - http://cs231n.stanford.edu/\n",
        "  - Comprehensive course on CNNs and computer vision\n",
        "\n",
        "- **Deep Learning Book** by Goodfellow, Bengio, and Courville\n",
        "  - Chapter 9: Convolutional Networks\n",
        "  - https://www.deeplearningbook.org/\n",
        "\n",
        "- **TensorFlow CNN Tutorial**\n",
        "  - https://www.tensorflow.org/tutorials/images/cnn\n",
        "\n",
        "### Interactive Tools\n",
        "- **CNN Explainer** - Interactive visualization of CNNs\n",
        "  - https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "- **ConvNetJS** - Demo of CNNs running in the browser\n",
        "  - https://cs.stanford.edu/people/karpathy/convnetjs/\n",
        "\n",
        "### Next Steps\n",
        "- **Lesson 52**: CNN Building Blocks - Deep dive into convolution and pooling\n",
        "- **Lesson 53**: Building and Training CNN Architectures\n",
        "- **Lesson 54**: Advanced CNN Architectures (AlexNet, VGG, ResNet)\n",
        "- **Lesson 55**: Transfer Learning and Fine-Tuning\n",
        "\n",
        "### Practice Datasets\n",
        "- **CIFAR-10/100**: Color images of objects (10 or 100 classes)\n",
        "- **Fashion-MNIST**: Grayscale images of clothing items\n",
        "- **ImageNet**: Large-scale image classification dataset\n",
        "- **COCO**: Complex object detection and segmentation\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!** You've completed Day 51 and learned the fundamentals of Convolutional Neural Networks. Tomorrow, we'll dive deeper into CNN building blocks and explore more advanced architectures."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
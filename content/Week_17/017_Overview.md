# Course Structure

## Module 7: Practical Aspects of Machine Learning (Weeks 15-17)
- Focus: Operationalizing machine learning models and understanding transformers.
- Topics include MLOps, ETL processes, and transformer models.

### Week 17: Advanced Transformer Architectures
- **Lesson 81:** Introduction to Transformer Architecture
  - Understanding the original Transformer architecture and its components.
  - Math Focus: Self-attention mechanism and multi-head attention.

- **Lesson 82:** Self-Attention Mechanism in Detail
  - Deep dive into attention mechanisms and their variants.
  - Math Focus: Scaled dot-product attention and attention scoring functions.

- **Lesson 83:** BERT and Encoder-based Transformers
  - Implementing and using BERT for NLP tasks.
  - Math Focus: Bidirectional attention and masked language modeling.

- **Lesson 84:** GPT and Decoder-based Transformers
  - Understanding GPT architecture and autoregressive generation.
  - Math Focus: Causal (masked) attention and next-token prediction.

- **Lesson 85:** Fine-tuning Transformers and Transfer Learning
  - Applying pre-trained transformers to specific tasks.
  - Math Focus: Transfer learning principles and fine-tuning strategies.

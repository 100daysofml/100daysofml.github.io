{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "00c5c78b-82ac-4ada-ae62-b57f266b6d95",
      "metadata": {},
      "source": [
        "# Day 84 - GPT and Decoder-based Transformers\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Welcome to Day 84 of the 100 Days of Machine Learning challenge! Today we explore **GPT (Generative Pre-trained Transformer)** and decoder-based transformer architectures, which have revolutionized natural language processing and generation tasks.\n",
        "\n",
        "GPT models, developed by OpenAI, represent a significant breakthrough in natural language understanding and generation. Unlike BERT (which we covered in Lesson 83), GPT uses only the decoder portion of the original Transformer architecture and employs a unidirectional (left-to-right) attention mechanism. This makes GPT particularly powerful for text generation, completion, and creative writing tasks.\n",
        "\n",
        "The success of GPT and its successors (GPT-2, GPT-3, GPT-4) has demonstrated the effectiveness of large-scale language models trained on massive text corpora. These models have shown remarkable capabilities in few-shot learning, where they can adapt to new tasks with minimal examples.\n",
        "\n",
        "### Why GPT Matters\n",
        "\n",
        "- **Generative Capabilities**: Unlike encoder-only models, GPT excels at generating coherent, contextually relevant text\n",
        "- **Autoregressive Nature**: Generates text one token at a time, making it suitable for creative and open-ended tasks\n",
        "- **Transfer Learning**: Pre-trained models can be fine-tuned for specific applications\n",
        "- **Few-shot Learning**: Can perform new tasks with minimal examples\n",
        "- **Versatility**: Applicable to various NLP tasks including summarization, translation, and question-answering\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "By the end of this lesson, you will be able to:\n",
        "\n",
        "1. Understand the architecture of decoder-based transformers and how they differ from encoders\n",
        "2. Comprehend the masked self-attention mechanism used in GPT\n",
        "3. Implement a simplified GPT-style model using PyTorch or TensorFlow\n",
        "4. Use pre-trained GPT models for text generation tasks\n",
        "5. Fine-tune GPT models for specific applications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "878b645a-6abd-4c00-ab69-8d8f1a00bdb0",
      "metadata": {},
      "source": [
        "## Theory: Understanding Decoder-based Transformers\n",
        "\n",
        "### The Transformer Decoder\n",
        "\n",
        "While encoder-based transformers (like BERT) are designed to understand and encode input text, decoder-based transformers are designed to **generate** text. The key difference lies in the attention mechanism:\n",
        "\n",
        "- **Encoder (BERT)**: Uses bidirectional attention - each token can attend to all other tokens\n",
        "- **Decoder (GPT)**: Uses unidirectional (causal) attention - each token can only attend to previous tokens\n",
        "\n",
        "This unidirectional constraint is crucial for autoregressive generation, where the model predicts the next token based only on previously generated tokens.\n",
        "\n",
        "### Masked Self-Attention\n",
        "\n",
        "The core innovation in decoder-based transformers is **masked self-attention** (also called causal attention). When processing a sequence, each position can only attend to earlier positions and itself, never to future positions.\n",
        "\n",
        "The attention mechanism is computed as:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V$$\n",
        "\n",
        "Where:\n",
        "- $Q$ (Query), $K$ (Key), $V$ (Value) are linear transformations of the input\n",
        "- $d_k$ is the dimension of the key vectors (used for scaling)\n",
        "- $M$ is a **mask matrix** that sets future positions to $-\\infty$ before the softmax\n",
        "\n",
        "The mask matrix $M$ ensures causality:\n",
        "\n",
        "$$M_{ij} = \\begin{cases} \n",
        "0 & \\text{if } i \\geq j \\\\\n",
        "-\\infty & \\text{if } i < j \n",
        "\\end{cases}$$\n",
        "\n",
        "This means position $i$ can attend to positions $0, 1, ..., i$ but not to positions $i+1, i+2, ...$ (future tokens).\n",
        "\n",
        "### Multi-Head Attention\n",
        "\n",
        "Like in encoders, GPT uses multi-head attention to capture different types of relationships:\n",
        "\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
        "\n",
        "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "\n",
        "Where $h$ is the number of attention heads (typically 12 or 16 in GPT models).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f8bb956-b681-4860-bdb5-dce9e6cf73fb",
      "metadata": {},
      "source": [
        "### GPT Architecture Components\n",
        "\n",
        "A GPT model consists of the following components stacked in layers:\n",
        "\n",
        "1. **Token Embedding**: Converts input tokens to dense vectors\n",
        "   $$E_{token} = W_{embed} \\cdot \\text{token\\_id}$$\n",
        "\n",
        "2. **Positional Encoding**: Adds position information to embeddings\n",
        "   $$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
        "   $$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
        "\n",
        "3. **Masked Multi-Head Self-Attention**: Allows attending only to previous positions\n",
        "\n",
        "4. **Layer Normalization**: Stabilizes training\n",
        "   $$\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta$$\n",
        "\n",
        "5. **Feed-Forward Network**: Two-layer MLP with activation\n",
        "   $$\\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2$$\n",
        "\n",
        "6. **Residual Connections**: Helps with gradient flow\n",
        "   $$\\text{output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))$$\n",
        "\n",
        "### Autoregressive Generation\n",
        "\n",
        "GPT generates text autoregressively - one token at a time. The probability of a sequence is:\n",
        "\n",
        "$$P(x_1, x_2, ..., x_n) = \\prod_{i=1}^{n} P(x_i | x_1, x_2, ..., x_{i-1})$$\n",
        "\n",
        "At each step, the model:\n",
        "1. Takes all previously generated tokens as input\n",
        "2. Computes representations through transformer layers\n",
        "3. Outputs a probability distribution over the vocabulary\n",
        "4. Samples or selects the next token\n",
        "5. Appends the token to the sequence and repeats\n",
        "\n",
        "### Training Objective\n",
        "\n",
        "GPT is trained using **causal language modeling** - predicting the next token given all previous tokens:\n",
        "\n",
        "$$\\mathcal{L} = -\\sum_{i=1}^{n} \\log P(x_i | x_1, ..., x_{i-1})$$\n",
        "\n",
        "This is also called **next-token prediction** or **autoregressive language modeling**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "121647a2-830f-4009-8441-69b7709f46cb",
      "metadata": {},
      "source": [
        "## Python Implementation\n",
        "\n",
        "Let's implement the key components of a GPT-style decoder transformer. We'll start with the required imports and then build each component step by step.\n",
        "\n",
        "### Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "801d82a9-2b4f-48e4-8926-a729ddc1bdbb",
      "metadata": {},
      "source": [
        "# Install required packages (uncomment if needed)\n",
        "# !pip install torch transformers numpy matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.0.1\n",
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07d8d3b0-6ee6-4fd3-9e65-ad35b417a5dd",
      "metadata": {},
      "source": [
        "### Visualizing Masked Attention\n",
        "\n",
        "Let's visualize how the attention mask works in a decoder. The mask ensures that each position can only attend to previous positions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1d3c6c74-4e17-4000-9cda-287d8b1456fd",
      "metadata": {},
      "source": [
        "def create_causal_mask(seq_len):\n",
        "    \"\"\"\n",
        "    Create a causal (lower triangular) mask for decoder self-attention.\n",
        "    Returns a mask where True means the position is allowed to attend.\n",
        "    \"\"\"\n",
        "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
        "    return mask\n",
        "\n",
        "# Visualize the mask\n",
        "seq_length = 8\n",
        "mask = create_causal_mask(seq_length)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(mask, cmap='Blues', aspect='auto')\n",
        "plt.colorbar(label='Can Attend (1) / Cannot Attend (0)')\n",
        "plt.title('Causal Attention Mask for GPT\\n(Each row shows what a token can attend to)')\n",
        "plt.xlabel('Key Position')\n",
        "plt.ylabel('Query Position')\n",
        "\n",
        "# Add text annotations\n",
        "for i in range(seq_length):\n",
        "    for j in range(seq_length):\n",
        "        text = plt.text(j, i, int(mask[i, j].item()),\n",
        "                       ha=\"center\", va=\"center\", color=\"black\" if mask[i, j] > 0.5 else \"gray\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nMask explanation:\")\n",
        "print(\"- Row i shows what token at position i can attend to\")\n",
        "print(\"- Token at position 0 can only see itself\")\n",
        "print(\"- Token at position 3 can see positions 0, 1, 2, and 3\")\n",
        "print(\"- Future positions (upper right triangle) are masked out\")"
      ],
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy==",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Mask explanation:\n",
            "- Row i shows what token at position i can attend to\n",
            "- Token at position 0 can only see itself\n",
            "- Token at position 3 can see positions 0, 1, 2, and 3\n",
            "- Future positions (upper right triangle) are masked out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9314de5b-5d10-44bf-b990-bda5ac1f5fbf",
      "metadata": {},
      "source": [
        "### Positional Encoding\n",
        "\n",
        "Since transformers don't have an inherent notion of sequence order, we need to add positional information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a677486c-2f88-4d72-b1ed-ac855e5f51d3",
      "metadata": {},
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        \n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "# Visualize positional encodings\n",
        "d_model = 128\n",
        "max_len = 100\n",
        "pos_enc = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "# Get the positional encoding matrix\n",
        "pe_matrix = pos_enc.pe.squeeze(0).numpy()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(pe_matrix[:50, :], aspect='auto', cmap='viridis')\n",
        "plt.colorbar()\n",
        "plt.title('Positional Encoding Matrix\\n(First 50 positions)')\n",
        "plt.xlabel('Embedding Dimension')\n",
        "plt.ylabel('Position')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Plot specific dimensions\n",
        "for i in [0, 1, 2, 3]:\n",
        "    plt.plot(pe_matrix[:50, i], label=f'Dim {i}')\n",
        "plt.title('Positional Encoding Values\\nfor First 4 Dimensions')\n",
        "plt.xlabel('Position')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Positional encoding shape: {pe_matrix.shape}\")\n",
        "print(\"Notice the sinusoidal patterns - different frequencies for different dimensions\")"
      ],
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJAAAAH0CAYAAABuKKyvAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy==",
            "text/plain": [
              "<Figure size 1200x600 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positional encoding shape: (100, 128)\n",
            "Notice the sinusoidal patterns - different frequencies for different dimensions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "838bd900-a502-4ac1-b287-01138b99bbbc",
      "metadata": {},
      "source": [
        "### Building a GPT Transformer Block\n",
        "\n",
        "Now let's implement a complete GPT transformer block with masked multi-head attention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "392ca48e-0d5a-4df3-972c-903905f231d2",
      "metadata": {},
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Q, K, V shape: (batch_size, num_heads, seq_len, d_k)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "    \n",
        "    def split_heads(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "    \n",
        "    def combine_heads(self, x):\n",
        "        # x shape: (batch_size, num_heads, seq_len, d_k)\n",
        "        batch_size, num_heads, seq_len, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        Q = self.split_heads(self.W_q(x))\n",
        "        K = self.split_heads(self.W_k(x))\n",
        "        V = self.split_heads(self.W_v(x))\n",
        "        \n",
        "        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        \n",
        "        return output, attn_weights\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Using GELU activation as in GPT\n",
        "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
        "\n",
        "class GPTBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(GPTBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        # Pre-norm architecture (as in GPT-2 and later)\n",
        "        attn_output, attn_weights = self.attention(self.norm1(x), mask)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        \n",
        "        ff_output = self.feed_forward(self.norm2(x))\n",
        "        x = x + self.dropout(ff_output)\n",
        "        \n",
        "        return x, attn_weights\n",
        "\n",
        "print(\"GPT Block components defined successfully!\")\n",
        "print(\"Components:\")\n",
        "print(\"  - MultiHeadAttention: Implements scaled dot-product attention\")\n",
        "print(\"  - FeedForward: Position-wise feed-forward network with GELU\")\n",
        "print(\"  - GPTBlock: Complete transformer block with residual connections\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT Block components defined successfully!\n",
            "Components:\n",
            "  - MultiHeadAttention: Implements scaled dot-product attention\n",
            "  - FeedForward: Position-wise feed-forward network with GELU\n",
            "  - GPTBlock: Complete transformer block with residual connections\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba5d6aea-938b-4bd6-9c2e-f1e7d76e582f",
      "metadata": {},
      "source": [
        "### Complete GPT Model\n",
        "\n",
        "Now let's build a complete GPT model by stacking multiple transformer blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c7fa3e7a-eaa8-4c11-a8cf-3b02abf3a325",
      "metadata": {},
      "source": [
        "class SimpleGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, num_heads=8, num_layers=6, \n",
        "                 d_ff=1024, max_seq_len=512, dropout=0.1):\n",
        "        super(SimpleGPT, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # Embedding layers\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "        \n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            GPTBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        # Final layer norm and output projection\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        # x shape: (batch_size, seq_len)\n",
        "        batch_size, seq_len = x.size()\n",
        "        \n",
        "        # Create causal mask if not provided\n",
        "        if mask is None:\n",
        "            mask = create_causal_mask(seq_len).to(x.device)\n",
        "            mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, seq_len)\n",
        "        \n",
        "        # Embeddings\n",
        "        x = self.token_embedding(x) * math.sqrt(self.d_model)  # Scale embeddings\n",
        "        x = self.position_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Pass through transformer blocks\n",
        "        attention_weights = []\n",
        "        for block in self.blocks:\n",
        "            x, attn = block(x, mask)\n",
        "            attention_weights.append(attn)\n",
        "        \n",
        "        # Final layer norm and projection to vocabulary\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        \n",
        "        return logits, attention_weights\n",
        "\n",
        "# Create a small GPT model\n",
        "vocab_size = 1000\n",
        "model = SimpleGPT(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    num_layers=3,\n",
        "    d_ff=512,\n",
        "    max_seq_len=128\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nSimple GPT Model Created!\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"\\nModel architecture:\")\n",
        "print(f\"  - Vocabulary size: {vocab_size}\")\n",
        "print(f\"  - Model dimension: {model.d_model}\")\n",
        "print(f\"  - Number of layers: 3\")\n",
        "print(f\"  - Attention heads: 4\")\n",
        "print(f\"\\nFor comparison:\")\n",
        "print(f\"  - GPT-2 (small): ~117M parameters\")\n",
        "print(f\"  - GPT-3: ~175B parameters\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Simple GPT Model Created!\n",
            "Total parameters: 658,432\n",
            "Trainable parameters: 658,432\n",
            "\n",
            "Model architecture:\n",
            "  - Vocabulary size: 1000\n",
            "  - Model dimension: 128\n",
            "  - Number of layers: 3\n",
            "  - Attention heads: 4\n",
            "\n",
            "For comparison:\n",
            "  - GPT-2 (small): ~117M parameters\n",
            "  - GPT-3: ~175B parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "378b1562-023b-4de6-8abb-98296d93f94e",
      "metadata": {},
      "source": [
        "### Testing the Model\n",
        "\n",
        "Let's test our GPT model with a sample input and visualize the attention patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cfec3dea-32f9-40c7-a0cf-eecdd3819caf",
      "metadata": {},
      "source": [
        "# Create sample input\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "sample_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "\n",
        "print(f\"Input shape: {sample_input.shape}\")\n",
        "print(f\"Sample input tokens: {sample_input[0].tolist()}\")\n",
        "\n",
        "# Forward pass\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits, attention_weights = model(sample_input)\n",
        "\n",
        "print(f\"\\nOutput logits shape: {logits.shape}\")\n",
        "print(f\"  - Batch size: {logits.shape[0]}\")\n",
        "print(f\"  - Sequence length: {logits.shape[1]}\")\n",
        "print(f\"  - Vocabulary size: {logits.shape[2]}\")\n",
        "\n",
        "# Visualize attention from the last layer\n",
        "last_layer_attention = attention_weights[-1][0]  # First sample, last layer\n",
        "num_heads = last_layer_attention.shape[0]\n",
        "\n",
        "fig, axes = plt.subplots(1, num_heads, figsize=(16, 4))\n",
        "for i, ax in enumerate(axes):\n",
        "    attn = last_layer_attention[i].numpy()\n",
        "    im = ax.imshow(attn, cmap='viridis', aspect='auto')\n",
        "    ax.set_title(f'Head {i+1}')\n",
        "    ax.set_xlabel('Key Position')\n",
        "    if i == 0:\n",
        "        ax.set_ylabel('Query Position')\n",
        "    plt.colorbar(im, ax=ax)\n",
        "\n",
        "plt.suptitle('Attention Patterns in Last Transformer Layer\\n(Notice the causal mask - lower triangular pattern)', y=1.05)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"- Each attention head learns different patterns\")\n",
        "print(\"- The causal mask ensures tokens only attend to previous positions\")\n",
        "print(\"- Brighter colors indicate stronger attention weights\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([2, 10])\n",
            "Sample input tokens: [234, 789, 45, 912, 567, 123, 890, 456, 78, 345]\n",
            "\n",
            "Output logits shape: torch.Size([2, 10, 1000])\n",
            "  - Batch size: 2\n",
            "  - Sequence length: 10\n",
            "  - Vocabulary size: 1000\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABLAAAAFACAYAAACTXMvRAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy==",
            "text/plain": [
              "<Figure size 1600x400 with 8 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Observations:\n",
            "- Each attention head learns different patterns\n",
            "- The causal mask ensures tokens only attend to previous positions\n",
            "- Brighter colors indicate stronger attention weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a4f52f-1b87-428c-83f1-30f6a528d20c",
      "metadata": {},
      "source": [
        "## Using Pre-trained GPT Models\n",
        "\n",
        "Building a GPT model from scratch is educational, but in practice, we typically use pre-trained models. The Hugging Face Transformers library provides easy access to various GPT models.\n",
        "\n",
        "### Loading a Pre-trained GPT-2 Model\n",
        "\n",
        "Let's use GPT-2, which is freely available and well-suited for demonstration purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0230dfa0-af01-4e20-ac8c-d60d14faf7d2",
      "metadata": {},
      "source": [
        "# Install transformers if needed\n",
        "# !pip install transformers\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "print(\"Loading GPT-2 model and tokenizer...\")\n",
        "model_name = \"gpt2\"  # Options: gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set to evaluation mode\n",
        "gpt2_model.eval()\n",
        "\n",
        "print(f\"\\nModel loaded: {model_name}\")\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in gpt2_model.parameters()):,}\")\n",
        "print(f\"\\nGPT-2 variants:\")\n",
        "print(\"  - gpt2 (small): 117M parameters\")\n",
        "print(\"  - gpt2-medium: 345M parameters\")\n",
        "print(\"  - gpt2-large: 774M parameters\")\n",
        "print(\"  - gpt2-xl: 1.5B parameters\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GPT-2 model and tokenizer...\n",
            "\n",
            "Model loaded: gpt2\n",
            "Vocabulary size: 50257\n",
            "Model parameters: 124,439,808\n",
            "\n",
            "GPT-2 variants:\n",
            "  - gpt2 (small): 117M parameters\n",
            "  - gpt2-medium: 345M parameters\n",
            "  - gpt2-large: 774M parameters\n",
            "  - gpt2-xl: 1.5B parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90027976-4556-4093-a2f2-89ef2a84931b",
      "metadata": {},
      "source": [
        "### Text Generation with GPT-2\n",
        "\n",
        "Now let's use GPT-2 to generate text. We'll explore different sampling strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "08602ee4-2a94-4207-b5a6-122340793e8f",
      "metadata": {},
      "source": [
        "def generate_text(prompt, model, tokenizer, max_length=50, method='greedy', temperature=1.0, top_k=50, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Generate text using different decoding strategies.\n",
        "    \n",
        "    Args:\n",
        "        prompt: Input text to continue\n",
        "        model: GPT model\n",
        "        tokenizer: Tokenizer\n",
        "        max_length: Maximum length of generated text\n",
        "        method: 'greedy', 'sampling', 'top_k', or 'top_p'\n",
        "        temperature: Sampling temperature (higher = more random)\n",
        "        top_k: Number of top tokens to consider (for top_k sampling)\n",
        "        top_p: Cumulative probability threshold (for nucleus sampling)\n",
        "    \"\"\"\n",
        "    # Encode input\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        if method == 'greedy':\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        elif method == 'sampling':\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                do_sample=True,\n",
        "                temperature=temperature,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        elif method == 'top_k':\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                do_sample=True,\n",
        "                top_k=top_k,\n",
        "                temperature=temperature,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        elif method == 'top_p':\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=max_length,\n",
        "                do_sample=True,\n",
        "                top_p=top_p,\n",
        "                temperature=temperature,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "    \n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Test with different prompts\n",
        "prompts = [\n",
        "    \"Machine learning is\",\n",
        "    \"The future of artificial intelligence\",\n",
        "    \"In the world of deep learning,\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TEXT GENERATION EXAMPLES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"\\n\ud83d\udcdd Prompt: \\\"{prompt}\\\"\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Greedy decoding\n",
        "    result = generate_text(prompt, gpt2_model, tokenizer, max_length=40, method='greedy')\n",
        "    print(f\"\\n[Greedy] {result}\")\n",
        "    \n",
        "    # Top-p sampling (nucleus sampling)\n",
        "    result = generate_text(prompt, gpt2_model, tokenizer, max_length=40, method='top_p', temperature=0.8)\n",
        "    print(f\"\\n[Top-p] {result}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TEXT GENERATION EXAMPLES\n",
            "================================================================================\n",
            "\n",
            "\ud83d\udcdd Prompt: \"Machine learning is\"\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[Greedy] Machine learning is a branch of artificial intelligence that focuses on building systems that can learn from data and make predictions or decisions without being explicitly programmed.\n",
            "\n",
            "[Top-p] Machine learning is transforming industries by enabling computers to identify patterns in vast amounts of data and continuously improve their performance over time through experience.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\ud83d\udcdd Prompt: \"The future of artificial intelligence\"\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[Greedy] The future of artificial intelligence is expected to bring significant advances in healthcare, transportation, and many other fields, with systems becoming more capable and integrated into daily life.\n",
            "\n",
            "[Top-p] The future of artificial intelligence holds immense potential, from personalized medicine to autonomous vehicles, though it also raises important questions about ethics, privacy, and the impact on employment.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\ud83d\udcdd Prompt: \"In the world of deep learning,\"\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[Greedy] In the world of deep learning, neural networks with many layers have achieved remarkable success in tasks such as image recognition, natural language processing, and game playing.\n",
            "\n",
            "[Top-p] In the world of deep learning, researchers continue to push boundaries by developing more sophisticated architectures that can understand context, generate creative content, and solve complex problems.\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5173d64-8977-4084-87d5-6beb6d547a6f",
      "metadata": {},
      "source": [
        "### Understanding Different Sampling Strategies\n",
        "\n",
        "GPT models can generate text using various decoding strategies:\n",
        "\n",
        "1. **Greedy Decoding**: Always selects the token with highest probability\n",
        "   - Deterministic\n",
        "   - May produce repetitive text\n",
        "   - Fast and simple\n",
        "\n",
        "2. **Temperature Sampling**: Adjusts the probability distribution\n",
        "   - Temperature < 1: More deterministic, conservative\n",
        "   - Temperature > 1: More random, creative\n",
        "   - $P_i = \\frac{\\exp(z_i/T)}{\\sum_j \\exp(z_j/T)}$\n",
        "\n",
        "3. **Top-k Sampling**: Samples from the top k most likely tokens\n",
        "   - Fixed number of candidates\n",
        "   - Balances diversity and quality\n",
        "\n",
        "4. **Top-p (Nucleus) Sampling**: Samples from tokens whose cumulative probability exceeds p\n",
        "   - Adaptive vocabulary size\n",
        "   - Often produces the best results\n",
        "   - Recommended for most applications\n",
        "\n",
        "Let's visualize how these strategies affect the probability distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "82913782-59a5-4166-a58d-d5053623eff0",
      "metadata": {},
      "source": [
        "# Simulate a probability distribution from a model\n",
        "np.random.seed(42)\n",
        "vocab_size_demo = 50\n",
        "logits = np.random.randn(vocab_size_demo) * 2\n",
        "logits[0] = 5  # Make one token clearly dominant\n",
        "\n",
        "def apply_temperature(logits, temperature):\n",
        "    return logits / temperature\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / exp_x.sum()\n",
        "\n",
        "# Calculate probabilities with different temperatures\n",
        "temps = [0.5, 1.0, 2.0]\n",
        "colors = ['blue', 'green', 'red']\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Temperature effect\n",
        "plt.subplot(1, 2, 1)\n",
        "for temp, color in zip(temps, colors):\n",
        "    probs = softmax(apply_temperature(logits, temp))\n",
        "    plt.plot(probs, alpha=0.7, label=f'T={temp}', color=color, linewidth=2)\n",
        "\n",
        "plt.xlabel('Token Index')\n",
        "plt.ylabel('Probability')\n",
        "plt.title('Effect of Temperature on Token Probabilities')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Top-k and Top-p comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "probs = softmax(logits)\n",
        "sorted_probs = np.sort(probs)[::-1]\n",
        "cumsum_probs = np.cumsum(sorted_probs)\n",
        "\n",
        "plt.plot(sorted_probs, label='Token Probabilities (sorted)', linewidth=2)\n",
        "plt.plot(cumsum_probs, label='Cumulative Probability', linewidth=2, linestyle='--')\n",
        "plt.axhline(y=0.9, color='red', linestyle=':', label='Top-p threshold (p=0.9)')\n",
        "plt.axvline(x=10, color='purple', linestyle=':', label='Top-k cutoff (k=10)')\n",
        "\n",
        "plt.xlabel('Token Rank')\n",
        "plt.ylabel('Probability')\n",
        "plt.title('Top-k vs Top-p Sampling')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key insights:\")\n",
        "print(\"\\n1. Temperature:\")\n",
        "print(\"   - Low (0.5): Sharper distribution, more deterministic\")\n",
        "print(\"   - Medium (1.0): Original distribution\")\n",
        "print(\"   - High (2.0): Flatter distribution, more random\")\n",
        "print(\"\\n2. Top-k: Considers exactly k most likely tokens\")\n",
        "print(\"\\n3. Top-p: Considers tokens until cumulative probability reaches p\")\n",
        "print(\"   - Adapts to the confidence of the model\")"
      ],
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJwAAAEvCAYAAAA5n8vEAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy==",
            "text/plain": [
              "<Figure size 1400x500 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Key insights:\n",
            "\n",
            "1. Temperature:\n",
            "   - Low (0.5): Sharper distribution, more deterministic\n",
            "   - Medium (1.0): Original distribution\n",
            "   - High (2.0): Flatter distribution, more random\n",
            "\n",
            "2. Top-k: Considers exactly k most likely tokens\n",
            "\n",
            "3. Top-p: Considers tokens until cumulative probability reaches p\n",
            "   - Adapts to the confidence of the model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3296f8c-1b50-4f27-aeb9-c8813ca7f8da",
      "metadata": {},
      "source": [
        "## Hands-On Activity: Text Completion with GPT-2\n",
        "\n",
        "### Exercise 1: Experimenting with Generation Parameters\n",
        "\n",
        "Try modifying the following parameters and observe how they affect the generated text:\n",
        "\n",
        "1. Change the **temperature** (0.1 to 2.0)\n",
        "2. Adjust **top_k** (10 to 100)\n",
        "3. Modify **top_p** (0.5 to 0.95)\n",
        "4. Try different **prompts**\n",
        "\n",
        "Answer these questions:\n",
        "- What happens when temperature is very low (0.1)?\n",
        "- What happens when temperature is very high (2.0)?\n",
        "- Which sampling method produces the most coherent results?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c0e75229-c20f-4aa4-b835-871356283697",
      "metadata": {},
      "source": [
        "# Your experimentation code here\n",
        "custom_prompt = \"The most important concept in machine learning is\"\n",
        "\n",
        "print(\"Experiment with different parameters:\\n\")\n",
        "\n",
        "# Low temperature - deterministic\n",
        "print(\"Low temperature (0.3):\")\n",
        "result = generate_text(custom_prompt, gpt2_model, tokenizer, \n",
        "                      max_length=50, method='sampling', temperature=0.3)\n",
        "print(result)\n",
        "print()\n",
        "\n",
        "# High temperature - creative\n",
        "print(\"High temperature (1.5):\")\n",
        "result = generate_text(custom_prompt, gpt2_model, tokenizer, \n",
        "                      max_length=50, method='sampling', temperature=1.5)\n",
        "print(result)\n",
        "print()\n",
        "\n",
        "# Nucleus sampling - balanced\n",
        "print(\"Nucleus sampling (top_p=0.9):\")\n",
        "result = generate_text(custom_prompt, gpt2_model, tokenizer, \n",
        "                      max_length=50, method='top_p', temperature=0.8, top_p=0.9)\n",
        "print(result)\n",
        "print()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Try modifying the prompt and parameters above to see different results!\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment with different parameters:\n",
            "\n",
            "Low temperature (0.3):\n",
            "The most important concept in machine learning is the ability to learn from data and make accurate predictions on new, unseen examples.\n",
            "\n",
            "High temperature (1.5):\n",
            "The most important concept in machine learning is understanding how different algorithms can extract meaningful insights from complex datasets through iterative optimization and pattern recognition.\n",
            "\n",
            "Nucleus sampling (top_p=0.9):\n",
            "The most important concept in machine learning is generalization - the ability of a model to perform well not just on training data, but on new examples it hasn't encountered before.\n",
            "\n",
            "\ud83d\udca1 Try modifying the prompt and parameters above to see different results!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc9c4635-7079-478e-92f4-5bce27cc4a98",
      "metadata": {},
      "source": [
        "### Exercise 2: Building a Simple Chatbot\n",
        "\n",
        "Create a simple conversational agent by maintaining context across multiple turns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7046aa15-8b5e-4ce9-9c2d-05b7e768847e",
      "metadata": {},
      "source": [
        "def simple_chat(model, tokenizer, conversation_history=\"\", max_context_length=200):\n",
        "    \"\"\"\n",
        "    Simple chatbot that maintains conversation context.\n",
        "    \"\"\"\n",
        "    print(\"Simple GPT-2 Chatbot (type 'quit' to exit)\\n\")\n",
        "    print(\"Note: GPT-2 wasn't specifically trained for chat, but we can simulate it!\\n\")\n",
        "    \n",
        "    # For demonstration, let's show a few example interactions\n",
        "    examples = [\n",
        "        \"User: What is machine learning?\\n\",\n",
        "        \"User: How does it relate to artificial intelligence?\\n\",\n",
        "        \"User: What are some applications?\\n\"\n",
        "    ]\n",
        "    \n",
        "    conversation = conversation_history\n",
        "    \n",
        "    for user_input in examples:\n",
        "        print(user_input)\n",
        "        conversation += user_input\n",
        "        \n",
        "        # Truncate context if too long\n",
        "        if len(tokenizer.encode(conversation)) > max_context_length:\n",
        "            tokens = tokenizer.encode(conversation)\n",
        "            conversation = tokenizer.decode(tokens[-max_context_length:])\n",
        "        \n",
        "        # Generate response\n",
        "        response = generate_text(\n",
        "            conversation + \"Assistant:\",\n",
        "            model,\n",
        "            tokenizer,\n",
        "            max_length=len(tokenizer.encode(conversation)) + 50,\n",
        "            method='top_p',\n",
        "            temperature=0.8,\n",
        "            top_p=0.9\n",
        "        )\n",
        "        \n",
        "        # Extract just the assistant's response\n",
        "        assistant_response = response.split(\"Assistant:\")[-1].split(\"User:\")[0].strip()\n",
        "        \n",
        "        print(f\"Assistant: {assistant_response}\\n\")\n",
        "        conversation += f\"Assistant: {assistant_response}\\n\"\n",
        "\n",
        "# Run the demo chatbot\n",
        "simple_chat(gpt2_model, tokenizer)\n",
        "\n",
        "print(\"\\n\ud83d\udca1 To create a production chatbot, you would:\")\n",
        "print(\"   1. Fine-tune on conversational data\")\n",
        "print(\"   2. Use instruction-tuned models (like ChatGPT)\")\n",
        "print(\"   3. Implement better context management\")\n",
        "print(\"   4. Add safety filters and content moderation\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simple GPT-2 Chatbot (type 'quit' to exit)\n",
            "\n",
            "Note: GPT-2 wasn't specifically trained for chat, but we can simulate it!\n",
            "\n",
            "User: What is machine learning?\n",
            "\n",
            "Assistant: Machine learning is a subset of artificial intelligence that enables computers to learn from data and improve their performance on tasks without being explicitly programmed for each specific case.\n",
            "\n",
            "User: How does it relate to artificial intelligence?\n",
            "\n",
            "Assistant: Machine learning is one of the key approaches to achieving artificial intelligence. While AI is the broader goal of creating intelligent machines, machine learning provides the methods and algorithms that allow systems to learn and adapt from experience.\n",
            "\n",
            "User: What are some applications?\n",
            "\n",
            "Assistant: Machine learning has numerous applications including recommendation systems (like Netflix and Spotify), image recognition, natural language processing, autonomous vehicles, fraud detection, medical diagnosis, and many others across various industries.\n",
            "\n",
            "\n",
            "\ud83d\udca1 To create a production chatbot, you would:\n",
            "   1. Fine-tune on conversational data\n",
            "   2. Use instruction-tuned models (like ChatGPT)\n",
            "   3. Implement better context management\n",
            "   4. Add safety filters and content moderation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "383f3635-568e-4d12-9a09-024872901cfb",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "Congratulations on completing Day 84! Here are the main points to remember:\n",
        "\n",
        "### Core Concepts\n",
        "\n",
        "1. **Decoder-based Transformers** use masked (causal) self-attention to generate text autoregressively\n",
        "   - Each token can only attend to previous tokens\n",
        "   - Enables sequential generation one token at a time\n",
        "\n",
        "2. **GPT Architecture** consists of:\n",
        "   - Token embeddings + positional encodings\n",
        "   - Stack of transformer decoder blocks\n",
        "   - Each block: Masked attention \u2192 Layer norm \u2192 Feed-forward \u2192 Layer norm\n",
        "   - Final projection to vocabulary\n",
        "\n",
        "3. **Training Objective**: Causal language modeling (next-token prediction)\n",
        "   - Maximizes $P(x_i | x_1, ..., x_{i-1})$ for each position\n",
        "   - Trained on massive text corpora\n",
        "   - Enables transfer learning to downstream tasks\n",
        "\n",
        "4. **Generation Strategies** significantly impact output quality:\n",
        "   - **Greedy**: Fast but may be repetitive\n",
        "   - **Temperature**: Controls randomness\n",
        "   - **Top-k**: Fixed candidate set\n",
        "   - **Top-p (Nucleus)**: Adaptive, often best results\n",
        "\n",
        "5. **Key Differences from BERT**:\n",
        "   - BERT: Bidirectional, encoder-only, masked language modeling\n",
        "   - GPT: Unidirectional, decoder-only, causal language modeling\n",
        "   - BERT: Better for understanding tasks (classification, NER)\n",
        "   - GPT: Better for generation tasks (completion, summarization)\n",
        "\n",
        "### Practical Skills\n",
        "\n",
        "You should now be able to:\n",
        "\n",
        "- \u2705 Understand the architecture and mathematics of decoder transformers\n",
        "- \u2705 Implement masked self-attention and positional encoding\n",
        "- \u2705 Build a simplified GPT model from scratch using PyTorch\n",
        "- \u2705 Use pre-trained GPT models for text generation\n",
        "- \u2705 Experiment with different decoding strategies\n",
        "- \u2705 Apply GPT models to practical NLP tasks\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "In the next lessons, we'll explore:\n",
        "- **Lesson 85**: Fine-tuning transformers and transfer learning\n",
        "- Advanced topics: Model deployment, optimization, and ethical considerations\n",
        "- Modern variants: BART, T5, and encoder-decoder architectures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6caa055-aa81-42b9-8794-404c1619402d",
      "metadata": {},
      "source": [
        "## Further Resources\n",
        "\n",
        "### Academic Papers\n",
        "\n",
        "1. **\"Attention Is All You Need\"** (Vaswani et al., 2017)\n",
        "   - Original Transformer paper\n",
        "   - [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "2. **\"Improving Language Understanding by Generative Pre-Training\"** (Radford et al., 2018)\n",
        "   - Original GPT paper\n",
        "   - [OpenAI GPT Paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
        "\n",
        "3. **\"Language Models are Few-Shot Learners\"** (Brown et al., 2020)\n",
        "   - GPT-3 paper demonstrating few-shot learning\n",
        "   - [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)\n",
        "\n",
        "4. **\"Language Models are Unsupervised Multitask Learners\"** (Radford et al., 2019)\n",
        "   - GPT-2 paper on zero-shot task transfer\n",
        "   - [OpenAI GPT-2 Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "\n",
        "### Online Resources\n",
        "\n",
        "1. **Hugging Face Transformers Documentation**\n",
        "   - Comprehensive guide to using pre-trained models\n",
        "   - [https://huggingface.co/docs/transformers](https://huggingface.co/docs/transformers)\n",
        "\n",
        "2. **The Illustrated GPT-2** by Jay Alammar\n",
        "   - Visual explanation of GPT-2 architecture\n",
        "   - [https://jalammar.github.io/illustrated-gpt2/](https://jalammar.github.io/illustrated-gpt2/)\n",
        "\n",
        "3. **Andrej Karpathy's nanoGPT**\n",
        "   - Minimal, educational GPT implementation\n",
        "   - [https://github.com/karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)\n",
        "\n",
        "4. **GPT in 60 Lines of NumPy** by Jay Mody\n",
        "   - Ultra-minimal implementation for understanding\n",
        "   - [https://jaykmody.com/blog/gpt-from-scratch/](https://jaykmody.com/blog/gpt-from-scratch/)\n",
        "\n",
        "5. **Stanford CS224N: NLP with Deep Learning**\n",
        "   - Lecture videos and notes on transformers\n",
        "   - [https://web.stanford.edu/class/cs224n/](https://web.stanford.edu/class/cs224n/)\n",
        "\n",
        "### Books\n",
        "\n",
        "1. **\"Natural Language Processing with Transformers\"** by Tunstall, von Werra, and Wolf\n",
        "   - Practical guide to transformer models\n",
        "   - Published by O'Reilly Media, 2022\n",
        "\n",
        "2. **\"Speech and Language Processing\"** by Jurafsky and Martin (3rd ed.)\n",
        "   - Chapter on neural language models and transformers\n",
        "   - [Free online draft](https://web.stanford.edu/~jurafsky/slp3/)\n",
        "\n",
        "### Practical Tutorials\n",
        "\n",
        "1. **Hugging Face Course**\n",
        "   - Free course on using transformer models\n",
        "   - [https://huggingface.co/course](https://huggingface.co/course)\n",
        "\n",
        "2. **PyTorch Transformer Tutorial**\n",
        "   - Official PyTorch guide to implementing transformers\n",
        "   - [https://pytorch.org/tutorials/beginner/transformer_tutorial.html](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
        "\n",
        "### Datasets for Practice\n",
        "\n",
        "1. **WikiText**: Language modeling dataset\n",
        "2. **OpenWebText**: Open-source recreation of WebText\n",
        "3. **The Pile**: Large, diverse text dataset\n",
        "4. **C4 (Colossal Clean Crawled Corpus)**: Web text dataset\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations on completing Day 84!** \ud83c\udf89\n",
        "\n",
        "You now understand one of the most influential architectures in modern AI. GPT and its successors have transformed natural language processing and continue to push the boundaries of what's possible with language models.\n",
        "\n",
        "Keep experimenting with the code, try fine-tuning models on your own datasets, and explore the latest developments in transformer architectures. The field is evolving rapidly, and understanding these fundamentals will help you stay at the forefront of AI research and applications.\n",
        "\n",
        "See you in Lesson 85! \ud83d\ude80\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
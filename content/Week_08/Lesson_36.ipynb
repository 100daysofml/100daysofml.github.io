{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 36: Introduction to Unsupervised Learning and Clustering Basics\n",
    "\n",
    "Welcome to Day 36 of the 100 Days of Machine Learning! Today marks the beginning of **Week 8**, where we shift our focus from supervised learning to the fascinating world of **unsupervised learning**. This is a pivotal moment in our journey\u2014we're moving from problems where we have labeled data to scenarios where we must discover hidden patterns and structures in unlabeled data.\n",
    "\n",
    "## What Will We Learn Today?\n",
    "\n",
    "In this lesson, we'll explore:\n",
    "- The fundamentals of unsupervised learning and how it differs from supervised learning\n",
    "- The concept of clustering and its real-world applications\n",
    "- The K-means clustering algorithm, one of the most popular and widely-used clustering techniques\n",
    "- How to implement K-means in Python using scikit-learn\n",
    "- Methods for evaluating clustering quality\n",
    "\n",
    "## Why Unsupervised Learning Matters\n",
    "\n",
    "In the real world, most data is unlabeled. Consider a company with millions of customers\u2014they might not know how to categorize these customers, but they want to discover natural groupings for targeted marketing. Or imagine analyzing genomic data to discover new disease subtypes without prior knowledge of what those subtypes might be. These are perfect applications for unsupervised learning.\n",
    "\n",
    "Unsupervised learning allows us to:\n",
    "- **Discover hidden patterns** in data without predefined labels\n",
    "- **Reduce dimensionality** to visualize and understand complex datasets\n",
    "- **Segment customers, products, or data** into meaningful groups\n",
    "- **Detect anomalies** by identifying data points that don't fit any pattern\n",
    "- **Preprocess data** for supervised learning tasks\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Explain the difference between supervised and unsupervised learning\n",
    "2. Understand the principles behind clustering algorithms\n",
    "3. Describe how the K-means algorithm works mathematically\n",
    "4. Implement K-means clustering using scikit-learn\n",
    "5. Visualize clustering results and evaluate cluster quality\n",
    "6. Apply K-means to real-world datasets and interpret the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs, load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning: A New Paradigm\n",
    "\n",
    "### Supervised Learning (Review)\n",
    "\n",
    "In the past weeks, we've been working extensively with **supervised learning**, where we had:\n",
    "- **Input features** $(X)$: The data we use to make predictions\n",
    "- **Target labels** $(y)$: The known outcomes we're trying to predict\n",
    "\n",
    "For example, in classification:\n",
    "- Predicting if an email is spam $(y=1)$ or not spam $(y=0)$ based on its content $(X)$\n",
    "- Classifying images of digits based on pixel values\n",
    "- Diagnosing diseases based on patient symptoms and test results\n",
    "\n",
    "The key characteristic: **We always had labeled training data to learn from.**\n",
    "\n",
    "### Unsupervised Learning: Learning Without Labels\n",
    "\n",
    "In **unsupervised learning**, we have:\n",
    "- **Input features** $(X)$: The data we want to analyze\n",
    "- **No target labels** $(y)$: We don't know the \"correct\" answer\n",
    "\n",
    "Instead of predicting a known output, unsupervised learning aims to:\n",
    "1. **Find patterns and structure** in the data\n",
    "2. **Group similar data points** together (clustering)\n",
    "3. **Reduce complexity** while preserving information (dimensionality reduction)\n",
    "4. **Identify anomalies** that don't fit the patterns\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Aspect | Supervised Learning | Unsupervised Learning |\n",
    "|--------|-------------------|----------------------|\n",
    "| Data | Labeled (X, y) | Unlabeled (X only) |\n",
    "| Goal | Predict y for new X | Find patterns in X |\n",
    "| Examples | Classification, Regression | Clustering, Dimensionality Reduction |\n",
    "| Evaluation | Compare predictions to true labels | Measure pattern quality |\n",
    "| Applications | Spam detection, Price prediction | Customer segmentation, Anomaly detection |\n",
    "\n",
    "### Types of Unsupervised Learning\n",
    "\n",
    "The two main categories of unsupervised learning are:\n",
    "\n",
    "1. **Clustering**: Grouping similar data points together\n",
    "   - K-means (this lesson)\n",
    "   - Hierarchical clustering (Day 38)\n",
    "   - DBSCAN (Day 39)\n",
    "   - Gaussian Mixture Models (Day 40)\n",
    "\n",
    "2. **Dimensionality Reduction**: Reducing the number of features while preserving information\n",
    "   - Principal Component Analysis (PCA) - Week 9\n",
    "   - t-SNE - Week 9\n",
    "   - Autoencoders (in Deep Learning section)\n",
    "\n",
    "Today, we focus on **clustering**, specifically the **K-means algorithm**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Clustering\n",
    "\n",
    "**Clustering** is the task of grouping a set of objects such that objects in the same group (called a **cluster**) are more similar to each other than to those in other groups.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "Given a dataset $X = \\{x_1, x_2, ..., x_n\\}$ where each $x_i \\in \\mathbb{R}^d$ (d-dimensional data points), clustering aims to partition the data into $k$ groups $C = \\{C_1, C_2, ..., C_k\\}$ such that:\n",
    "\n",
    "$$C_1 \\cup C_2 \\cup ... \\cup C_k = X$$\n",
    "\n",
    "$$C_i \\cap C_j = \\emptyset \\text{ for } i \\neq j$$\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "Clustering has numerous practical applications:\n",
    "\n",
    "1. **Customer Segmentation**: Group customers by purchasing behavior for targeted marketing\n",
    "2. **Image Segmentation**: Separate different objects or regions in images\n",
    "3. **Document Organization**: Group similar documents together\n",
    "4. **Anomaly Detection**: Identify data points that don't belong to any cluster\n",
    "5. **Genomics**: Group genes with similar expression patterns\n",
    "6. **Social Network Analysis**: Detect communities in social networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering Algorithm\n",
    "\n",
    "**K-means** is one of the simplest and most widely used clustering algorithms. It partitions data into $k$ clusters by minimizing the within-cluster variance.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "#### Objective Function\n",
    "\n",
    "K-means aims to minimize the **within-cluster sum of squares (WCSS)**, also called **inertia**:\n",
    "\n",
    "$$J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2$$\n",
    "\n",
    "Where:\n",
    "- $k$ = number of clusters\n",
    "- $C_i$ = the $i$-th cluster\n",
    "- $\\mu_i$ = the centroid (mean) of cluster $C_i$\n",
    "- $\\|x - \\mu_i\\|^2$ = squared Euclidean distance between point $x$ and centroid $\\mu_i$\n",
    "\n",
    "#### Euclidean Distance\n",
    "\n",
    "The distance between two points $x = (x_1, x_2, ..., x_d)$ and $y = (y_1, y_2, ..., y_d)$ is:\n",
    "\n",
    "$$d(x, y) = \\sqrt{\\sum_{j=1}^{d} (x_j - y_j)^2}$$\n",
    "\n",
    "#### Cluster Centroid\n",
    "\n",
    "The centroid $\\mu_i$ of cluster $C_i$ containing $n_i$ points is the mean of all points in the cluster:\n",
    "\n",
    "$$\\mu_i = \\frac{1}{n_i} \\sum_{x \\in C_i} x$$\n",
    "\n",
    "### The K-Means Algorithm\n",
    "\n",
    "**Input**: Dataset $X = \\{x_1, x_2, ..., x_n\\}$, Number of clusters $k$\n",
    "\n",
    "**Algorithm**:\n",
    "\n",
    "1. **Initialization**: Randomly select $k$ data points as initial centroids $\\mu_1, \\mu_2, ..., \\mu_k$\n",
    "\n",
    "2. **Repeat until convergence**:\n",
    "   \n",
    "   a. **Assignment Step**: Assign each point $x_i$ to the nearest centroid\n",
    "   $$C_j = \\{x_i : \\|x_i - \\mu_j\\|^2 \\leq \\|x_i - \\mu_l\\|^2 \\text{ for all } l = 1,...,k\\}$$\n",
    "   \n",
    "   b. **Update Step**: Recalculate centroids as the mean of all points in each cluster\n",
    "   $$\\mu_j = \\frac{1}{|C_j|} \\sum_{x \\in C_j} x$$\n",
    "   \n",
    "3. **Convergence**: Stop when centroids no longer change\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "**Advantages**:\n",
    "- Simple and easy to implement\n",
    "- Computationally efficient: $O(nkd \\cdot i)$ where $i$ is the number of iterations\n",
    "- Works well when clusters are spherical and similar in size\n",
    "- Scales to large datasets\n",
    "\n",
    "**Limitations**:\n",
    "- Must specify $k$ (number of clusters) in advance\n",
    "- Sensitive to initial centroid positions\n",
    "- Assumes clusters are convex and isotropic\n",
    "- Sensitive to outliers\n",
    "- Only finds local optima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means clustering completed successfully!\n",
      "Number of clusters: 3\n",
      "Inertia (WCSS): 78.85\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data with clear clusters\n",
    "X, y_true = make_blobs(n_samples=300, centers=3, n_features=2, \n",
    "                       random_state=42, cluster_std=0.8)\n",
    "\n",
    "# Create and fit K-means model\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# Print results\n",
    "print(\"K-means clustering completed successfully!\")\n",
    "print(f\"Number of clusters: {kmeans.n_clusters}\")\n",
    "print(f\"Inertia (WCSS): {kmeans.inertia_:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering results visualized!\n"
     ]
    }
   ],
   "source": [
    "# Visualize clustering results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: True clusters (for comparison)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, \n",
    "           edgecolors='black', linewidth=0.5, alpha=0.7)\n",
    "plt.title('True Clusters (Hidden in Real Scenarios)', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Subplot 2: K-means results\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=50, \n",
    "           edgecolors='black', linewidth=0.5, alpha=0.7)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "           c='red', marker='X', s=300, edgecolors='black', linewidth=2,\n",
    "           label='Centroids')\n",
    "plt.title('K-Means Clustering Results (k=3)', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Clustering results visualized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Clustering Quality\n",
    "\n",
    "Since we don't have true labels in unsupervised learning, we need special metrics to evaluate clustering quality:\n",
    "\n",
    "### Silhouette Score\n",
    "\n",
    "The **silhouette score** measures how similar an object is to its own cluster compared to other clusters. It ranges from -1 to 1:\n",
    "- **1**: Perfect clustering\n",
    "- **0**: Overlapping clusters\n",
    "- **-1**: Wrong clustering\n",
    "\n",
    "For each sample $i$, the silhouette coefficient is:\n",
    "\n",
    "$$s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}$$\n",
    "\n",
    "Where:\n",
    "- $a(i)$ = average distance between $i$ and all other points in the same cluster\n",
    "- $b(i)$ = average distance between $i$ and all points in the nearest cluster\n",
    "\n",
    "### Elbow Method\n",
    "\n",
    "The **elbow method** helps determine the optimal number of clusters by plotting the inertia (WCSS) for different values of $k$. The \"elbow\" point where the rate of decrease sharply shifts is often considered the optimal $k$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different numbers of clusters...\n",
      "Elbow method plot created!\n"
     ]
    }
   ],
   "source": [
    "# Elbow Method: Test different k values\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "print(\"Testing different numbers of clusters...\")\n",
    "for k in K_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_temp.fit(X)\n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X, kmeans_temp.labels_))\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow curve\n",
    "ax1.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax1.set_ylabel('Inertia (WCSS)', fontsize=12)\n",
    "ax1.set_title('Elbow Method', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette scores\n",
    "ax2.plot(K_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax2.set_ylabel('Silhouette Score', fontsize=12)\n",
    "ax2.set_title('Silhouette Score vs. Number of Clusters', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Elbow method plot created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Example: Clustering the Iris Dataset\n",
    "\n",
    "Let's apply K-means to the famous Iris dataset. The Iris dataset contains measurements of 150 iris flowers from 3 different species. We'll use only the measurements (features) and see if K-means can discover the 3 species without being told what they are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris dataset loaded: 150 samples, 4 features\n",
      "K-means clustering performed (k=3)\n",
      "Silhouette Score: 0.55\n"
     ]
    }
   ],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris_true = iris.target\n",
    "\n",
    "print(f\"Iris dataset loaded: {X_iris.shape[0]} samples, {X_iris.shape[1]} features\")\n",
    "\n",
    "# Standardize features (important for K-means!)\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "# Apply K-means\n",
    "kmeans_iris = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "y_iris_pred = kmeans_iris.fit_predict(X_iris_scaled)\n",
    "\n",
    "# Calculate silhouette score\n",
    "sil_score = silhouette_score(X_iris_scaled, y_iris_pred)\n",
    "\n",
    "print(\"K-means clustering performed (k=3)\")\n",
    "print(f\"Silhouette Score: {sil_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris clustering results visualized!\n"
     ]
    }
   ],
   "source": [
    "# Visualize using first two features\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# True labels\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter1 = plt.scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris_true, \n",
    "                       cmap='viridis', s=50, edgecolors='black', linewidth=0.5)\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.title('True Species Labels', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(scatter1, label='Species')\n",
    "\n",
    "# K-means predictions\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter2 = plt.scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris_pred, \n",
    "                       cmap='viridis', s=50, edgecolors='black', linewidth=0.5)\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.title('K-Means Clustering Results', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(scatter2, label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Iris clustering results visualized!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Congratulations! You've completed your introduction to unsupervised learning and K-means clustering. Here are the key points to remember:\n",
    "\n",
    "1. **Unsupervised learning** works with unlabeled data to discover hidden patterns and structures\n",
    "2. **Clustering** groups similar data points together based on their features\n",
    "3. **K-means** is a simple yet powerful clustering algorithm that minimizes within-cluster variance\n",
    "4. The K-means algorithm alternates between **assigning points to clusters** and **updating centroids**\n",
    "5. **Feature scaling** is important for K-means since it uses distance-based calculations\n",
    "6. The **elbow method** and **silhouette score** help evaluate clustering quality and choose the optimal number of clusters\n",
    "7. K-means works best when clusters are **spherical** and **similar in size**\n",
    "8. Applications include customer segmentation, image processing, anomaly detection, and more\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In the coming days, we'll explore:\n",
    "- **Day 37**: Implementing K-Means for Different Data Types\n",
    "- **Day 38**: Hierarchical Clustering Techniques\n",
    "- **Day 39**: Density-Based Clustering with DBSCAN\n",
    "- **Day 40**: Gaussian Mixture Models and Expectation-Maximization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise for the Reader\n",
    "\n",
    "To solidify your understanding of K-means clustering, try these exercises:\n",
    "\n",
    "### Exercise 1: Experiment with Different K Values\n",
    "Using the synthetic data we generated, try different values of k (2, 4, 5, 10) and observe:\n",
    "- How the clusters change\n",
    "- How the inertia and silhouette scores change\n",
    "- Which value of k seems most appropriate for this dataset\n",
    "\n",
    "### Exercise 2: Impact of Initialization\n",
    "Run K-means multiple times with different `random_state` values. Do you get the same results? Why or why not?\n",
    "\n",
    "### Exercise 3: Feature Scaling\n",
    "Apply K-means to the Iris dataset WITHOUT scaling the features. Compare the results to the scaled version. What differences do you observe?\n",
    "\n",
    "### Exercise 4: Real-World Application\n",
    "Find a dataset online (e.g., from Kaggle or UCI Machine Learning Repository) and apply K-means clustering:\n",
    "1. Load and explore the data\n",
    "2. Preprocess and scale features if necessary\n",
    "3. Use the elbow method to determine optimal k\n",
    "4. Apply K-means and visualize results\n",
    "5. Interpret the clusters - what patterns did you discover?\n",
    "\n",
    "### Bonus Challenge\n",
    "Implement the K-means algorithm from scratch using only NumPy (without scikit-learn). This will deepen your understanding of how the algorithm works!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "To deepen your understanding of unsupervised learning and clustering, explore these resources:\n",
    "\n",
    "### Documentation and Tutorials\n",
    "1. **Scikit-learn Clustering Documentation**: https://scikit-learn.org/stable/modules/clustering.html\n",
    "2. **K-Means Clustering Explained**: https://stanford.edu/~cpiech/cs221/handouts/kmeans.html\n",
    "3. **Real Python K-Means Guide**: https://realpython.com/k-means-clustering-python/\n",
    "\n",
    "### Academic Papers\n",
    "4. MacQueen, J. (1967). \"Some methods for classification and analysis of multivariate observations\" - Original K-means paper\n",
    "5. Arthur, D., & Vassilvitskii, S. (2007). \"k-means++: The advantages of careful seeding\" - K-means++ initialization\n",
    "\n",
    "### Interactive Learning\n",
    "6. **K-Means Visualization**: https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\n",
    "7. **Seeing Theory - Clustering**: https://seeing-theory.brown.edu/\n",
    "\n",
    "### Videos\n",
    "8. **StatQuest: K-means clustering**: https://www.youtube.com/watch?v=4b5d3muPQmA\n",
    "9. **3Blue1Brown - Understanding Clustering**: Various videos on mathematical intuition\n",
    "\n",
    "### Books\n",
    "10. \"Pattern Recognition and Machine Learning\" by Christopher Bishop (Chapter 9)\n",
    "11. \"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman (Chapter 14)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
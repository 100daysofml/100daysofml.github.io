{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Day 45: Advanced Dimensionality Reduction - UMAP and Comparisons\n\nWelcome to Day 45, the final lesson of Week 9! In this lesson, we explore advanced dimensionality reduction techniques, focusing on UMAP (Uniform Manifold Approximation and Projection) and comparing it with traditional methods like PCA and t-SNE.\n\n## Definition\n\n**Dimensionality Reduction** is the transformation of data from a high-dimensional space into a low-dimensional space while retaining meaningful properties of the original data. This is critical in modern machine learning where datasets often have hundreds or thousands of features.\n\n**UMAP (Uniform Manifold Approximation and Projection)** is a novel manifold learning technique for dimensionality reduction based on Riemannian geometry and algebraic topology. Unlike traditional methods, UMAP excels at preserving both local and global data structure.\n\n## Importance\n\nDimensionality reduction is essential for several reasons:\n\n1. **Visualization**: Humans can only visualize 2D or 3D data effectively\n2. **Computational Efficiency**: Fewer dimensions mean faster algorithms\n3. **Curse of Dimensionality**: Many ML algorithms struggle in high dimensions\n4. **Noise Reduction**: Removing irrelevant features improves model performance\n5. **Feature Engineering**: Creating better representations for downstream tasks\n\n## Applications and Examples\n\n1. **Genomics**: Analyzing expression patterns across thousands of genes\n2. **Computer Vision**: Extracting features from images with millions of pixels\n3. **NLP**: Visualizing word embeddings and document similarities\n4. **Anomaly Detection**: Identifying outliers in high-dimensional sensor data\n5. **Recommendation Systems**: Understanding user preferences across many items"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_digits, make_swiss_roll\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE, trustworthiness\nimport warnings\nimport time\nwarnings.filterwarnings('ignore')\n\n# Set random seed\nnp.random.seed(42)\n\n# Set plotting style\ntry:\n    plt.style.use('seaborn-v0_8-darkgrid')\nexcept:\n    plt.style.use('default')\n\nsns.set_palette(\"husl\")\n\nprint(\"\u2713 Libraries imported successfully!\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Mathematical Foundations\n\n## 1. Principal Component Analysis (PCA)\n\nPCA is a linear technique that finds orthogonal directions of maximum variance.\n\n**Mathematical Formulation:**\n\nFor data matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$:\n\n1. Center data: $\\mathbf{X}_c = \\mathbf{X} - \\bar{\\mathbf{X}}$\n2. Compute covariance: $\\mathbf{C} = \\frac{1}{n-1}\\mathbf{X}_c^T\\mathbf{X}_c$\n3. Find eigenvectors $\\mathbf{v}_i$ and eigenvalues $\\lambda_i$\n4. Project: $\\mathbf{Y} = \\mathbf{X}_c \\mathbf{W}$\n\n**Variance explained:**\n\n$$\\text{Variance}_i = \\frac{\\lambda_i}{\\sum_j \\lambda_j}$$\n\n**Advantages**: Fast, deterministic, interpretable, preserves global structure\n\n**Limitations**: Only linear, may miss complex patterns\n\n## 2. t-SNE (t-Distributed Stochastic Neighbor Embedding)\n\nt-SNE minimizes KL divergence between high and low-dimensional probability distributions.\n\n**High-dimensional similarity:**\n\n$$p_{ij} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma^2)}{\\sum_{k \\neq l} \\exp(-||x_k - x_l||^2 / 2\\sigma^2)}$$\n\n**Low-dimensional similarity (t-distribution):**\n\n$$q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l}(1 + ||y_k - y_l||^2)^{-1}}$$\n\n**Objective:**\n\n$$C = KL(P||Q) = \\sum_i \\sum_j p_{ij} \\log\\frac{p_{ij}}{q_{ij}}$$\n\n**Advantages**: Excellent visualization, preserves local structure\n\n**Limitations**: Slow, non-deterministic, poor global structure preservation\n\n## 3. UMAP (Uniform Manifold Approximation and Projection)\n\nUMAP uses topological data analysis and Riemannian geometry.\n\n**Fuzzy set membership (high-dimensional):**\n\n$$w_{ij} = \\exp\\left(-\\frac{d(x_i, x_j) - \\rho_i}{\\sigma_i}\\right)$$\n\nwhere $\\rho_i$ is distance to nearest neighbor, $\\sigma_i$ normalizes\n\n**Low-dimensional embedding:**\n\n$$v_{ij} = \\frac{1}{1 + a \\cdot d(y_i, y_j)^{2b}}$$\n\n**Optimization via cross-entropy** (similar concept to t-SNE but different distribution)\n\n**Advantages**: Fast, scalable, preserves both local and global structure\n\n**Limitations**: Newer, requires parameter tuning"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load digits dataset (8x8=64 dimensional)\ndigits = load_digits()\nX_digits = digits.data\ny_digits = digits.target\n\nprint(\"=\"*60)\nprint(\"DIGITS DATASET\")\nprint(\"=\"*60)\nprint(f\"Shape: {X_digits.shape}\")\nprint(f\"Features: {X_digits.shape[1]} (8x8 pixel images)\")\nprint(f\"Samples: {X_digits.shape[0]}\")\nprint(f\"Classes: {len(np.unique(y_digits))} (digits 0-9)\")\nprint(\"=\"*60)\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_digits)\n\nprint(f\"\\nStandardized: mean={X_scaled.mean():.2e}, std={X_scaled.std():.2f}\")\n\n# Visualize samples\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X_digits[i].reshape(8, 8), cmap='gray')\n    ax.set_title(f\"Digit: {y_digits[i]}\")\n    ax.axis('off')\nplt.suptitle('Sample Handwritten Digits (64 features each)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Apply PCA\nprint(\"\\nApplying PCA...\")\npca = PCA(n_components=2, random_state=42)\nX_pca = pca.fit_transform(X_scaled)\n\nprint(f\"Variance explained: {pca.explained_variance_ratio_}\")\nprint(f\"Total: {pca.explained_variance_ratio_.sum():.2%}\")\n\n# Visualize\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_digits, cmap='tab10',\n                     alpha=0.6, edgecolors='k', linewidth=0.5, s=50)\nplt.colorbar(scatter, label='Digit', ticks=range(10))\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\nplt.title('PCA: Digits in 2D', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nNote: PCA captures {pca.explained_variance_ratio_.sum():.1%} of variance\")\nprint(\"Some digit classes overlap - PCA is linear!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Apply t-SNE\nprint(\"\\nApplying t-SNE (may take 30-60 seconds)...\")\nstart = time.time()\n\ntsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000, verbose=0)\nX_tsne = tsne.fit_transform(X_scaled)\n\nprint(f\"Completed in {time.time()-start:.2f}s\")\nprint(f\"Final KL divergence: {tsne.kl_divergence_:.4f}\")\n\n# Visualize\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_digits, cmap='tab10',\n                     alpha=0.6, edgecolors='k', linewidth=0.5, s=50)\nplt.colorbar(scatter, label='Digit', ticks=range(10))\nplt.xlabel('t-SNE 1')\nplt.ylabel('t-SNE 2')\nplt.title('t-SNE: Digits in 2D', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nExcellent cluster separation!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Install and apply UMAP\nimport subprocess\nimport sys\n\ntry:\n    import umap\n    print(\"\u2713 UMAP already installed\")\nexcept ImportError:\n    print(\"Installing UMAP...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"umap-learn\"])\n    import umap\n    print(\"\u2713 UMAP installed!\")\n\n# Apply UMAP\nprint(\"\\nApplying UMAP...\")\nstart = time.time()\n\nreducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\nX_umap = reducer.fit_transform(X_scaled)\n\nprint(f\"Completed in {time.time()-start:.2f}s\")\n\n# Visualize\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y_digits, cmap='tab10',\n                     alpha=0.6, edgecolors='k', linewidth=0.5, s=50)\nplt.colorbar(scatter, label='Digit', ticks=range(10))\nplt.xlabel('UMAP 1')\nplt.ylabel('UMAP 2')\nplt.title('UMAP: Digits in 2D', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nClear clusters + faster than t-SNE!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Side-by-side comparison\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor (emb, name, ax) in [(X_pca, 'PCA', axes[0]),\n                         (X_tsne, 't-SNE', axes[1]),\n                         (X_umap, 'UMAP', axes[2])]:\n    scatter = ax.scatter(emb[:, 0], emb[:, 1], c=y_digits, cmap='tab10',\n                        alpha=0.6, edgecolors='k', linewidth=0.5, s=40)\n    ax.set_title(name, fontsize=14, fontweight='bold')\n    ax.set_xlabel(f'{name} 1')\n    ax.set_ylabel(f'{name} 2')\n    ax.grid(True, alpha=0.3)\n\nplt.colorbar(scatter, ax=axes, label='Digit', ticks=range(10), pad=0.01)\nplt.suptitle('Comparison: PCA vs t-SNE vs UMAP', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nObservations:\")\nprint(\"  PCA: Fast, linear, some overlap\")\nprint(\"  t-SNE: Slow, excellent clusters, loses global structure\")\nprint(\"  UMAP: Fast, good clusters + global structure\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Understanding UMAP Parameters\n\nUMAP has two key hyperparameters:\n\n## 1. `n_neighbors` (default=15)\n\nControls local vs global structure balance:\n\n- **Small (5-10)**: Very local focus, tight clusters\n- **Large (30-50)**: More global structure preservation\n\n## 2. `min_dist` (default=0.1)\n\nControls point packing in low-dimensional space:\n\n- **Small (0.0-0.1)**: Tight clusters\n- **Large (0.5-0.99)**: Spread out points\n\n**Recommended Settings:**\n- Clustering: `min_dist=0.0`, `n_neighbors=15`\n- Visualization: `min_dist=0.1`, `n_neighbors=15-30`\n- Topology: `min_dist=0.1-0.3`, `n_neighbors=30-50`"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Parameter exploration\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\nfig.suptitle('UMAP Parameter Effects', fontsize=16, fontweight='bold')\n\nparams = [\n    {'n_neighbors': 5, 'min_dist': 0.0, 'title': 'Local + Tight\\n(n=5, d=0.0)'},\n    {'n_neighbors': 15, 'min_dist': 0.1, 'title': 'Balanced (DEFAULT)\\n(n=15, d=0.1)'},\n    {'n_neighbors': 50, 'min_dist': 0.1, 'title': 'Global + Tight\\n(n=50, d=0.1)'},\n    {'n_neighbors': 15, 'min_dist': 0.9, 'title': 'Balanced + Loose\\n(n=15, d=0.9)'},\n]\n\nfor ax, param in zip(axes.flat, params):\n    print(f\"Computing: {param['title'].split(chr(10))[0]}...\")\n    reducer = umap.UMAP(n_components=2, n_neighbors=param['n_neighbors'],\n                       min_dist=param['min_dist'], random_state=42)\n    emb = reducer.fit_transform(X_scaled)\n    scatter = ax.scatter(emb[:, 0], emb[:, 1], c=y_digits, cmap='tab10',\n                        alpha=0.6, s=20, edgecolors='w', linewidth=0.3)\n    ax.set_title(param['title'], fontsize=12, fontweight='bold')\n    ax.grid(True, alpha=0.3)\n\nplt.colorbar(scatter, ax=axes, label='Digit', pad=0.01)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nLower n_neighbors \u2192 more local, tighter\")\nprint(\"Higher n_neighbors \u2192 more global, looser\")\nprint(\"Lower min_dist \u2192 tighter packing\")\nprint(\"Higher min_dist \u2192 more spread out\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# The Swiss Roll Test\n\nThe Swiss Roll is a classic test for manifold learning - a 2D surface rolled in 3D.\n\n**Why it matters**: Success means the algorithm can \"unroll\" non-linear manifolds while preserving structure.\n\n- **PCA fails**: It's linear\n- **t-SNE fragments**: Focuses too locally  \n- **UMAP succeeds**: True manifold learning"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Generate Swiss Roll\nX_swiss, color = make_swiss_roll(n_samples=1500, noise=0.1, random_state=42)\n\nprint(f\"Swiss Roll shape: {X_swiss.shape} (3D)\")\nprint(f\"Color range: [{color.min():.1f}, {color.max():.1f}]\")\n\n# Visualize 3D\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(14, 6))\n\nax1 = fig.add_subplot(121, projection='3d')\nscatter1 = ax1.scatter(X_swiss[:, 0], X_swiss[:, 1], X_swiss[:, 2],\n                      c=color, cmap='viridis', s=20, alpha=0.6)\nax1.set_title('Swiss Roll in 3D', fontweight='bold')\nax1.view_init(elev=12, azim=-66)\nfig.colorbar(scatter1, ax=ax1, shrink=0.8, label='Position')\n\nax2 = fig.add_subplot(122, projection='3d')\nscatter2 = ax2.scatter(X_swiss[:, 0], X_swiss[:, 1], X_swiss[:, 2],\n                      c=color, cmap='viridis', s=20, alpha=0.6)\nax2.set_title('Swiss Roll (Another Angle)', fontweight='bold')\nax2.view_init(elev=20, azim=45)\nfig.colorbar(scatter2, ax=ax2, shrink=0.8, label='Position')\n\nplt.suptitle('A 2D Manifold Embedded in 3D Space', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Apply all methods to Swiss Roll\nX_swiss_scaled = StandardScaler().fit_transform(X_swiss)\n\nprint(\"Unrolling the Swiss Roll...\")\nX_sw_pca = PCA(n_components=2, random_state=42).fit_transform(X_swiss_scaled)\nX_sw_tsne = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(X_swiss_scaled)\nX_sw_umap = umap.UMAP(n_components=2, random_state=42, n_neighbors=30, min_dist=0.0).fit_transform(X_swiss_scaled)\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\naxes[0].scatter(X_sw_pca[:, 0], X_sw_pca[:, 1], c=color, cmap='viridis', alpha=0.6, s=20)\naxes[0].set_title('PCA: Cannot Unroll\\n(Linear fails)', fontsize=13, fontweight='bold')\naxes[0].grid(True, alpha=0.3)\n\naxes[1].scatter(X_sw_tsne[:, 0], X_sw_tsne[:, 1], c=color, cmap='viridis', alpha=0.6, s=20)\naxes[1].set_title('t-SNE: Fragments\\n(Too local)', fontsize=13, fontweight='bold')\naxes[1].grid(True, alpha=0.3)\n\nscatter = axes[2].scatter(X_sw_umap[:, 0], X_sw_umap[:, 1], c=color, cmap='viridis', alpha=0.6, s=20)\naxes[2].set_title('UMAP: Success!\\n(Preserves topology)', fontsize=13, fontweight='bold')\naxes[2].grid(True, alpha=0.3)\n\nplt.colorbar(scatter, ax=axes, label='Position Along Roll', pad=0.01)\nplt.suptitle('Which Method Successfully Unrolls the Swiss Roll?', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\u2717 PCA: Failed - cannot handle non-linearity\")\nprint(\"~ t-SNE: Partial - breaks continuity\")\nprint(\"\u2713 UMAP: Success - unrolls with preserved gradient!\")\nprint(\"=\"*60)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Quantitative evaluation\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRUSTWORTHINESS METRICS\")\nprint(\"=\"*60)\nprint(\"Measures if k-nearest neighbors in high-D are also close in low-D\")\nprint(\"Higher is better (0 to 1)\")\nprint(\"=\"*60)\n\nresults = []\nfor k in [5, 10, 20]:\n    t_pca = trustworthiness(X_scaled, X_pca, n_neighbors=k)\n    t_tsne = trustworthiness(X_scaled, X_tsne, n_neighbors=k)\n    t_umap = trustworthiness(X_scaled, X_umap, n_neighbors=k)\n    results.append({'k': k, 'PCA': f'{t_pca:.4f}', 't-SNE': f'{t_tsne:.4f}', 'UMAP': f'{t_umap:.4f}'})\n\ndf = pd.DataFrame(results)\nprint(\"\\nDigits Dataset:\")\nprint(df.to_string(index=False))\n\n# Timing\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPUTATIONAL EFFICIENCY (1000 samples)\")\nprint(\"=\"*60)\n\nX_sample = X_scaled[:1000]\ntimings = {}\n\nstart = time.time()\n_ = PCA(n_components=2, random_state=42).fit_transform(X_sample)\ntimings['PCA'] = time.time() - start\n\nstart = time.time()\n_ = TSNE(n_components=2, random_state=42, n_iter=300, verbose=0).fit_transform(X_sample)\ntimings['t-SNE'] = time.time() - start\n\nstart = time.time()\n_ = umap.UMAP(n_components=2, random_state=42).fit_transform(X_sample)\ntimings['UMAP'] = time.time() - start\n\nfor method, t in timings.items():\n    print(f\"{method:8s}: {t:6.3f} seconds\")\n\nprint(f\"\\nSpeedup vs t-SNE:\")\nprint(f\"  PCA:  {timings['t-SNE']/timings['PCA']:.1f}x faster\")\nprint(f\"  UMAP: {timings['t-SNE']/timings['UMAP']:.1f}x faster\")\nprint(\"=\"*60)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Decision Guide\n\n## Use PCA when:\n\u2705 Speed is critical  \n\u2705 Linear relationships suffice  \n\u2705 Need interpretable components  \n\u2705 Want global structure  \n\u2705 Need to reverse transformation  \n\u2705 Preprocessing for ML  \n\n**Examples**: Image compression, noise reduction, initial EDA\n\n## Use t-SNE when:\n\u2705 Visualization is primary goal  \n\u2705 Local structure critical  \n\u2705 < 10,000 samples  \n\u2705 Time not constrained  \n\u2705 Publication-quality figures  \n\n**Examples**: Cluster visualization, exploring embeddings, understanding separability\n\n## Use UMAP when:\n\u2705 Need both local AND global structure  \n\u2705 Large datasets (> 10,000)  \n\u2705 Embedding for downstream tasks  \n\u2705 Need to transform new data  \n\u2705 Efficiency matters  \n\u2705 Complex manifolds  \n\n**Examples**: Single-cell genomics, document clustering, anomaly detection, large-scale viz\n\n## Quick Reference\n\n| Criterion | PCA | t-SNE | UMAP |\n|-----------|-----|-------|------|\n| **Speed** | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50 | \u2b50\u2b50\u2b50\u2b50 |\n| **Local Structure** | \u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50 |\n| **Global Structure** | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50 | \u2b50\u2b50\u2b50\u2b50 |\n| **Scalability** | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50 | \u2b50\u2b50\u2b50\u2b50 |\n| **Deterministic** | \u2705 | \u274c | \u2705 |\n| **New Data** | \u2705 | \u274c | \u2705 |\n| **Best For** | Preprocessing | Visualization | General Purpose |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise For The Reader\n\nApply these techniques to the Wine dataset!\n\n**Your Tasks:**\n1. Load the Wine dataset from sklearn\n2. Standardize features\n3. Apply PCA, t-SNE, and UMAP\n4. Create side-by-side visualizations\n5. Compute trustworthiness metrics\n6. Answer: Which method best separates wine classes?"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Exercise: Wine Dataset\nfrom sklearn.datasets import load_wine\n\nwine = load_wine()\nX_wine = wine.data\ny_wine = wine.target\n\nprint(f\"Wine dataset: {X_wine.shape}\")\nprint(f\"Classes: {wine.target_names}\")\n\n# Standardize\nX_wine_scaled = StandardScaler().fit_transform(X_wine)\n\n# Apply methods\nX_wine_pca = PCA(n_components=2, random_state=42).fit_transform(X_wine_scaled)\nX_wine_tsne = TSNE(n_components=2, random_state=42, verbose=0).fit_transform(X_wine_scaled)\nX_wine_umap = umap.UMAP(n_components=2, random_state=42).fit_transform(X_wine_scaled)\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\ncolors = ['red', 'green', 'blue']\n\nfor i, (emb, name, ax) in enumerate([(X_wine_pca, 'PCA', axes[0]),\n                                       (X_wine_tsne, 't-SNE', axes[1]),\n                                       (X_wine_umap, 'UMAP', axes[2])]):\n    for j, wine_name in enumerate(wine.target_names):\n        mask = y_wine == j\n        ax.scatter(emb[mask, 0], emb[mask, 1], c=colors[j],\n                  label=wine_name, alpha=0.7, edgecolors='k', s=60)\n    ax.set_title(name, fontsize=14, fontweight='bold')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('Wine Dataset Comparison', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Metrics\nprint(\"\\nTrustworthiness (k=10):\")\nfor emb, name in [(X_wine_pca, 'PCA'), (X_wine_tsne, 't-SNE'), (X_wine_umap, 'UMAP')]:\n    t = trustworthiness(X_wine_scaled, emb, n_neighbors=10)\n    print(f\"  {name:8s}: {t:.4f}\")\n\nprint(\"\\n\u2713 Exercise complete!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Key Takeaways\n\nCongratulations on completing Day 45 and Week 9!\n\n## What You've Learned\n\n1. **Three Major Techniques**\n   - PCA: Linear, fast, interpretable\n   - t-SNE: Excellent visualization, local focus\n   - UMAP: Balanced, modern, scalable\n\n2. **Mathematical Foundations**\n   - PCA: Eigendecomposition, variance maximization\n   - t-SNE: KL divergence, probability distributions\n   - UMAP: Manifold learning, topology preservation\n\n3. **Practical Skills**\n   - Apply and compare methods\n   - Choose appropriate technique\n   - Tune UMAP parameters\n   - Evaluate with trustworthiness\n   - Visualize effectively\n\n4. **Key Principles**\n   - No single best method\n   - Use quantitative metrics\n   - Consider downstream tasks\n   - UMAP often best balance\n   - t-SNE still king for viz\n   - PCA essential for preprocessing\n\n## What's Next?\n\n- Apply to your datasets\n- Explore supervised UMAP\n- Learn about autoencoders\n- Investigate other manifold methods (Isomap, LLE)\n- Use in ML pipelines\n\n**Your dimensionality reduction journey is complete!** \ud83c\udf89"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Further Resources\n\n### Papers\n1. **UMAP** - McInnes et al. (2018) - https://arxiv.org/abs/1802.03426\n2. **t-SNE** - van der Maaten & Hinton (2008) - JMLR\n3. **PCA** - Jolliffe (2002) - Springer\n\n### Documentation\n4. **UMAP Docs** - https://umap-learn.readthedocs.io/\n5. **Scikit-learn Manifold** - https://scikit-learn.org/stable/modules/manifold.html\n6. **Distill.pub t-SNE** - https://distill.pub/2016/misread-tsne/\n\n### Interactive\n7. **Understanding UMAP** - https://pair-code.github.io/understanding-umap/\n8. **PCA Explained Visually** - http://setosa.io/ev/principal-component-analysis/\n\n### Books\n9. **Elements of Statistical Learning** - Hastie, Tibshirani, Friedman\n10. **Hands-On Machine Learning** - Aur\u00e9lien G\u00e9ron (3rd Ed)\n\n### Practice\n11. **Kaggle Datasets** - https://www.kaggle.com/datasets\n12. **UCI ML Repository** - https://archive.ics.uci.edu/ml/\n\n**Happy Learning! \ud83d\ude80**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 79: Fine-tuning Pre-trained Transformers\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Fine-tuning pre-trained transformer models has revolutionized natural language processing and many other machine learning domains. Instead of training large neural networks from scratch, which requires massive computational resources and datasets, we can leverage models that have already learned rich representations from enormous corpora of text. This approach, known as transfer learning, allows us to adapt powerful pre-trained models to specific tasks with relatively small datasets and computational budgets.\n",
    "\n",
    "In this lesson, we'll explore the theory and practice of fine-tuning transformer models. We'll understand why transfer learning works, examine the mathematical foundations of the fine-tuning process, and implement a complete example using the Hugging Face Transformers library.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "- Understand the concept of transfer learning and why it's effective for transformers\n",
    "- Explain the mathematical principles behind fine-tuning pre-trained models\n",
    "- Implement fine-tuning for a text classification task using pre-trained transformers\n",
    "- Evaluate and visualize the performance of fine-tuned models\n",
    "- Apply best practices for fine-tuning to avoid common pitfalls like catastrophic forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Transfer Learning Works\n",
    "\n",
    "Transfer learning leverages the idea that knowledge gained from one task can be applied to related tasks. In the context of transformers and natural language processing:\n",
    "\n",
    "1. **Pre-training Phase**: A model is trained on a large, general corpus (e.g., Wikipedia, BookCorpus) using self-supervised objectives like masked language modeling or next token prediction. During this phase, the model learns:\n",
    "   - Syntactic patterns (grammar, sentence structure)\n",
    "   - Semantic relationships (word meanings, contextual usage)\n",
    "   - World knowledge encoded in the training data\n",
    "\n",
    "2. **Fine-tuning Phase**: The pre-trained model is adapted to a specific downstream task (e.g., sentiment analysis, named entity recognition) using a smaller, task-specific dataset. The model's parameters are updated, but they start from the rich representations learned during pre-training rather than random initialization.\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "Fine-tuned transformers are used extensively across industries:\n",
    "\n",
    "- **Healthcare**: Classifying medical records, extracting clinical entities\n",
    "- **Finance**: Sentiment analysis of financial news, fraud detection in transactions\n",
    "- **Customer Service**: Intent classification, automated response generation\n",
    "- **E-commerce**: Product categorization, review analysis\n",
    "- **Legal**: Contract analysis, case law research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundations of Fine-tuning\n",
    "\n",
    "### Transfer Learning Framework\n",
    "\n",
    "Let's formalize the fine-tuning process mathematically. Consider a pre-trained model with parameters $\\theta_{\\text{pre}}$ that has been trained on a source task $\\mathcal{T}_S$ with dataset $\\mathcal{D}_S$.\n",
    "\n",
    "The pre-training objective can be expressed as:\n",
    "\n",
    "$$\\theta_{\\text{pre}} = \\arg\\min_{\\theta} \\mathcal{L}_S(\\theta; \\mathcal{D}_S)$$\n",
    "\n",
    "where $\\mathcal{L}_S$ is the loss function for the source task (e.g., masked language modeling loss).\n",
    "\n",
    "### Fine-tuning Objective\n",
    "\n",
    "For a target task $\\mathcal{T}_T$ with dataset $\\mathcal{D}_T$, we initialize the model with $\\theta_{\\text{pre}}$ and optimize:\n",
    "\n",
    "$$\\theta_{\\text{fine}} = \\arg\\min_{\\theta} \\mathcal{L}_T(\\theta; \\mathcal{D}_T)$$\n",
    "\n",
    "where $\\mathcal{L}_T$ is the task-specific loss (e.g., cross-entropy for classification).\n",
    "\n",
    "The key insight is that starting from $\\theta_{\\text{pre}}$ rather than random initialization provides a much better starting point for optimization, especially when $|\\mathcal{D}_T| \\ll |\\mathcal{D}_S|$.\n",
    "\n",
    "### Layer-wise Learning Rates\n",
    "\n",
    "A common practice in fine-tuning is to use different learning rates for different layers. Lower layers (closer to input) typically learn more general features, while higher layers learn more task-specific features. We can use discriminative fine-tuning:\n",
    "\n",
    "$$\\theta^{(l)} \\leftarrow \\theta^{(l)} - \\eta^{(l)} \\nabla_{\\theta^{(l)}} \\mathcal{L}_T$$\n",
    "\n",
    "where $\\eta^{(l)}$ is the learning rate for layer $l$, often with $\\eta^{(l)} < \\eta^{(l+1)}$ (lower layers use smaller learning rates).\n",
    "\n",
    "### Catastrophic Forgetting\n",
    "\n",
    "One challenge in fine-tuning is catastrophic forgetting, where the model \"forgets\" knowledge from pre-training. To mitigate this, we can add a regularization term that keeps parameters close to their pre-trained values:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_T(\\theta; \\mathcal{D}_T) + \\frac{\\lambda}{2} \\|\\theta - \\theta_{\\text{pre}}\\|^2$$\n",
    "\n",
    "where $\\lambda$ controls the strength of regularization. This is similar to L2 regularization but centered on the pre-trained weights rather than zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation: Fine-tuning for Sentiment Analysis\n",
    "\n",
    "Now let's implement fine-tuning using a practical example. We'll fine-tune a BERT model for sentiment classification on movie reviews. We'll use the Hugging Face Transformers library, which provides easy access to pre-trained models and fine-tuning utilities.\n",
    "\n",
    "### Installing Required Libraries\n",
    "\n",
    "First, let's import the necessary libraries. In a real environment, you might need to install transformers and datasets:\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets torch scikit-learn matplotlib seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Libraries imported successfully!",
      "NumPy version: 2.3.4",
      "Pandas version: 2.3.3",
      ""
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Sample Dataset\n",
    "\n",
    "For demonstration purposes, we'll create a synthetic sentiment analysis dataset. In practice, you would use real datasets like IMDB reviews, SST-2, or domain-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset created with 20 samples",
      "",
      "Class distribution:",
      "sentiment",
      "positive    10",
      "negative    10",
      "Name: count, dtype: int64",
      "",
      "First few samples:",
      ""
     ]
    }
   ],
   "source": [
    "# Create a sample sentiment dataset\n",
    "positive_samples = [\n",
    "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "    \"An outstanding performance by the lead actor. Highly recommend!\",\n",
    "    \"Beautiful cinematography and a compelling story. A must-watch!\",\n",
    "    \"I was thoroughly entertained from start to finish. Brilliant!\",\n",
    "    \"One of the best films I've seen this year. Absolutely amazing!\",\n",
    "    \"The plot was engaging and the acting was superb.\",\n",
    "    \"A masterpiece of cinema. I can't wait to watch it again!\",\n",
    "    \"Exceptional direction and wonderful performances throughout.\",\n",
    "    \"This film exceeded all my expectations. Simply brilliant!\",\n",
    "    \"A perfect blend of drama and emotion. Loved it!\"\n",
    "]\n",
    "\n",
    "negative_samples = [\n",
    "    \"This movie was terrible. I couldn't even finish watching it.\",\n",
    "    \"Poor acting and a confusing plot. Complete waste of time.\",\n",
    "    \"I was disappointed by this film. Expected much better.\",\n",
    "    \"The worst movie I've seen in years. Avoid at all costs!\",\n",
    "    \"Boring and predictable. I fell asleep halfway through.\",\n",
    "    \"Terrible screenplay and uninspired performances.\",\n",
    "    \"I regret spending money on this disappointing film.\",\n",
    "    \"Poorly executed with no redeeming qualities.\",\n",
    "    \"A complete disaster. Nothing about this movie worked.\",\n",
    "    \"Dull and forgettable. Don't waste your time.\"\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "texts = positive_samples + negative_samples\n",
    "labels = [1] * len(positive_samples) + [0] * len(negative_samples)  # 1 = positive, 0 = negative\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'label': labels,\n",
    "    'sentiment': ['positive' if l == 1 else 'negative' for l in labels]\n",
    "})\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset created with {len(df)} samples\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(f\"\\nFirst few samples:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Fine-tuning Process\n",
    "\n",
    "The fine-tuning process for transformers typically involves these steps:\n",
    "\n",
    "1. **Tokenization**: Convert text into tokens that the model can process\n",
    "2. **Model Architecture**: Add a task-specific head (e.g., classification layer) on top of the pre-trained transformer\n",
    "3. **Training**: Update model parameters using the task-specific dataset\n",
    "4. **Evaluation**: Assess performance on a held-out test set\n",
    "\n",
    "Let's implement a simplified version that demonstrates the key concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample tokenization:",
      "Text: This movie was absolutely fantastic! I loved every minute of it.",
      "Tokens (first 10): [567, 699, 262, 106, 767, 671, 977, 157, 1, 217]",
      "",
      "Token sequence length: 20",
      ""
     ]
    }
   ],
   "source": [
    "# Simulate tokenization process\n",
    "# In practice, you would use: tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def simple_tokenize(text, max_length=20):\n",
    "    \"\"\"Simplified tokenization for demonstration\"\"\"\n",
    "    words = text.lower().split()\n",
    "    # Simulate token IDs (in practice, these come from the tokenizer's vocabulary)\n",
    "    token_ids = [hash(word) % 1000 for word in words[:max_length]]\n",
    "    # Pad to max_length\n",
    "    while len(token_ids) < max_length:\n",
    "        token_ids.append(0)  # 0 is typically the padding token\n",
    "    return token_ids[:max_length]\n",
    "\n",
    "# Apply tokenization\n",
    "df['tokens'] = df['text'].apply(simple_tokenize)\n",
    "\n",
    "print(\"Sample tokenization:\")\n",
    "print(f\"Text: {df.iloc[0]['text']}\")\n",
    "print(f\"Tokens (first 10): {df.iloc[0]['tokens'][:10]}\")\n",
    "print(f\"\\nToken sequence length: {len(df.iloc[0]['tokens'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating the Fine-tuning Process\n",
    "\n",
    "While a full transformer implementation would require the actual Transformers library and significant computational resources, we can demonstrate the learning process with a simplified model that captures the key concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training samples: 14",
      "Test samples: 6",
      "",
      "Feature matrix shape: (14, 100)",
      "Number of features: 100",
      "",
      "Training Accuracy: 1.0000",
      "Test Accuracy: 0.8333",
      ""
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'].values, df['label'].values, test_size=0.3, random_state=42, stratify=df['label']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Test samples: {len(test_texts)}\")\n",
    "\n",
    "# Create TF-IDF features (simulating transformer embeddings)\n",
    "vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X_train.shape}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "\n",
    "# Train classifier (simulating the fine-tuning process)\n",
    "# In reality, this would be updating the transformer's parameters\n",
    "classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
    "classifier.fit(X_train, train_labels)\n",
    "\n",
    "# Make predictions\n",
    "train_preds = classifier.predict(X_train)\n",
    "test_preds = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating Learning Curves\n",
    "\n",
    "One important aspect of fine-tuning is monitoring the training process to ensure the model is learning effectively without overfitting. Let's visualize how model performance typically evolves during fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "",
      "Key Observations:",
      "- Training loss decreases consistently as the model learns",
      "- Validation loss decreases but may plateau or increase slightly (overfitting signal)",
      "- The gap between training and validation metrics indicates generalization",
      "- Early stopping should be used when validation loss stops improving",
      ""
     ]
    }
   ],
   "source": [
    "# Simulate training curves (in practice, these would come from actual training)\n",
    "epochs = np.arange(1, 11)\n",
    "\n",
    "# Simulate realistic training and validation loss curves\n",
    "# Training loss decreases steadily\n",
    "train_loss = 0.693 * np.exp(-0.3 * epochs) + 0.1 + np.random.normal(0, 0.02, len(epochs))\n",
    "# Validation loss decreases but plateaus/increases slightly (showing some overfitting)\n",
    "val_loss = 0.693 * np.exp(-0.25 * epochs) + 0.15 + np.random.normal(0, 0.03, len(epochs))\n",
    "\n",
    "# Accuracy curves (inverse of loss)\n",
    "train_acc = 0.5 + 0.48 * (1 - np.exp(-0.35 * epochs)) + np.random.normal(0, 0.01, len(epochs))\n",
    "val_acc = 0.5 + 0.45 * (1 - np.exp(-0.3 * epochs)) + np.random.normal(0, 0.015, len(epochs))\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot loss curves\n",
    "ax1.plot(epochs, train_loss, 'b-o', label='Training Loss', linewidth=2, markersize=8)\n",
    "ax1.plot(epochs, val_loss, 'r-s', label='Validation Loss', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Fine-tuning Loss Curves', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 0.8)\n",
    "\n",
    "# Plot accuracy curves\n",
    "ax2.plot(epochs, train_acc, 'b-o', label='Training Accuracy', linewidth=2, markersize=8)\n",
    "ax2.plot(epochs, val_acc, 'r-s', label='Validation Accuracy', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Fine-tuning Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0.4, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Training loss decreases consistently as the model learns\")\n",
    "print(\"- Validation loss decreases but may plateau or increase slightly (overfitting signal)\")\n",
    "print(\"- The gap between training and validation metrics indicates generalization\")\n",
    "print(\"- Early stopping should be used when validation loss stops improving\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Performance Analysis\n",
    "\n",
    "After fine-tuning, it's crucial to thoroughly evaluate the model's performance. Let's examine various evaluation metrics and visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification Report:",
      "============================================================",
      "              precision    recall  f1-score   support",
      "",
      "    Negative     0.7500    1.0000    0.8571         3",
      "    Positive     1.0000    0.6667    0.8000         3",
      "",
      "    accuracy                         0.8333         6",
      "   macro avg     0.8750    0.8333    0.8286         6",
      "weighted avg     0.8750    0.8333    0.8286         6",
      "",
      "",
      "Sample Predictions with Confidence:",
      "============================================================",
      "",
      "Text: I regret spending money on this disappointing film....",
      "True: Negative | Predicted: Negative | Confidence: 0.5337",
      "",
      "Text: A masterpiece of cinema. I can't wait to watch it again!...",
      "True: Positive | Predicted: Positive | Confidence: 0.5041",
      "",
      "Text: A complete disaster. Nothing about this movie worked....",
      "True: Negative | Predicted: Negative | Confidence: 0.5448",
      ""
     ]
    }
   ],
   "source": [
    "# Generate predictions with probabilities\n",
    "test_probs = classifier.predict_proba(X_test)\n",
    "\n",
    "# Create detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\" * 60)\n",
    "report = classification_report(test_labels, test_preds, \n",
    "                               target_names=['Negative', 'Positive'],\n",
    "                               digits=4)\n",
    "print(report)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Fine-tuned Sentiment Classifier', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show prediction probabilities for sample texts\n",
    "print(\"\\nSample Predictions with Confidence:\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(min(3, len(test_texts))):\n",
    "    true_label = 'Positive' if test_labels[i] == 1 else 'Negative'\n",
    "    pred_label = 'Positive' if test_preds[i] == 1 else 'Negative'\n",
    "    confidence = test_probs[i].max()\n",
    "    print(f\"\\nText: {test_texts[i][:70]}...\")\n",
    "    print(f\"True: {true_label} | Predicted: {pred_label} | Confidence: {confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Feature Importance\n",
    "\n",
    "Understanding which features (words) the model finds most important can provide insights into what it has learned during fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Feature importance analysis shows which words/phrases the model",
      "associates most strongly with each sentiment class.",
      ""
     ]
    }
   ],
   "source": [
    "# Get feature names and importance\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = classifier.coef_[0]\n",
    "\n",
    "# Get top positive and negative features\n",
    "top_n = 10\n",
    "top_positive_idx = np.argsort(coefficients)[-top_n:][::-1]\n",
    "top_negative_idx = np.argsort(coefficients)[:top_n]\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Positive features\n",
    "pos_features = [feature_names[i] for i in top_positive_idx]\n",
    "pos_scores = [coefficients[i] for i in top_positive_idx]\n",
    "\n",
    "ax1.barh(range(len(pos_features)), pos_scores, color='green', alpha=0.7)\n",
    "ax1.set_yticks(range(len(pos_features)))\n",
    "ax1.set_yticklabels(pos_features)\n",
    "ax1.set_xlabel('Coefficient Value', fontsize=12)\n",
    "ax1.set_title('Top Features for Positive Sentiment', fontsize=13, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Negative features\n",
    "neg_features = [feature_names[i] for i in top_negative_idx]\n",
    "neg_scores = [coefficients[i] for i in top_negative_idx]\n",
    "\n",
    "ax2.barh(range(len(neg_features)), neg_scores, color='red', alpha=0.7)\n",
    "ax2.set_yticks(range(len(neg_features)))\n",
    "ax2.set_yticklabels(neg_features)\n",
    "ax2.set_xlabel('Coefficient Value', fontsize=12)\n",
    "ax2.set_title('Top Features for Negative Sentiment', fontsize=13, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature importance analysis shows which words/phrases the model\")\n",
    "print(\"associates most strongly with each sentiment class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Fine-tuning Transformers\n",
    "\n",
    "Based on research and practical experience, here are key best practices for fine-tuning:\n",
    "\n",
    "### 1. Learning Rate Selection\n",
    "\n",
    "- Use a smaller learning rate than pre-training (typically 1e-5 to 5e-5)\n",
    "- Consider using different learning rates for different layers (discriminative fine-tuning)\n",
    "- The classification head can use a higher learning rate than the transformer layers\n",
    "\n",
    "### 2. Training Duration\n",
    "\n",
    "- Fine-tuning typically requires only 2-4 epochs for good performance\n",
    "- Monitor validation metrics to avoid overfitting\n",
    "- Use early stopping based on validation loss\n",
    "\n",
    "### 3. Batch Size and Gradient Accumulation\n",
    "\n",
    "- Use the largest batch size your GPU memory allows\n",
    "- If memory is limited, use gradient accumulation to simulate larger batches\n",
    "- Typical batch sizes: 16-32 for base models, 8-16 for large models\n",
    "\n",
    "### 4. Regularization\n",
    "\n",
    "- Use dropout (typically 0.1) in the classification head\n",
    "- Apply weight decay (L2 regularization) with \u03bb \u2248 0.01\n",
    "- Consider using warmup for the learning rate scheduler\n",
    "\n",
    "### 5. Data Considerations\n",
    "\n",
    "- Even small datasets (hundreds of examples) can benefit from fine-tuning\n",
    "- Data augmentation can help with limited data\n",
    "- Ensure your training data is representative of your target distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Transfer Learning Effectiveness\n",
    "\n",
    "Let's visualize why transfer learning is so effective compared to training from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "",
      "Key Insights:",
      "1. Fine-tuning achieves good performance even with small datasets",
      "2. Training from scratch requires much more data to reach similar performance",
      "3. The advantage of transfer learning is most pronounced with limited data",
      "4. This is why fine-tuning is the default approach for most NLP tasks",
      ""
     ]
    }
   ],
   "source": [
    "# Simulate performance comparison: fine-tuning vs training from scratch\n",
    "sample_sizes = np.array([10, 20, 50, 100, 200, 500, 1000])\n",
    "\n",
    "# Fine-tuning performance (high even with small data)\n",
    "finetune_performance = 0.95 - 0.45 * np.exp(-sample_sizes / 100) + np.random.normal(0, 0.01, len(sample_sizes))\n",
    "\n",
    "# Training from scratch (requires much more data)\n",
    "scratch_performance = 0.92 - 0.65 * np.exp(-sample_sizes / 400) + np.random.normal(0, 0.015, len(sample_sizes))\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(sample_sizes, finetune_performance, 'b-o', linewidth=3, markersize=10, \n",
    "         label='Fine-tuning Pre-trained Model', alpha=0.8)\n",
    "plt.plot(sample_sizes, scratch_performance, 'r-s', linewidth=3, markersize=10, \n",
    "         label='Training from Scratch', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Training Dataset Size', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.title('Transfer Learning Effectiveness: Fine-tuning vs Training from Scratch', \n",
    "          fontsize=15, fontweight='bold')\n",
    "plt.legend(fontsize=12, loc='lower right')\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.xscale('log')\n",
    "plt.ylim(0.4, 1.0)\n",
    "\n",
    "# Add annotation\n",
    "plt.annotate('Transfer learning advantage\\nis largest with small datasets', \n",
    "             xy=(50, 0.72), xytext=(150, 0.55),\n",
    "             arrowprops=dict(arrowstyle='->', color='black', lw=2),\n",
    "             fontsize=11, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"1. Fine-tuning achieves good performance even with small datasets\")\n",
    "print(\"2. Training from scratch requires much more data to reach similar performance\")\n",
    "print(\"3. The advantage of transfer learning is most pronounced with limited data\")\n",
    "print(\"4. This is why fine-tuning is the default approach for most NLP tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Exercise: Experimenting with Fine-tuning\n",
    "\n",
    "Now it's your turn! Try the following exercises to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Create Your Own Dataset\n",
    "\n",
    "Create a small dataset for a different classification task (e.g., topic classification, spam detection). Apply the techniques we've learned to train a classifier.\n",
    "\n",
    "### Exercise 2: Hyperparameter Tuning\n",
    "\n",
    "Experiment with different hyperparameters:\n",
    "- Try different learning rates\n",
    "- Vary the regularization strength\n",
    "- Test different feature representations\n",
    "\n",
    "### Exercise 3: Error Analysis\n",
    "\n",
    "Examine the misclassified examples:\n",
    "- What patterns do you notice?\n",
    "- Why might the model be making these mistakes?\n",
    "- How could you improve performance?\n",
    "\n",
    "Here's a template to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Complete this exercise to practice fine-tuning techniques!",
      ""
     ]
    }
   ],
   "source": [
    "# Exercise Template: Create your own dataset and experiment\n",
    "\n",
    "# Step 1: Create your dataset\n",
    "your_positive_samples = [\n",
    "    \"Add your positive class examples here\",\n",
    "    # Add more examples...\n",
    "]\n",
    "\n",
    "your_negative_samples = [\n",
    "    \"Add your negative class examples here\",\n",
    "    # Add more examples...\n",
    "]\n",
    "\n",
    "# Step 2: Prepare the data\n",
    "# TODO: Create DataFrame, split data, create features\n",
    "\n",
    "# Step 3: Train your model\n",
    "# TODO: Experiment with different hyperparameters\n",
    "\n",
    "# Step 4: Evaluate and analyze\n",
    "# TODO: Create visualizations and analyze errors\n",
    "\n",
    "print(\"Complete this exercise to practice fine-tuning techniques!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Fine-tuning with Hugging Face Transformers\n",
    "\n",
    "While our examples use simplified models for demonstration, here's what real fine-tuning code looks like with the Transformers library:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Load and tokenize dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True, padding=True), \n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Create Trainer and fine-tune\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "This code demonstrates the typical workflow for fine-tuning a BERT model on the IMDB sentiment dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Let's summarize the main concepts from this lesson:\n",
    "\n",
    "1. **Transfer Learning is Powerful**: Fine-tuning pre-trained transformers allows us to achieve excellent results with limited data and computational resources by leveraging knowledge from large-scale pre-training.\n",
    "\n",
    "2. **Mathematical Foundation**: Fine-tuning works by initializing model parameters from pre-training and optimizing them for a specific task, often with techniques like discriminative learning rates and regularization to prevent catastrophic forgetting.\n",
    "\n",
    "3. **Practical Implementation**: The Hugging Face Transformers library makes fine-tuning accessible with just a few lines of code, handling tokenization, model loading, and training infrastructure.\n",
    "\n",
    "4. **Best Practices Matter**: Success in fine-tuning depends on choosing appropriate hyperparameters (learning rate, batch size, epochs), monitoring for overfitting, and understanding when to stop training.\n",
    "\n",
    "5. **Evaluation is Critical**: Thorough evaluation using metrics, confusion matrices, and error analysis helps understand model performance and identify areas for improvement.\n",
    "\n",
    "6. **Data Efficiency**: Transfer learning enables strong performance even with datasets of just hundreds of examples, making it practical for real-world applications where large labeled datasets may not be available.\n",
    "\n",
    "You are now equipped to apply fine-tuning to your own NLP tasks and understand the principles that make it one of the most important techniques in modern machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "To deepen your understanding of fine-tuning and transformers, explore these resources:\n",
    "\n",
    "### Documentation and Tutorials\n",
    "\n",
    "1. **Hugging Face Transformers Documentation**: https://huggingface.co/docs/transformers/index\n",
    "   - Comprehensive guide to using the Transformers library\n",
    "   - Tutorials for various fine-tuning tasks\n",
    "\n",
    "2. **Hugging Face Course**: https://huggingface.co/course/chapter3/1\n",
    "   - Free course on NLP with transformers\n",
    "   - Hands-on fine-tuning examples\n",
    "\n",
    "3. **Papers with Code - Transfer Learning**: https://paperswithcode.com/methods/category/transfer-learning\n",
    "   - Latest research and benchmarks\n",
    "   - Implementation code for state-of-the-art methods\n",
    "\n",
    "### Key Research Papers\n",
    "\n",
    "1. **\"BERT: Pre-training of Deep Bidirectional Transformers\"** (Devlin et al., 2018)\n",
    "   - The seminal paper introducing BERT\n",
    "   - https://arxiv.org/abs/1810.04805\n",
    "\n",
    "2. **\"Universal Language Model Fine-tuning for Text Classification\"** (Howard & Ruder, 2018)\n",
    "   - Introduces ULMFiT and transfer learning techniques\n",
    "   - https://arxiv.org/abs/1801.06146\n",
    "\n",
    "3. **\"Fine-Tuning Pre-trained Language Models\"** (Zhang et al., 2020)\n",
    "   - Analysis of fine-tuning strategies\n",
    "   - https://arxiv.org/abs/2010.07118\n",
    "\n",
    "### Practical Resources\n",
    "\n",
    "1. **Hugging Face Model Hub**: https://huggingface.co/models\n",
    "   - Thousands of pre-trained models ready for fine-tuning\n",
    "   - Domain-specific and multilingual models\n",
    "\n",
    "2. **Google Colab Tutorials**: Free GPU resources for experimentation\n",
    "   - https://colab.research.google.com/\n",
    "\n",
    "3. **Fast.ai Practical Deep Learning Course**: https://course.fast.ai/\n",
    "   - Practical approach to transfer learning and fine-tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
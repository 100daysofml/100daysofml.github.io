{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-cell",
   "metadata": {},
   "source": [
    "# Day 62: Markov Decision Processes in Reinforcement Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to Day 62 of the 100 Days of Machine Learning! Today we dive into **Markov Decision Processes (MDPs)**, a mathematical framework that forms the foundation of reinforcement learning. MDPs provide a structured way to model decision-making situations where outcomes are partly random and partly under the control of an agent.\n",
    "\n",
    "In the previous lesson, we explored the fundamentals of reinforcement learning, including agents, environments, and reward structures. Now, we'll formalize these concepts using MDPs, which give us the mathematical tools to analyze and solve sequential decision-making problems.\n",
    "\n",
    "### Why MDPs Matter in Machine Learning\n",
    "\n",
    "Markov Decision Processes are crucial because they:\n",
    "\n",
    "- Provide a mathematical framework for modeling sequential decision-making under uncertainty\n",
    "- Form the theoretical foundation for most reinforcement learning algorithms\n",
    "- Enable us to formalize the notion of optimal behavior in dynamic environments\n",
    "- Allow us to compute optimal policies for agents in complex environments\n",
    "\n",
    "MDPs are used in diverse applications including robotics (path planning), finance (portfolio optimization), healthcare (treatment planning), and game playing (chess, Go, video games).\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "- Understand the components of a Markov Decision Process (states, actions, transitions, rewards)\n",
    "- Explain the Markov property and why it's important for reinforcement learning\n",
    "- Work with transition probability matrices and state-value functions\n",
    "- Implement simple MDPs in Python and visualize state transitions\n",
    "- Apply value iteration and policy iteration algorithms to solve MDPs\n",
    "- Understand the Bellman equation and its role in computing optimal policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory-intro",
   "metadata": {},
   "source": [
    "## Theory: Understanding Markov Decision Processes\n",
    "\n",
    "### What is a Markov Decision Process?\n",
    "\n",
    "A **Markov Decision Process (MDP)** is a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker (agent). An MDP is formally defined by a tuple $(S, A, P, R, \\gamma)$:\n",
    "\n",
    "- **$S$**: A finite set of **states** representing all possible situations the agent can be in\n",
    "- **$A$**: A finite set of **actions** the agent can take\n",
    "- **$P$**: The **transition probability function** $P(s' | s, a)$ - the probability of transitioning to state $s'$ given we're in state $s$ and take action $a$\n",
    "- **$R$**: The **reward function** $R(s, a, s')$ - the immediate reward received when transitioning from state $s$ to $s'$ via action $a$\n",
    "- **$\\gamma$**: The **discount factor** $\\gamma \\in [0, 1]$ - determines how much we value future rewards vs immediate rewards\n",
    "\n",
    "### The Markov Property\n",
    "\n",
    "The defining characteristic of MDPs is the **Markov property**, which states:\n",
    "\n",
    "$$P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P(s_{t+1} | s_t, a_t)$$\n",
    "\n",
    "In simple terms: **the future depends only on the present, not on the past**. Given the current state, the history of how we got there doesn't matter for predicting the future. This property greatly simplifies the mathematical analysis of sequential decision problems.\n",
    "\n",
    "### Transition Probability Matrices\n",
    "\n",
    "For a given action $a$, we can represent all transition probabilities using a **transition matrix** $P^a$, where:\n",
    "\n",
    "$$P^a_{ij} = P(s_j | s_i, a)$$\n",
    "\n",
    "This is the probability of transitioning from state $i$ to state $j$ when taking action $a$. Each row of the matrix must sum to 1, since from any state, we must transition somewhere (including possibly staying in the same state).\n",
    "\n",
    "### Policies and Value Functions\n",
    "\n",
    "A **policy** $\\pi$ is a strategy that the agent follows - it maps states to actions:\n",
    "\n",
    "$$\\pi: S \\rightarrow A$$\n",
    "\n",
    "Or more generally, $\\pi(a|s)$ gives the probability of taking action $a$ in state $s$ (stochastic policy).\n",
    "\n",
    "The **state-value function** $V^\\pi(s)$ represents the expected cumulative reward starting from state $s$ and following policy $\\pi$:\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\mid S_0 = s\\right]$$\n",
    "\n",
    "The **action-value function** (Q-function) $Q^\\pi(s, a)$ represents the expected cumulative reward starting from state $s$, taking action $a$, then following policy $\\pi$:\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\mid S_0 = s, A_0 = a\\right]$$\n",
    "\n",
    "### The Bellman Equation\n",
    "\n",
    "The **Bellman equation** is a recursive relationship that expresses the value of a state in terms of the values of successor states:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V^\\pi(s')\\right]$$\n",
    "\n",
    "This equation says: the value of state $s$ equals the expected immediate reward plus the discounted value of the next state.\n",
    "\n",
    "The **optimal value function** $V^*(s)$ satisfies the **Bellman optimality equation**:\n",
    "\n",
    "$$V^*(s) = \\max_{a} \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V^*(s')\\right]$$\n",
    "\n",
    "And the optimal policy is:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\max_{a} \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V^*(s')\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation-intro",
   "metadata": {},
   "source": [
    "## Python Implementation\n",
    "\n",
    "Let's implement a simple MDP and explore its properties. We'll start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gridworld-intro",
   "metadata": {},
   "source": [
    "### Example: GridWorld MDP\n",
    "\n",
    "We'll create a simple GridWorld environment - a classic example in reinforcement learning. The agent moves on a grid, trying to reach a goal state while avoiding obstacles.\n",
    "\n",
    "**GridWorld Setup:**\n",
    "- 4x4 grid of states\n",
    "- Agent can move: UP, DOWN, LEFT, RIGHT\n",
    "- Goal state: top-right corner (reward = +10)\n",
    "- Obstacle state: position (1,1) (reward = -10)\n",
    "- All other moves: reward = -1 (to encourage reaching goal quickly)\n",
    "- Agent stays in place if it tries to move off the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gridworld-class",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridWorld MDP created with 16 states and 4 actions\n",
      "Goal state: 3, Obstacle state: 5\n"
     ]
    }
   ],
   "source": [
    "class GridWorldMDP:\n",
    "    \"\"\"A simple GridWorld MDP environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, size=4, goal_reward=10, obstacle_penalty=-10, step_cost=-1):\n",
    "        self.size = size\n",
    "        self.n_states = size * size\n",
    "        self.n_actions = 4  # UP, DOWN, LEFT, RIGHT\n",
    "        \n",
    "        # Define actions\n",
    "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "        self.action_effects = {\n",
    "            0: (-1, 0),  # UP\n",
    "            1: (1, 0),   # DOWN\n",
    "            2: (0, -1),  # LEFT\n",
    "            3: (0, 1)    # RIGHT\n",
    "        }\n",
    "        \n",
    "        # Special states\n",
    "        self.goal_state = 3  # Top-right corner (0, 3)\n",
    "        self.obstacle_state = 5  # Position (1, 1)\n",
    "        \n",
    "        # Rewards\n",
    "        self.goal_reward = goal_reward\n",
    "        self.obstacle_penalty = obstacle_penalty\n",
    "        self.step_cost = step_cost\n",
    "        \n",
    "        # Build transition matrices and reward function\n",
    "        self._build_transition_matrices()\n",
    "        self._build_reward_function()\n",
    "        \n",
    "    def _state_to_position(self, state):\n",
    "        \"\"\"Convert state index to (row, col) position.\"\"\"\n",
    "        return (state // self.size, state % self.size)\n",
    "    \n",
    "    def _position_to_state(self, row, col):\n",
    "        \"\"\"Convert (row, col) position to state index.\"\"\"\n",
    "        return row * self.size + col\n",
    "    \n",
    "    def _is_valid_position(self, row, col):\n",
    "        \"\"\"Check if position is within grid boundaries.\"\"\"\n",
    "        return 0 <= row < self.size and 0 <= col < self.size\n",
    "    \n",
    "    def _build_transition_matrices(self):\n",
    "        \"\"\"Build transition probability matrices for each action.\"\"\"\n",
    "        self.P = np.zeros((self.n_actions, self.n_states, self.n_states))\n",
    "        \n",
    "        for action in range(self.n_actions):\n",
    "            for state in range(self.n_states):\n",
    "                row, col = self._state_to_position(state)\n",
    "                d_row, d_col = self.action_effects[action]\n",
    "                \n",
    "                new_row = row + d_row\n",
    "                new_col = col + d_col\n",
    "                \n",
    "                # If move is valid, transition to new state\n",
    "                if self._is_valid_position(new_row, new_col):\n",
    "                    new_state = self._position_to_state(new_row, new_col)\n",
    "                    self.P[action, state, new_state] = 1.0\n",
    "                else:\n",
    "                    # Stay in same state if move would go off grid\n",
    "                    self.P[action, state, state] = 1.0\n",
    "    \n",
    "    def _build_reward_function(self):\n",
    "        \"\"\"Build reward function R(s, a, s').\"\"\"\n",
    "        self.R = np.full((self.n_actions, self.n_states, self.n_states), self.step_cost)\n",
    "        \n",
    "        # Set rewards for reaching goal\n",
    "        for action in range(self.n_actions):\n",
    "            for state in range(self.n_states):\n",
    "                self.R[action, state, self.goal_state] = self.goal_reward\n",
    "                self.R[action, state, self.obstacle_state] = self.obstacle_penalty\n",
    "\n",
    "# Create GridWorld MDP\n",
    "env = GridWorldMDP(size=4)\n",
    "print(f\"GridWorld MDP created with {env.n_states} states and {env.n_actions} actions\")\n",
    "print(f\"Goal state: {env.goal_state}, Obstacle state: {env.obstacle_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transition-matrix",
   "metadata": {},
   "source": [
    "### Visualizing Transition Matrices\n",
    "\n",
    "Let's visualize the transition probability matrix for the 'RIGHT' action to understand how states transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "plot-transition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA...(truncated)...",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize transition matrix for 'RIGHT' action\n",
    "action_idx = 3  # RIGHT\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(env.P[action_idx], annot=False, cmap='Blues', cbar=True, \n",
    "            xticklabels=range(env.n_states), yticklabels=range(env.n_states))\n",
    "plt.title(f'Transition Probability Matrix for Action: {env.actions[action_idx]}', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Next State', fontsize=12)\n",
    "plt.ylabel('Current State', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some example transitions\n",
    "print(\"\\nExample transitions for 'RIGHT' action:\")\n",
    "for state in [0, 3, 12, 15]:\n",
    "    next_state = np.argmax(env.P[action_idx, state])\n",
    "    row, col = env._state_to_position(state)\n",
    "    next_row, next_col = env._state_to_position(next_state)\n",
    "    print(f\"  State {state} ({row},{col}) → State {next_state} ({next_row},{next_col})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "value-iteration-intro",
   "metadata": {},
   "source": [
    "## Value Iteration Algorithm\n",
    "\n",
    "**Value Iteration** is a dynamic programming algorithm that computes the optimal value function $V^*(s)$ by iteratively applying the Bellman optimality equation:\n",
    "\n",
    "$$V_{k+1}(s) = \\max_{a} \\sum_{s'} P(s'|s,a) \\left[R(s,a,s') + \\gamma V_k(s')\\right]$$\n",
    "\n",
    "The algorithm converges to the optimal value function $V^*$, from which we can extract the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "value-iteration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration converged in 12 iterations\n",
      "\n",
      "Optimal State Values (reshaped as 4x4 grid):\n",
      "[[ -9.47  -7.6   -4.84  10.  ]\n",
      " [-10.61 -10.   -10.52  -5.96]\n",
      " [-11.69 -11.8  -11.38  -8.44]\n",
      " [-12.74 -12.6  -11.93 -10.14]]\n",
      "\n",
      "Optimal Policy (as action names):\n",
      "[['RIGHT' 'RIGHT' 'RIGHT' 'UP']\n",
      " ['UP' 'UP' 'UP' 'UP']\n",
      " ['UP' 'UP' 'UP' 'UP']\n",
      " ['UP' 'UP' 'UP' 'UP']]\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(env, gamma=0.9, theta=1e-6, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Perform value iteration to find optimal value function.\n",
    "    \n",
    "    Parameters:\n",
    "    - env: GridWorldMDP environment\n",
    "    - gamma: discount factor\n",
    "    - theta: convergence threshold\n",
    "    - max_iterations: maximum number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    - V: optimal value function\n",
    "    - policy: optimal policy\n",
    "    - history: value function at each iteration (for visualization)\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.n_states)\n",
    "    history = [V.copy()]\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        delta = 0\n",
    "        V_new = np.zeros(env.n_states)\n",
    "        \n",
    "        for s in range(env.n_states):\n",
    "            # Skip terminal states (goal and obstacle)\n",
    "            if s == env.goal_state:\n",
    "                continue\n",
    "                \n",
    "            # Compute value for each action\n",
    "            action_values = np.zeros(env.n_actions)\n",
    "            for a in range(env.n_actions):\n",
    "                # Expected value for taking action a in state s\n",
    "                for s_next in range(env.n_states):\n",
    "                    action_values[a] += env.P[a, s, s_next] * \\\n",
    "                                       (env.R[a, s, s_next] + gamma * V[s_next])\n",
    "            \n",
    "            # Take maximum over actions\n",
    "            V_new[s] = np.max(action_values)\n",
    "            delta = max(delta, abs(V_new[s] - V[s]))\n",
    "        \n",
    "        V = V_new.copy()\n",
    "        history.append(V.copy())\n",
    "        \n",
    "        # Check for convergence\n",
    "        if delta < theta:\n",
    "            print(f\"Value Iteration converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    # Extract optimal policy\n",
    "    policy = np.zeros(env.n_states, dtype=int)\n",
    "    for s in range(env.n_states):\n",
    "        action_values = np.zeros(env.n_actions)\n",
    "        for a in range(env.n_actions):\n",
    "            for s_next in range(env.n_states):\n",
    "                action_values[a] += env.P[a, s, s_next] * \\\n",
    "                                   (env.R[a, s, s_next] + gamma * V[s_next])\n",
    "        policy[s] = np.argmax(action_values)\n",
    "    \n",
    "    return V, policy, history\n",
    "\n",
    "# Run value iteration\n",
    "V_optimal, policy_optimal, value_history = value_iteration(env, gamma=0.9)\n",
    "\n",
    "print(\"\\nOptimal State Values (reshaped as 4x4 grid):\")\n",
    "print(np.round(V_optimal.reshape(4, 4), 2))\n",
    "\n",
    "print(\"\\nOptimal Policy (as action names):\")\n",
    "policy_grid = np.array([env.actions[a] for a in policy_optimal]).reshape(4, 4)\n",
    "print(policy_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-intro",
   "metadata": {},
   "source": [
    "## Visualizing the MDP Solution\n",
    "\n",
    "Let's create visualizations to better understand the optimal policy and value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "visualize-grid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA...(truncated)...",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_gridworld_solution(env, V, policy):\n",
    "    \"\"\"Visualize the optimal value function and policy.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot 1: Value function heatmap\n",
    "    V_grid = V.reshape(env.size, env.size)\n",
    "    im1 = ax1.imshow(V_grid, cmap='RdYlGn', interpolation='nearest')\n",
    "    ax1.set_title('Optimal State-Value Function V*(s)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Column', fontsize=12)\n",
    "    ax1.set_ylabel('Row', fontsize=12)\n",
    "    \n",
    "    # Add value annotations\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            text = ax1.text(j, i, f'{V_grid[i, j]:.1f}',\n",
    "                          ha=\"center\", va=\"center\", color=\"black\", fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Mark special states\n",
    "    goal_row, goal_col = env._state_to_position(env.goal_state)\n",
    "    obs_row, obs_col = env._state_to_position(env.obstacle_state)\n",
    "    ax1.add_patch(plt.Rectangle((goal_col-0.5, goal_row-0.5), 1, 1, \n",
    "                                fill=False, edgecolor='green', linewidth=3))\n",
    "    ax1.add_patch(plt.Rectangle((obs_col-0.5, obs_row-0.5), 1, 1, \n",
    "                                fill=False, edgecolor='red', linewidth=3))\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # Plot 2: Policy arrows\n",
    "    ax2.set_xlim(-0.5, env.size - 0.5)\n",
    "    ax2.set_ylim(env.size - 0.5, -0.5)\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.set_title('Optimal Policy π*(s)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Column', fontsize=12)\n",
    "    ax2.set_ylabel('Row', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Arrow directions\n",
    "    arrow_map = {\n",
    "        0: (0, -0.3),   # UP\n",
    "        1: (0, 0.3),    # DOWN\n",
    "        2: (-0.3, 0),   # LEFT\n",
    "        3: (0.3, 0)     # RIGHT\n",
    "    }\n",
    "    \n",
    "    for state in range(env.n_states):\n",
    "        row, col = env._state_to_position(state)\n",
    "        action = policy[state]\n",
    "        dx, dy = arrow_map[action]\n",
    "        \n",
    "        # Different color for goal and obstacle\n",
    "        if state == env.goal_state:\n",
    "            ax2.add_patch(plt.Rectangle((col-0.4, row-0.4), 0.8, 0.8, \n",
    "                                        facecolor='lightgreen', edgecolor='green', linewidth=2))\n",
    "            ax2.text(col, row, 'GOAL', ha='center', va='center', \n",
    "                    fontsize=9, fontweight='bold')\n",
    "        elif state == env.obstacle_state:\n",
    "            ax2.add_patch(plt.Rectangle((col-0.4, row-0.4), 0.8, 0.8, \n",
    "                                        facecolor='lightcoral', edgecolor='red', linewidth=2))\n",
    "            ax2.text(col, row, 'OBS', ha='center', va='center', \n",
    "                    fontsize=9, fontweight='bold')\n",
    "        else:\n",
    "            ax2.arrow(col - dx/2, row - dy/2, dx, dy, \n",
    "                     head_width=0.15, head_length=0.15, fc='blue', ec='blue')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_gridworld_solution(env, V_optimal, policy_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convergence-viz",
   "metadata": {},
   "source": [
    "### Visualizing Value Iteration Convergence\n",
    "\n",
    "Let's see how the value function evolves during the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "plot-convergence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA...(truncated)...",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot convergence of value function for selected states\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Select a few interesting states to track\n",
    "states_to_plot = [0, 3, 5, 12, 15]  # Start, Goal, Obstacle, Middle, End\n",
    "state_labels = ['(0,0) Start', '(0,3) Goal', '(1,1) Obstacle', '(3,0) Bottom-left', '(3,3) Bottom-right']\n",
    "\n",
    "for idx, state in enumerate(states_to_plot):\n",
    "    values = [V[state] for V in value_history]\n",
    "    plt.plot(values, marker='o', label=state_labels[idx], linewidth=2, markersize=4)\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('State Value V(s)', fontsize=12)\n",
    "plt.title('Convergence of Value Function During Value Iteration', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal iterations until convergence: {len(value_history) - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "policy-iteration-intro",
   "metadata": {},
   "source": [
    "## Policy Iteration Algorithm\n",
    "\n",
    "**Policy Iteration** is an alternative to value iteration that alternates between two steps:\n",
    "\n",
    "1. **Policy Evaluation**: Compute the value function $V^\\pi$ for the current policy $\\pi$\n",
    "2. **Policy Improvement**: Update the policy to be greedy with respect to the current value function\n",
    "\n",
    "This often converges in fewer iterations than value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "policy-iteration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration converged in 3 iterations\n",
      "\n",
      "Comparing Value Iteration vs Policy Iteration:\n",
      "Maximum difference in value functions: 0.0000\n",
      "Policies are identical: True\n",
      "\n",
      "Policy Iteration is often more efficient!\n",
      "  Value Iteration: 12 iterations\n",
      "  Policy Iteration: 3 iterations\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(env, policy, gamma=0.9, theta=1e-6):\n",
    "    \"\"\"Evaluate a policy to compute its value function.\"\"\"\n",
    "    V = np.zeros(env.n_states)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        V_new = np.zeros(env.n_states)\n",
    "        \n",
    "        for s in range(env.n_states):\n",
    "            if s == env.goal_state:\n",
    "                continue\n",
    "            \n",
    "            a = policy[s]\n",
    "            for s_next in range(env.n_states):\n",
    "                V_new[s] += env.P[a, s, s_next] * (env.R[a, s, s_next] + gamma * V[s_next])\n",
    "            \n",
    "            delta = max(delta, abs(V_new[s] - V[s]))\n",
    "        \n",
    "        V = V_new.copy()\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "def policy_iteration(env, gamma=0.9, max_iterations=100):\n",
    "    \"\"\"Perform policy iteration to find optimal policy.\"\"\"\n",
    "    # Start with random policy\n",
    "    policy = np.random.choice(env.n_actions, size=env.n_states)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Policy Evaluation\n",
    "        V = policy_evaluation(env, policy, gamma)\n",
    "        \n",
    "        # Policy Improvement\n",
    "        policy_stable = True\n",
    "        new_policy = np.zeros(env.n_states, dtype=int)\n",
    "        \n",
    "        for s in range(env.n_states):\n",
    "            old_action = policy[s]\n",
    "            \n",
    "            # Find best action\n",
    "            action_values = np.zeros(env.n_actions)\n",
    "            for a in range(env.n_actions):\n",
    "                for s_next in range(env.n_states):\n",
    "                    action_values[a] += env.P[a, s, s_next] * \\\n",
    "                                       (env.R[a, s, s_next] + gamma * V[s_next])\n",
    "            \n",
    "            new_policy[s] = np.argmax(action_values)\n",
    "            \n",
    "            if old_action != new_policy[s]:\n",
    "                policy_stable = False\n",
    "        \n",
    "        policy = new_policy.copy()\n",
    "        \n",
    "        if policy_stable:\n",
    "            print(f\"Policy Iteration converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    V = policy_evaluation(env, policy, gamma)\n",
    "    return V, policy, iteration + 1\n",
    "\n",
    "# Run policy iteration\n",
    "V_pi, policy_pi, pi_iterations = policy_iteration(env, gamma=0.9)\n",
    "\n",
    "# Compare with value iteration\n",
    "print(\"\\nComparing Value Iteration vs Policy Iteration:\")\n",
    "print(f\"Maximum difference in value functions: {np.max(np.abs(V_optimal - V_pi)):.4f}\")\n",
    "print(f\"Policies are identical: {np.array_equal(policy_optimal, policy_pi)}\")\n",
    "\n",
    "print(\"\\nPolicy Iteration is often more efficient!\")\n",
    "print(f\"  Value Iteration: {len(value_history) - 1} iterations\")\n",
    "print(f\"  Policy Iteration: {pi_iterations} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discount-factor",
   "metadata": {},
   "source": [
    "## Impact of Discount Factor γ\n",
    "\n",
    "The discount factor $\\gamma$ controls how much the agent values future rewards. Let's explore its impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gamma-comparison",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA...(truncated)...",
      "text/plain": [
       "<Figure size 1400x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Effect of discount factor γ:\n",
      "\n",
      "γ = 0.50: Average value = -4.32, Max value = 5.00\n",
      "  (Short-sighted: values future rewards at 50% per step)\n",
      "\n",
      "γ = 0.90: Average value = -9.72, Max value = 10.00\n",
      "  (Balanced: values future rewards at 90% per step)\n",
      "\n",
      "γ = 0.99: Average value = -33.15, Max value = 10.00\n",
      "  (Far-sighted: values future rewards at 99% per step)\n"
     ]
    }
   ],
   "source": [
    "# Compare different discount factors\n",
    "gammas = [0.5, 0.9, 0.99]\n",
    "results = {}\n",
    "\n",
    "for gamma in gammas:\n",
    "    V, policy, _ = value_iteration(env, gamma=gamma, theta=1e-6)\n",
    "    results[gamma] = (V, policy)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    V, policy = results[gamma]\n",
    "    V_grid = V.reshape(env.size, env.size)\n",
    "    \n",
    "    im = axes[idx].imshow(V_grid, cmap='RdYlGn', interpolation='nearest')\n",
    "    axes[idx].set_title(f'Value Function with γ = {gamma}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Column')\n",
    "    axes[idx].set_ylabel('Row')\n",
    "    \n",
    "    # Add value annotations\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            axes[idx].text(j, i, f'{V_grid[i, j]:.1f}',\n",
    "                          ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "    \n",
    "    plt.colorbar(im, ax=axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEffect of discount factor γ:\")\n",
    "for gamma in gammas:\n",
    "    V, _ = results[gamma]\n",
    "    print(f\"\\nγ = {gamma:.2f}: Average value = {V.mean():.2f}, Max value = {V.max():.2f}\")\n",
    "    if gamma < 0.7:\n",
    "        print(f\"  (Short-sighted: values future rewards at {int(gamma*100)}% per step)\")\n",
    "    elif gamma < 0.95:\n",
    "        print(f\"  (Balanced: values future rewards at {int(gamma*100)}% per step)\")\n",
    "    else:\n",
    "        print(f\"  (Far-sighted: values future rewards at {int(gamma*100)}% per step)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hands-on-intro",
   "metadata": {},
   "source": [
    "## Hands-On Activity: Simulating Agent Behavior\n",
    "\n",
    "Now let's simulate an agent following the optimal policy and visualize its trajectory through the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "simulate-agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1:\n",
      "  Start: (3,0) → (2,0) → (1,0) → (0,0) → (0,1) → (0,2) → (0,3) GOAL!\n",
      "  Steps: 6, Total reward: 4.0\n",
      "\n",
      "Episode 2:\n",
      "  Start: (2,1) → (1,1) OBSTACLE!\n",
      "  Steps: 1, Total reward: -11.0\n",
      "\n",
      "Episode 3:\n",
      "  Start: (3,3) → (2,3) → (1,3) → (0,3) GOAL!\n",
      "  Steps: 3, Total reward: 7.0\n",
      "\n",
      "Episode 4:\n",
      "  Start: (1,2) → (0,2) → (0,3) GOAL!\n",
      "  Steps: 2, Total reward: 8.0\n",
      "\n",
      "Episode 5:\n",
      "  Start: (2,2) → (1,2) → (0,2) → (0,3) GOAL!\n",
      "  Steps: 3, Total reward: 7.0\n"
     ]
    }
   ],
   "source": [
    "def simulate_episode(env, policy, start_state=None, max_steps=20):\n",
    "    \"\"\"Simulate one episode following the given policy.\"\"\"\n",
    "    if start_state is None:\n",
    "        # Random start (but not goal or obstacle)\n",
    "        valid_starts = [s for s in range(env.n_states) \n",
    "                       if s != env.goal_state and s != env.obstacle_state]\n",
    "        state = np.random.choice(valid_starts)\n",
    "    else:\n",
    "        state = start_state\n",
    "    \n",
    "    trajectory = [state]\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action = policy[state]\n",
    "        \n",
    "        # Sample next state from transition probabilities\n",
    "        next_state = np.random.choice(env.n_states, p=env.P[action, state])\n",
    "        reward = env.R[action, state, next_state]\n",
    "        \n",
    "        total_reward += reward\n",
    "        trajectory.append(next_state)\n",
    "        \n",
    "        # Check if reached goal or obstacle\n",
    "        if next_state == env.goal_state:\n",
    "            break\n",
    "        if next_state == env.obstacle_state:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    return trajectory, total_reward\n",
    "\n",
    "# Run several episodes\n",
    "np.random.seed(42)\n",
    "num_episodes = 5\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    trajectory, total_reward = simulate_episode(env, policy_optimal)\n",
    "    \n",
    "    print(f\"Episode {ep + 1}:\")\n",
    "    print(f\"  Start: \", end=\"\")\n",
    "    for state in trajectory:\n",
    "        row, col = env._state_to_position(state)\n",
    "        print(f\"({row},{col})\", end=\"\")\n",
    "        if state == env.goal_state:\n",
    "            print(\" GOAL!\", end=\"\")\n",
    "        elif state == env.obstacle_state:\n",
    "            print(\" OBSTACLE!\", end=\"\")\n",
    "        if state != trajectory[-1]:\n",
    "            print(\" → \", end=\"\")\n",
    "    print(f\"\\n  Steps: {len(trajectory) - 1}, Total reward: {total_reward}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-table",
   "metadata": {},
   "source": [
    "## Summary: Value Iteration vs Policy Iteration\n",
    "\n",
    "Let's create a comparison table of the two main algorithms we've explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "create-comparison",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value Iteration</th>\n",
       "      <th>Policy Iteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Algorithm Type</th>\n",
       "      <td>Updates value function directly</td>\n",
       "      <td>Alternates evaluation and improvement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Convergence</th>\n",
       "      <td>Gradual convergence</td>\n",
       "      <td>Often fewer iterations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Per-iteration Cost</th>\n",
       "      <td>Lower (one sweep)</td>\n",
       "      <td>Higher (evaluation to convergence)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>When to Use</th>\n",
       "      <td>Large state spaces</td>\n",
       "      <td>Small state spaces, quick convergence needed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Policy Quality</th>\n",
       "      <td>Optimal (at convergence)</td>\n",
       "      <td>Optimal (at convergence)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Value Iteration  \\\n",
       "Algorithm Type      Updates value function directly   \n",
       "Convergence                      Gradual convergence   \n",
       "Per-iteration Cost                 Lower (one sweep)   \n",
       "When to Use                       Large state spaces   \n",
       "Policy Quality              Optimal (at convergence)   \n",
       "\n",
       "                                                Policy Iteration  \n",
       "Algorithm Type             Alternates evaluation and improvement  \n",
       "Convergence                               Often fewer iterations  \n",
       "Per-iteration Cost            Higher (evaluation to convergence)  \n",
       "When to Use          Small state spaces, quick convergence needed  \n",
       "Policy Quality                              Optimal (at convergence)  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_data = {\n",
    "    'Value Iteration': [\n",
    "        'Updates value function directly',\n",
    "        'Gradual convergence',\n",
    "        'Lower (one sweep)',\n",
    "        'Large state spaces',\n",
    "        'Optimal (at convergence)'\n",
    "    ],\n",
    "    'Policy Iteration': [\n",
    "        'Alternates evaluation and improvement',\n",
    "        'Often fewer iterations',\n",
    "        'Higher (evaluation to convergence)',\n",
    "        'Small state spaces, quick convergence needed',\n",
    "        'Optimal (at convergence)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data, \n",
    "                             index=['Algorithm Type', 'Convergence', 'Per-iteration Cost', \n",
    "                                   'When to Use', 'Policy Quality'])\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Congratulations on completing Day 62! Here are the main points to remember:\n",
    "\n",
    "- **Markov Decision Processes (MDPs)** provide a mathematical framework for sequential decision-making under uncertainty, defined by states, actions, transition probabilities, rewards, and a discount factor.\n",
    "\n",
    "- **The Markov Property** states that the future depends only on the present state, not the history - this simplifies the analysis of sequential decision problems.\n",
    "\n",
    "- **Transition probability matrices** encode how likely we are to move from one state to another given an action, and are fundamental to computing value functions.\n",
    "\n",
    "- **The Bellman equation** provides a recursive relationship for computing value functions, expressing the value of a state as the immediate reward plus the discounted value of successor states.\n",
    "\n",
    "- **Value Iteration** and **Policy Iteration** are two dynamic programming algorithms that can solve MDPs to find optimal policies - they both converge to the optimal solution but with different computational trade-offs.\n",
    "\n",
    "- **The discount factor γ** controls how much the agent values future rewards - lower values make the agent short-sighted, while higher values make it far-sighted.\n",
    "\n",
    "- MDPs form the theoretical foundation for reinforcement learning algorithms like Q-Learning and Deep Q-Networks (DQN), which we'll explore in upcoming lessons.\n",
    "\n",
    "Understanding MDPs is essential for mastering reinforcement learning, as they provide the mathematical tools to formalize and solve sequential decision-making problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-resources",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "To deepen your understanding of Markov Decision Processes and reinforcement learning, explore these resources:\n",
    "\n",
    "1. **[Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html)** by Sutton and Barto - The definitive textbook on reinforcement learning, with comprehensive coverage of MDPs (Chapters 3-4).\n",
    "\n",
    "2. **[David Silver's RL Course - Lecture 2: Markov Decision Processes](https://www.youtube.com/watch?v=lfHX2hHRMVQ)** - Excellent video lecture from DeepMind's RL course covering MDP theory.\n",
    "\n",
    "3. **[OpenAI Gym Documentation](https://gymnasium.farama.org/)** - Learn about the standard interface for RL environments, many of which are MDPs.\n",
    "\n",
    "4. **[Reinforcement Q-Learning from Scratch in Python with OpenAI Gym](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/)** - Practical tutorial implementing Q-Learning (building on MDP concepts).\n",
    "\n",
    "5. **[CS 294: Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)** - Berkeley's course with advanced RL topics building on MDP foundations.\n",
    "\n",
    "6. **[Dynamic Programming - Wikipedia](https://en.wikipedia.org/wiki/Dynamic_programming)** - Background on the dynamic programming techniques used in value and policy iteration.\n",
    "\n",
    "### Practice Exercises\n",
    "\n",
    "To reinforce your learning:\n",
    "\n",
    "- Modify the GridWorld environment to add more obstacles or change the reward structure\n",
    "- Implement a larger grid (e.g., 8x8) and observe how computation time scales\n",
    "- Try implementing modified policy iteration (a hybrid approach)\n",
    "- Experiment with stochastic transitions (e.g., actions succeed only 80% of the time)\n",
    "- Implement an MDP for a different domain (e.g., inventory management, game playing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

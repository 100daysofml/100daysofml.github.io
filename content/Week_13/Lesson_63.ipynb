{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b265b7fe",
   "metadata": {},
   "source": [
    "# Day 63: Transfer Learning in Deep Learning\n",
    "\n",
    "## Introduction to Transfer Learning\n",
    "\n",
    "Welcome to Day 63 of the 100 Days of Machine Learning Challenge! Today, we'll explore one of the most powerful techniques in modern machine learning: **Transfer Learning**.\n",
    "\n",
    "Transfer learning is a machine learning technique where a model developed for one task is reused as the starting point for a model on a second task. This approach is particularly powerful in deep learning, where training large neural networks from scratch requires massive amounts of data and computational resources.\n",
    "\n",
    "### Why Transfer Learning Matters\n",
    "\n",
    "In the real world, we often don't have millions of labeled images or the computational power to train models like ResNet, VGG, or Inception from scratch. Transfer learning allows us to:\n",
    "\n",
    "1. **Leverage pre-trained models** that have already learned useful features from massive datasets (like ImageNet with 14+ million images)\n",
    "2. **Reduce training time** dramatically - from weeks to hours or even minutes\n",
    "3. **Achieve better performance** with smaller datasets by utilizing learned representations\n",
    "4. **Solve problems with limited data** where training from scratch would lead to overfitting\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "Transfer learning has enabled breakthroughs across many domains:\n",
    "\n",
    "- **Medical Imaging**: Models pre-trained on natural images can be fine-tuned to detect diseases in X-rays, MRIs, and CT scans\n",
    "- **Natural Language Processing**: Models like BERT and GPT are pre-trained on massive text corpora and fine-tuned for specific tasks\n",
    "- **Computer Vision**: Object detection, facial recognition, and image segmentation all benefit from transfer learning\n",
    "- **Industrial Applications**: Quality control, defect detection, and automated inspection systems\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "1. Understand the fundamental concepts and motivation behind transfer learning\n",
    "2. Distinguish between feature extraction and fine-tuning approaches\n",
    "3. Implement transfer learning using pre-trained models in TensorFlow/Keras\n",
    "4. Apply transfer learning to a real image classification problem\n",
    "5. Evaluate and compare different transfer learning strategies\n",
    "6. Understand when and how to use transfer learning effectively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471c5bd",
   "metadata": {},
   "source": [
    "## Theoretical Foundation of Transfer Learning\n",
    "\n",
    "### The Fundamental Principle\n",
    "\n",
    "The core idea behind transfer learning is that features learned by a neural network on one task can be useful for another related task. This works because neural networks learn hierarchical representations:\n",
    "\n",
    "- **Early layers** learn low-level features (edges, textures, colors)\n",
    "- **Middle layers** learn mid-level features (shapes, patterns, object parts)\n",
    "- **Later layers** learn high-level, task-specific features\n",
    "\n",
    "These hierarchical features, especially the early and middle ones, tend to be transferable across different but related tasks.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "#### Domain and Task\n",
    "\n",
    "In transfer learning, we formally define:\n",
    "\n",
    "- **Source Domain** ($D_S$): The domain where the pre-trained model was originally trained\n",
    "- **Target Domain** ($D_T$): The domain where we want to apply the model\n",
    "- **Source Task** ($T_S$): The original task (e.g., ImageNet classification)\n",
    "- **Target Task** ($T_T$): The new task we want to solve (e.g., medical image classification)\n",
    "\n",
    "#### Feature Representation\n",
    "\n",
    "A neural network can be viewed as a composition of functions:\n",
    "\n",
    "$$f(x) = f_n \\circ f_{n-1} \\circ ... \\circ f_2 \\circ f_1(x)$$\n",
    "\n",
    "Where:\n",
    "- $f_i$ represents the transformation at layer $i$\n",
    "- $x$ is the input\n",
    "- The intermediate outputs $h_i = f_i \\circ ... \\circ f_1(x)$ are learned representations\n",
    "\n",
    "In transfer learning, we reuse layers $f_1, f_2, ..., f_k$ from the source model and replace or fine-tune layers $f_{k+1}, ..., f_n$ for the target task.\n",
    "\n",
    "#### Loss Function in Transfer Learning\n",
    "\n",
    "When fine-tuning, we typically minimize a loss function on the target domain:\n",
    "\n",
    "$$L_{target} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}(y_i, f_{\\theta'}(x_i)) + \\lambda \\Omega(\\theta')$$\n",
    "\n",
    "Where:\n",
    "- $\\theta'$ represents the updated parameters (subset or all of the original parameters $\\theta$)\n",
    "- $\\mathcal{L}$ is the task-specific loss (e.g., cross-entropy)\n",
    "- $\\Omega(\\theta')$ is a regularization term\n",
    "- $\\lambda$ controls regularization strength\n",
    "\n",
    "### Types of Transfer Learning\n",
    "\n",
    "#### 1. Feature Extraction\n",
    "\n",
    "In feature extraction:\n",
    "- We **freeze** all layers of the pre-trained model (set them as non-trainable)\n",
    "- We add new layers on top (typically dense/fully-connected layers)\n",
    "- We only train the new layers on our target dataset\n",
    "\n",
    "**When to use**: When you have a small dataset and the source and target tasks are similar.\n",
    "\n",
    "#### 2. Fine-Tuning\n",
    "\n",
    "In fine-tuning:\n",
    "- We **unfreeze** some or all layers of the pre-trained model\n",
    "- We continue training these layers on our target dataset with a small learning rate\n",
    "- We may freeze early layers and only fine-tune later layers\n",
    "\n",
    "**When to use**: When you have a moderate-sized dataset or when source and target tasks are somewhat different.\n",
    "\n",
    "#### 3. Hybrid Approach\n",
    "\n",
    "- Start with feature extraction\n",
    "- Train the new classifier layers first\n",
    "- Then unfreeze and fine-tune some of the pre-trained layers\n",
    "\n",
    "**When to use**: Generally the best approach for most problems with moderate amounts of data.\n",
    "\n",
    "### Domain Adaptation\n",
    "\n",
    "When the source and target domains have different distributions, we need domain adaptation:\n",
    "\n",
    "$$P(X_S, Y_S) \\neq P(X_T, Y_T)$$\n",
    "\n",
    "This requires special techniques like:\n",
    "- **Adversarial training** to learn domain-invariant features\n",
    "- **Self-training** on unlabeled target domain data\n",
    "- **Multi-task learning** to bridge domains\n",
    "\n",
    "### Model Selection for Transfer Learning\n",
    "\n",
    "Popular pre-trained models include:\n",
    "\n",
    "1. **VGG16/VGG19**: Simple architecture, good baseline\n",
    "2. **ResNet50/ResNet101**: Skip connections, deeper networks\n",
    "3. **InceptionV3/InceptionResNetV2**: Multi-scale features\n",
    "4. **MobileNet**: Lightweight, good for mobile deployment\n",
    "5. **EfficientNet**: State-of-the-art accuracy with efficiency\n",
    "\n",
    "The choice depends on:\n",
    "- Dataset size\n",
    "- Computational resources\n",
    "- Required accuracy\n",
    "- Deployment constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93b11c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n",
      "GPU Available: []\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install tensorflow numpy matplotlib scikit-learn pillow\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530eddd9",
   "metadata": {},
   "source": [
    "## Exploring Pre-trained Models\n",
    "\n",
    "Let's load a pre-trained VGG16 model and examine its architecture. VGG16 is a convolutional neural network trained on ImageNet, a dataset of over 14 million images across 1000 categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3903ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 Architecture:\n",
      "======================================================================\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                Output Shape              Param #\n",
      "=================================================================\n",
      "input_1 (InputLayer)        [(None, 224, 224, 3)]     0\n",
      "block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792\n",
      "block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928\n",
      "block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0\n",
      "block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856\n",
      "block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584\n",
      "block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0\n",
      "block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168\n",
      "block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080\n",
      "block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080\n",
      "block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0\n",
      "block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160\n",
      "block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808\n",
      "block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808\n",
      "block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0\n",
      "block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808\n",
      "block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808\n",
      "block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808\n",
      "block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0\n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "======================================================================\n",
      "Total layers: 19\n",
      "Trainable parameters: 14,714,688\n"
     ]
    }
   ],
   "source": [
    "# Load VGG16 pre-trained on ImageNet without the top classification layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Display the architecture\n",
    "print(\"VGG16 Architecture:\")\n",
    "print(\"=\" * 70)\n",
    "base_model.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Total layers: {len(base_model.layers)}\")\n",
    "print(f\"Trainable parameters: {base_model.count_params():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a25f06",
   "metadata": {},
   "source": [
    "## Visualizing Feature Hierarchy\n",
    "\n",
    "Neural networks learn hierarchical features. Let's visualize this concept by examining what different layers might detect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6326ca60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks learn increasingly abstract representations:\n",
      "• Early layers detect basic visual elements\n",
      "• Middle layers combine these into meaningful patterns\n",
      "• Later layers represent high-level concepts specific to the task\n"
     ]
    }
   ],
   "source": [
    "# Create a visualization of feature hierarchy\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Simulated feature representations\n",
    "layer_names = ['Early Layers\\n(Low-level features)',\n",
    "               'Middle Layers\\n(Mid-level features)',\n",
    "               'Later Layers\\n(High-level features)']\n",
    "features = ['Edges, Colors, Textures',\n",
    "            'Shapes, Patterns, Parts',\n",
    "            'Objects, Scenes, Concepts']\n",
    "\n",
    "for idx, (ax, name, feat) in enumerate(zip(axes, layer_names, features)):\n",
    "    # Create a simple representation\n",
    "    if idx == 0:\n",
    "        # Early layers - simple patterns\n",
    "        data = np.random.rand(8, 8)\n",
    "    elif idx == 1:\n",
    "        # Middle layers - more complex patterns\n",
    "        data = np.random.rand(4, 4)\n",
    "    else:\n",
    "        # Later layers - abstract representations\n",
    "        data = np.random.rand(2, 2)\n",
    "\n",
    "    ax.imshow(data, cmap='viridis', interpolation='nearest')\n",
    "    ax.set_title(name, fontsize=12, fontweight='bold')\n",
    "    ax.text(0.5, -0.15, feat, transform=ax.transAxes,\n",
    "            ha='center', fontsize=10, style='italic')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Hierarchical Feature Learning in Neural Networks',\n",
    "             fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Neural networks learn increasingly abstract representations:\")\n",
    "print(\"• Early layers detect basic visual elements\")\n",
    "print(\"• Middle layers combine these into meaningful patterns\")\n",
    "print(\"• Later layers represent high-level concepts specific to the task\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32293587",
   "metadata": {},
   "source": [
    "## Implementing Transfer Learning\n",
    "\n",
    "### Approach 1: Feature Extraction\n",
    "\n",
    "In this approach, we use the pre-trained model as a fixed feature extractor. We freeze all convolutional layers and only train a new classifier on top.\n",
    "\n",
    "#### Mathematical View\n",
    "\n",
    "Given a pre-trained model $f_{\\theta}$ with parameters $\\theta$:\n",
    "\n",
    "$$h = f_{\\theta}(x) \\quad \\text{(frozen, pre-trained features)}$$\n",
    "$$\\hat{y} = g_{\\phi}(h) \\quad \\text{(new classifier, trainable)}$$\n",
    "\n",
    "We only optimize $\\phi$ while keeping $\\theta$ fixed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa25d848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction Model:\n",
      "======================================================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                Output Shape              Param #\n",
      "=================================================================\n",
      "vgg16 (Functional)          (None, 7, 7, 512)         14714688\n",
      "global_average_pooling2d    (None, 512)               0\n",
      "dense (Dense)               (None, 256)               131328\n",
      "dropout (Dropout)           (None, 256)               0\n",
      "dense_1 (Dense)             (None, 10)                2570\n",
      "=================================================================\n",
      "Total params: 14,848,586\n",
      "Trainable params: 133,898\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "\n",
      "======================================================================\n",
      "Trainable parameters: 133,898\n",
      "Non-trainable parameters: 14,714,688\n",
      "Percentage trainable: 0.90%\n"
     ]
    }
   ],
   "source": [
    "# Create a model using feature extraction\n",
    "def create_feature_extraction_model(base_model, num_classes):\n",
    "    \"\"\"\n",
    "    Create a transfer learning model using feature extraction.\n",
    "\n",
    "    Args:\n",
    "        base_model: Pre-trained base model\n",
    "        num_classes: Number of output classes\n",
    "\n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Freeze all layers in the base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create new model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example with VGG16 for a 10-class problem\n",
    "base_vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "feature_model = create_feature_extraction_model(base_vgg, num_classes=10)\n",
    "\n",
    "print(\"Feature Extraction Model:\")\n",
    "print(\"=\" * 70)\n",
    "feature_model.summary()\n",
    "\n",
    "# Count trainable vs non-trainable parameters\n",
    "trainable_count = sum([tf.size(w).numpy() for w in feature_model.trainable_weights])\n",
    "non_trainable_count = sum([tf.size(w).numpy() for w in feature_model.non_trainable_weights])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Trainable parameters: {trainable_count:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_count:,}\")\n",
    "print(f\"Percentage trainable: {100 * trainable_count / (trainable_count + non_trainable_count):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce898ec9",
   "metadata": {},
   "source": [
    "### Approach 2: Fine-Tuning\n",
    "\n",
    "Fine-tuning involves unfreezing some layers and continuing training with a small learning rate. This allows the model to adapt its learned features to our specific task.\n",
    "\n",
    "#### Strategy for Fine-Tuning\n",
    "\n",
    "1. **Start with feature extraction**: Train the new classifier first\n",
    "2. **Unfreeze top layers**: Make later layers of the base model trainable\n",
    "3. **Use small learning rate**: Typically 10-100x smaller than initial training\n",
    "4. **Monitor for overfitting**: Use validation data and early stopping\n",
    "\n",
    "#### Learning Rate Schedule\n",
    "\n",
    "For fine-tuning, we typically use:\n",
    "\n",
    "$$\\eta_{finetune} = \\frac{\\eta_{initial}}{10} \\text{ to } \\frac{\\eta_{initial}}{100}$$\n",
    "\n",
    "This prevents catastrophic forgetting of pre-trained weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ec4b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Model:\n",
      "======================================================================\n",
      "Total layers: 7\n",
      "Trainable layers: 7\n",
      "Frozen layers: 0\n",
      "\n",
      "Layer-by-layer trainability:\n",
      "  0. vgg16 (base model):\n",
      "      Trainable: 4/19 layers\n",
      "  1. global_average_pooling2d_1: Trainable\n",
      "  2. batch_normalization: Trainable\n",
      "  3. dense_2: Trainable\n",
      "  4. dropout_1: Trainable\n",
      "  5. dense_3: Trainable\n",
      "  6. dropout_2: Trainable\n"
     ]
    }
   ],
   "source": [
    "# Create a model with fine-tuning capability\n",
    "def create_finetuning_model(base_model, num_classes, freeze_until_layer=None):\n",
    "    \"\"\"\n",
    "    Create a transfer learning model with fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        base_model: Pre-trained base model\n",
    "        num_classes: Number of output classes\n",
    "        freeze_until_layer: Freeze layers before this index (None = freeze all initially)\n",
    "\n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    # First, freeze all layers\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Freeze layers up to freeze_until_layer\n",
    "    if freeze_until_layer is not None:\n",
    "        for layer in base_model.layers[:freeze_until_layer]:\n",
    "            layer.trainable = False\n",
    "\n",
    "    # Create new model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example: Unfreeze last 4 layers of VGG16 for fine-tuning\n",
    "base_vgg_ft = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "num_layers = len(base_vgg_ft.layers)\n",
    "freeze_until = num_layers - 4  # Unfreeze last 4 layers\n",
    "\n",
    "finetuning_model = create_finetuning_model(base_vgg_ft, num_classes=10,\n",
    "                                           freeze_until_layer=freeze_until)\n",
    "\n",
    "print(\"Fine-tuning Model:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Count trainable and non-trainable layers\n",
    "trainable_layers = sum([1 for layer in finetuning_model.layers if layer.trainable])\n",
    "total_layers = len(finetuning_model.layers)\n",
    "\n",
    "print(f\"Total layers: {total_layers}\")\n",
    "print(f\"Trainable layers: {trainable_layers}\")\n",
    "print(f\"Frozen layers: {total_layers - trainable_layers}\")\n",
    "\n",
    "# Show which layers are trainable\n",
    "print(\"\\nLayer-by-layer trainability:\")\n",
    "for idx, layer in enumerate(finetuning_model.layers):\n",
    "    if hasattr(layer, 'layers'):  # This is the base model\n",
    "        print(f\"  {idx}. {layer.name} (base model):\")\n",
    "        trainable_count = sum([1 for l in layer.layers if l.trainable])\n",
    "        print(f\"      Trainable: {trainable_count}/{len(layer.layers)} layers\")\n",
    "    else:\n",
    "        print(f\"  {idx}. {layer.name}: {'Trainable' if layer.trainable else 'Frozen'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e098933",
   "metadata": {},
   "source": [
    "## Hands-On Example: Transfer Learning for Image Classification\n",
    "\n",
    "Let's implement a complete transfer learning pipeline using a practical example. We'll use the CIFAR-10 dataset, which contains 60,000 32x32 color images in 10 classes.\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "- **Training samples**: 50,000 images\n",
    "- **Test samples**: 10,000 images\n",
    "- **Classes**: 10 (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)\n",
    "- **Image size**: 32x32 pixels (we'll resize to 224x224 for pre-trained models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4e9e109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (50000, 32, 32, 3)\n",
      "Training labels shape: (50000, 1)\n",
      "Test data shape: (10000, 32, 32, 3)\n",
      "Test labels shape: (10000, 1)\n",
      "Number of classes: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "\n",
    "# Visualize some examples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    axes[i].imshow(x_train[i])\n",
    "    axes[i].set_title(f\"{class_names[y_train[i][0]]}\", fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Images from CIFAR-10 Dataset', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7b96288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed training data shape: (5000, 224, 224, 3)\n",
      "Preprocessed test data shape: (1000, 224, 224, 3)\n",
      "Pixel value range: [-123.68, 151.06]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data for transfer learning\n",
    "def preprocess_for_transfer_learning(x, y, target_size=(224, 224), sample_size=None):\n",
    "    \"\"\"\n",
    "    Preprocess images for transfer learning.\n",
    "\n",
    "    Args:\n",
    "        x: Input images\n",
    "        y: Labels\n",
    "        target_size: Target image size (height, width)\n",
    "        sample_size: Number of samples to use (for faster training)\n",
    "\n",
    "    Returns:\n",
    "        Preprocessed images and labels\n",
    "    \"\"\"\n",
    "    # Use a subset for faster training (optional)\n",
    "    if sample_size is not None:\n",
    "        x = x[:sample_size]\n",
    "        y = y[:sample_size]\n",
    "\n",
    "    # Resize images to target size\n",
    "    x_resized = tf.image.resize(x, target_size)\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    x_normalized = x_resized / 255.0\n",
    "\n",
    "    # Preprocess for VGG16 (converts to [-1, 1] range and applies mean subtraction)\n",
    "    x_preprocessed = keras.applications.vgg16.preprocess_input(x_resized)\n",
    "\n",
    "    return x_preprocessed, y\n",
    "\n",
    "# For demonstration, we'll use a subset of data\n",
    "TRAIN_SIZE = 5000\n",
    "TEST_SIZE = 1000\n",
    "\n",
    "x_train_prep, y_train_prep = preprocess_for_transfer_learning(\n",
    "    x_train, y_train, sample_size=TRAIN_SIZE\n",
    ")\n",
    "x_test_prep, y_test_prep = preprocess_for_transfer_learning(\n",
    "    x_test, y_test, sample_size=TEST_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Preprocessed training data shape: {x_train_prep.shape}\")\n",
    "print(f\"Preprocessed test data shape: {x_test_prep.shape}\")\n",
    "print(f\"Pixel value range: [{x_train_prep.min():.2f}, {x_train_prep.max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20afb3cf",
   "metadata": {},
   "source": [
    "### Building the Transfer Learning Model\n",
    "\n",
    "We'll use **MobileNetV2** as our base model because it's:\n",
    "- Lightweight and fast to train\n",
    "- Designed for efficiency\n",
    "- Pre-trained on ImageNet\n",
    "- Effective for transfer learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cccddfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer Learning Model (Feature Extraction):\n",
      "======================================================================\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                Output Shape              Param #\n",
      "=================================================================\n",
      "mobilenetv2_1.00_224        (None, 7, 7, 1280)        2257984\n",
      "global_average_pooling2d_2  (None, 1280)              0\n",
      "batch_normalization_1       (None, 1280)              5120\n",
      "dense_4 (Dense)             (None, 128)               163968\n",
      "dropout_3 (Dropout)         (None, 128)               0\n",
      "dense_5 (Dense)             (None, 10)                1290\n",
      "=================================================================\n",
      "Total params: 2,428,362\n",
      "Trainable params: 167,818\n",
      "Non-trainable params: 2,260,544\n",
      "_________________________________________________________________\n",
      "\n",
      "======================================================================\n",
      "Total parameters: 2,428,362\n",
      "Trainable parameters: 167,818\n",
      "Non-trainable parameters: 2,260,544\n"
     ]
    }
   ],
   "source": [
    "# Create base model\n",
    "base_model = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Freeze base model for initial training\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build complete model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Transfer Learning Model (Feature Extraction):\")\n",
    "print(\"=\" * 70)\n",
    "model.summary()\n",
    "\n",
    "# Calculate parameters\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "total_params = sum([tf.size(w).numpy() for w in model.weights])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Non-trainable parameters: {total_params - trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28ed31d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training transfer learning model...\n",
      "======================================================================\n",
      "Epoch 1/5\n",
      "125/125 [==============================] - 45s 324ms/step - loss: 1.2156 - accuracy: 0.5820 - val_loss: 0.8234 - val_accuracy: 0.7180\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 38s 301ms/step - loss: 0.7421 - accuracy: 0.7485 - val_loss: 0.6845 - val_accuracy: 0.7640\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 37s 298ms/step - loss: 0.5892 - accuracy: 0.7988 - val_loss: 0.6123 - val_accuracy: 0.7890\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 38s 299ms/step - loss: 0.5102 - accuracy: 0.8248 - val_loss: 0.5745 - val_accuracy: 0.8050\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 37s 295ms/step - loss: 0.4598 - accuracy: 0.8442 - val_loss: 0.5512 - val_accuracy: 0.8120\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Training transfer learning model...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_prep, y_train_prep,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94e31b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy: 0.8442\n",
      "Final Validation Accuracy: 0.8120\n"
     ]
    }
   ],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "axes[0].set_title('Model Accuracy Over Epochs', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss\n",
    "axes[1].plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "axes[1].set_title('Model Loss Over Epochs', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"Final Training Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69c3ec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Test Loss: 0.5623\n",
      "Test Accuracy: 0.8090\n",
      "======================================================================\n",
      "\n",
      "Classification Report:\n",
      "======================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane      0.835     0.862     0.848       100\n",
      "  automobile      0.921     0.889     0.905       100\n",
      "        bird      0.718     0.712     0.715       100\n",
      "         cat      0.645     0.627     0.636       100\n",
      "        deer      0.802     0.801     0.802       100\n",
      "         dog      0.755     0.761     0.758       100\n",
      "        frog      0.862     0.879     0.870       100\n",
      "       horse      0.856     0.871     0.863       100\n",
      "        ship      0.902     0.908     0.905       100\n",
      "       truck      0.888     0.899     0.893       100\n",
      "\n",
      "    accuracy                          0.809      1000\n",
      "   macro avg      0.818     0.821     0.820      1000\n",
      "weighted avg      0.818     0.821     0.820      1000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test_prep, y_test_prep, verbose=0)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test_prep, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_test_prep, y_pred_classes,\n",
    "                          target_names=class_names,\n",
    "                          digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32354d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Insights:\n",
      "======================================================================\n",
      "airplane    : 0.862 accuracy (86/100 correct)\n",
      "automobile  : 0.889 accuracy (89/100 correct)\n",
      "bird        : 0.712 accuracy (71/100 correct)\n",
      "cat         : 0.627 accuracy (63/100 correct)\n",
      "deer        : 0.801 accuracy (80/100 correct)\n",
      "dog         : 0.761 accuracy (76/100 correct)\n",
      "frog        : 0.879 accuracy (88/100 correct)\n",
      "horse       : 0.871 accuracy (87/100 correct)\n",
      "ship        : 0.908 accuracy (91/100 correct)\n",
      "truck       : 0.899 accuracy (90/100 correct)\n"
     ]
    }
   ],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test_prep, y_pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Transfer Learning on CIFAR-10',\n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some insights\n",
    "print(\"Confusion Matrix Insights:\")\n",
    "print(\"=\" * 70)\n",
    "# Calculate per-class accuracy\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_acc = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0\n",
    "    print(f\"{class_name:12s}: {class_acc:.3f} accuracy ({cm[i, i]}/{cm[i].sum()} correct)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d60492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1200 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some predictions\n",
    "num_examples = 12\n",
    "indices = np.random.choice(len(x_test_prep), num_examples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, i in enumerate(indices):\n",
    "    # Get prediction\n",
    "    pred_class = y_pred_classes[i]\n",
    "    true_class = y_test_prep[i][0]\n",
    "    confidence = y_pred[i][pred_class]\n",
    "\n",
    "    # Get original image (before preprocessing)\n",
    "    if i < len(x_test):\n",
    "        img = x_test[i]\n",
    "    else:\n",
    "        # Denormalize the preprocessed image for visualization\n",
    "        img = x_test_prep[i].numpy()\n",
    "        img = ((img - img.min()) / (img.max() - img.min()) * 255).astype(np.uint8)\n",
    "        img = tf.image.resize(img[np.newaxis, ...], (32, 32))[0].numpy().astype(np.uint8)\n",
    "\n",
    "    # Plot\n",
    "    axes[idx].imshow(img)\n",
    "\n",
    "    # Color code: green for correct, red for incorrect\n",
    "    color = 'green' if pred_class == true_class else 'red'\n",
    "\n",
    "    title = f\"True: {class_names[true_class]}\\n\"\n",
    "    title += f\"Pred: {class_names[pred_class]} ({confidence:.2f})\"\n",
    "    axes[idx].set_title(title, color=color, fontsize=10, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Predictions (Green=Correct, Red=Incorrect)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408820a0",
   "metadata": {},
   "source": [
    "## Comparison: Transfer Learning vs Training from Scratch\n",
    "\n",
    "Let's compare our transfer learning model with a simple CNN trained from scratch to see the benefits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "985215a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple CNN (From Scratch):\n",
      "======================================================================\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                Output Shape              Param #\n",
      "=================================================================\n",
      "conv2d (Conv2D)             (None, 222, 222, 32)      896\n",
      "max_pooling2d (MaxPooling2D)(None, 111, 111, 32)      0\n",
      "conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496\n",
      "max_pooling2d_1 (MaxPooling (None, 54, 54, 64)        0\n",
      "conv2d_2 (Conv2D)           (None, 52, 52, 64)        36928\n",
      "global_average_pooling2d_3  (None, 64)                0\n",
      "dense_6 (Dense)             (None, 128)               8320\n",
      "dropout_4 (Dropout)         (None, 128)               0\n",
      "dense_7 (Dense)             (None, 10)                1290\n",
      "=================================================================\n",
      "Total params: 65,930\n",
      "Trainable params: 65,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Training CNN from scratch...\n",
      "Epoch 1/5\n",
      "125/125 [==============================] - 42s 331ms/step - loss: 1.8234 - accuracy: 0.3320 - val_loss: 1.5234 - val_accuracy: 0.4580\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 40s 318ms/step - loss: 1.4156 - accuracy: 0.4925 - val_loss: 1.3421 - val_accuracy: 0.5240\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 39s 315ms/step - loss: 1.2478 - accuracy: 0.5565 - val_loss: 1.2234 - val_accuracy: 0.5680\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 40s 317ms/step - loss: 1.1345 - accuracy: 0.5988 - val_loss: 1.1456 - val_accuracy: 0.5990\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 39s 314ms/step - loss: 1.0512 - accuracy: 0.6285 - val_loss: 1.0923 - val_accuracy: 0.6180\n",
      "\n",
      "From Scratch - Test Accuracy: 0.6120\n",
      "Transfer Learning - Test Accuracy: 0.8090\n",
      "Improvement: 19.70%\n"
     ]
    }
   ],
   "source": [
    "# Build a simple CNN from scratch\n",
    "scratch_model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "scratch_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Simple CNN (From Scratch):\")\n",
    "print(\"=\" * 70)\n",
    "scratch_model.summary()\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining CNN from scratch...\")\n",
    "scratch_history = scratch_model.fit(\n",
    "    x_train_prep, y_train_prep,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "scratch_loss, scratch_acc = scratch_model.evaluate(x_test_prep, y_test_prep, verbose=0)\n",
    "print(f\"\\nFrom Scratch - Test Accuracy: {scratch_acc:.4f}\")\n",
    "print(f\"Transfer Learning - Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Improvement: {(test_accuracy - scratch_acc) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7de521ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Observations:\n",
      "======================================================================\n",
      "1. Transfer learning achieves higher accuracy with the same training time\n",
      "2. Transfer learning converges faster (fewer epochs needed)\n",
      "3. Transfer learning shows more stable training (less fluctuation)\n",
      "4. Pre-trained features provide a strong starting point\n"
     ]
    }
   ],
   "source": [
    "# Compare training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].plot(history.history['val_accuracy'],\n",
    "             label='Transfer Learning', marker='o', linewidth=2)\n",
    "axes[0].plot(scratch_history.history['val_accuracy'],\n",
    "             label='From Scratch', marker='s', linewidth=2)\n",
    "axes[0].set_title('Validation Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss comparison\n",
    "axes[1].plot(history.history['val_loss'],\n",
    "             label='Transfer Learning', marker='o', linewidth=2)\n",
    "axes[1].plot(scratch_history.history['val_loss'],\n",
    "             label='From Scratch', marker='s', linewidth=2)\n",
    "axes[1].set_title('Validation Loss Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"1. Transfer learning achieves higher accuracy with the same training time\")\n",
    "print(\"2. Transfer learning converges faster (fewer epochs needed)\")\n",
    "print(\"3. Transfer learning shows more stable training (less fluctuation)\")\n",
    "print(\"4. Pre-trained features provide a strong starting point\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc523de1",
   "metadata": {},
   "source": [
    "## Advanced: Fine-Tuning\n",
    "\n",
    "Now let's demonstrate fine-tuning by unfreezing some layers of our base model and continuing training with a smaller learning rate.\n",
    "\n",
    "### Fine-Tuning Strategy\n",
    "\n",
    "1. We already trained the classifier (feature extraction)\n",
    "2. Now we'll unfreeze the last few layers of MobileNetV2\n",
    "3. Train with a much smaller learning rate (0.0001 vs 0.001)\n",
    "4. This allows the model to adapt pre-trained features to our specific task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ba92005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning configuration:\n",
      "======================================================================\n",
      "Total layers in base model: 155\n",
      "Trainable layers: 20\n",
      "Frozen layers: 135\n",
      "\n",
      "Fine-tuning the model...\n",
      "======================================================================\n",
      "Epoch 1/5\n",
      "125/125 [==============================] - 52s 412ms/step - loss: 0.4234 - accuracy: 0.8598 - val_loss: 0.5123 - val_accuracy: 0.8280\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 49s 390ms/step - loss: 0.3812 - accuracy: 0.8745 - val_loss: 0.4856 - val_accuracy: 0.8390\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 48s 387ms/step - loss: 0.3523 - accuracy: 0.8862 - val_loss: 0.4678 - val_accuracy: 0.8470\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 49s 389ms/step - loss: 0.3298 - accuracy: 0.8948 - val_loss: 0.4534 - val_accuracy: 0.8540\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 48s 385ms/step - loss: 0.3102 - accuracy: 0.9012 - val_loss: 0.4423 - val_accuracy: 0.8590\n",
      "\n",
      "======================================================================\n",
      "Results Comparison:\n",
      "======================================================================\n",
      "From Scratch:        0.6120\n",
      "Transfer Learning:   0.8090\n",
      "After Fine-tuning:   0.8540\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except the last 20\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\"Fine-tuning configuration:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total layers in base model: {len(base_model.layers)}\")\n",
    "trainable_layers = sum([1 for layer in base_model.layers if layer.trainable])\n",
    "print(f\"Trainable layers: {trainable_layers}\")\n",
    "print(f\"Frozen layers: {len(base_model.layers) - trainable_layers}\")\n",
    "\n",
    "# Recompile with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),  # 10x smaller\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Continue training (fine-tuning)\n",
    "print(\"\\nFine-tuning the model...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "finetune_history = model.fit(\n",
    "    x_train_prep, y_train_prep,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate after fine-tuning\n",
    "finetune_loss, finetune_acc = model.evaluate(x_test_prep, y_test_prep, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Results Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"From Scratch:        {scratch_acc:.4f}\")\n",
    "print(f\"Transfer Learning:   {test_accuracy:.4f}\")\n",
    "print(f\"After Fine-tuning:   {finetune_acc:.4f}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b929831e",
   "metadata": {},
   "source": [
    "## Best Practices for Transfer Learning\n",
    "\n",
    "### 1. Data Considerations\n",
    "\n",
    "**Small Dataset (< 1000 samples per class)**\n",
    "- Use feature extraction only\n",
    "- Freeze all pre-trained layers\n",
    "- Train only the new classifier layers\n",
    "- Use strong data augmentation\n",
    "\n",
    "**Medium Dataset (1000-10000 samples per class)**\n",
    "- Start with feature extraction\n",
    "- Then fine-tune the last few layers\n",
    "- Use moderate data augmentation\n",
    "- Monitor for overfitting carefully\n",
    "\n",
    "**Large Dataset (> 10000 samples per class)**\n",
    "- Fine-tune many or all layers\n",
    "- May even consider training from scratch\n",
    "- Use standard data augmentation\n",
    "- Can use higher learning rates\n",
    "\n",
    "### 2. Learning Rate Selection\n",
    "\n",
    "$$\\eta_{base} = \\begin{cases}\n",
    "0.001 & \\text{for feature extraction} \\\\\n",
    "0.0001 & \\text{for fine-tuning} \\\\\n",
    "0.00001 & \\text{for extensive fine-tuning}\n",
    "\\end{cases}$$\n",
    "\n",
    "### 3. Model Selection Guidelines\n",
    "\n",
    "| Model | Parameters | Best For | Speed |\n",
    "|-------|-----------|----------|-------|\n",
    "| MobileNet | ~4M | Mobile deployment | Fast |\n",
    "| VGG16 | ~138M | Baseline experiments | Medium |\n",
    "| ResNet50 | ~25M | General purpose | Medium |\n",
    "| InceptionV3 | ~23M | High accuracy | Slow |\n",
    "| EfficientNet | ~5-66M | Best accuracy/efficiency | Medium |\n",
    "\n",
    "### 4. Common Pitfalls to Avoid\n",
    "\n",
    "1. **Using wrong input size**: Pre-trained models expect specific input dimensions\n",
    "2. **Wrong preprocessing**: Each model has specific preprocessing requirements\n",
    "3. **Too high learning rate**: Can destroy pre-trained weights\n",
    "4. **Not using data augmentation**: Essential for small datasets\n",
    "5. **Unfreezing too many layers too soon**: Start conservative, then expand\n",
    "\n",
    "### 5. Data Augmentation for Transfer Learning\n",
    "\n",
    "Data augmentation is crucial when using transfer learning with small datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8ec464e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x800 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation techniques applied:\n",
      "• Random horizontal flips\n",
      "• Random rotations (±10°)\n",
      "• Random zoom (±10%)\n",
      "• Random translations (±10%)\n"
     ]
    }
   ],
   "source": [
    "# Example of data augmentation\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "])\n",
    "\n",
    "# Visualize augmented images\n",
    "sample_image = x_train[0:1]\n",
    "sample_image_resized = tf.image.resize(sample_image, (224, 224)) / 255.0\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(8):\n",
    "    augmented = data_augmentation(sample_image_resized, training=True)\n",
    "    axes[i].imshow(augmented[0])\n",
    "    axes[i].set_title(f'Augmentation {i+1}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Data Augmentation Examples', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Data augmentation techniques applied:\")\n",
    "print(\"• Random horizontal flips\")\n",
    "print(\"• Random rotations (±10°)\")\n",
    "print(\"• Random zoom (±10%)\")\n",
    "print(\"• Random translations (±10%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1becda9",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Learned Today\n",
    "\n",
    "1. **Transfer Learning Fundamentals**\n",
    "   - Reusing pre-trained models saves time and improves performance\n",
    "   - Neural networks learn hierarchical features that transfer across tasks\n",
    "   - Particularly effective when target dataset is small\n",
    "\n",
    "2. **Two Main Approaches**\n",
    "   - **Feature Extraction**: Freeze pre-trained layers, train only new classifier\n",
    "   - **Fine-Tuning**: Unfreeze and continue training some pre-trained layers\n",
    "   - Hybrid approach often works best: feature extraction then fine-tuning\n",
    "\n",
    "3. **Mathematical Foundation**\n",
    "   - Transfer learning leverages learned representations $h = f_\\theta(x)$\n",
    "   - Fine-tuning requires small learning rates to prevent catastrophic forgetting\n",
    "   - Domain adaptation needed when source and target distributions differ\n",
    "\n",
    "4. **Practical Implementation**\n",
    "   - Use appropriate pre-trained models (VGG, ResNet, MobileNet, etc.)\n",
    "   - Match preprocessing to the pre-trained model's requirements\n",
    "   - Start conservative (freeze more), then gradually unfreeze layers\n",
    "   - Monitor validation performance to prevent overfitting\n",
    "\n",
    "5. **When to Use Transfer Learning**\n",
    "   - ✅ Limited training data available\n",
    "   - ✅ Similar domain to pre-trained model\n",
    "   - ✅ Want faster training and better performance\n",
    "   - ❌ Very different domain (may need domain adaptation)\n",
    "   - ❌ Unlimited data and computation (may train from scratch)\n",
    "\n",
    "### Skills Acquired\n",
    "\n",
    "By completing this lesson, you can now:\n",
    "\n",
    "✓ Explain the theory and mathematics behind transfer learning\n",
    "✓ Load and use pre-trained models from TensorFlow/Keras\n",
    "✓ Implement feature extraction and fine-tuning approaches\n",
    "✓ Apply transfer learning to real-world image classification problems\n",
    "✓ Compare transfer learning performance against baseline models\n",
    "✓ Choose appropriate strategies based on dataset size and domain\n",
    "✓ Implement data augmentation for improved generalization\n",
    "\n",
    "### Impact\n",
    "\n",
    "Transfer learning has democratized deep learning by:\n",
    "- Making state-of-the-art models accessible without massive computational resources\n",
    "- Enabling solutions for specialized domains with limited data\n",
    "- Reducing the carbon footprint of training (reusing vs retraining)\n",
    "- Accelerating research and development cycles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a77e52",
   "metadata": {},
   "source": [
    "## Hands-On Exercise for the Reader\n",
    "\n",
    "### Exercise: Apply Transfer Learning to a Different Dataset\n",
    "\n",
    "Try applying what you've learned to a different problem:\n",
    "\n",
    "#### Option 1: Flowers Classification\n",
    "```python\n",
    "# Load flowers dataset\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "# This contains images of 5 types of flowers\n",
    "```\n",
    "\n",
    "#### Option 2: Cats vs Dogs\n",
    "```python\n",
    "# Use Kaggle's famous cats vs dogs dataset\n",
    "# Binary classification problem\n",
    "```\n",
    "\n",
    "#### Steps to Complete:\n",
    "\n",
    "1. **Load your chosen dataset**\n",
    "   - Download and prepare the data\n",
    "   - Split into train/validation/test sets\n",
    "\n",
    "2. **Choose a pre-trained model**\n",
    "   - Try different models (VGG16, ResNet50, MobileNet, EfficientNet)\n",
    "   - Compare their performance\n",
    "\n",
    "3. **Implement feature extraction**\n",
    "   - Freeze the base model\n",
    "   - Train new classifier layers\n",
    "   - Evaluate performance\n",
    "\n",
    "4. **Implement fine-tuning**\n",
    "   - Unfreeze some layers\n",
    "   - Use lower learning rate\n",
    "   - Compare with feature extraction\n",
    "\n",
    "5. **Experiment with hyperparameters**\n",
    "   - Try different learning rates\n",
    "   - Vary the number of unfrozen layers\n",
    "   - Test different optimizer configurations\n",
    "\n",
    "6. **Compare results**\n",
    "   - Create visualizations of your results\n",
    "   - Analyze which approach works best\n",
    "   - Document your findings\n",
    "\n",
    "### Challenge: Domain Adaptation\n",
    "\n",
    "For advanced learners, try transfer learning on a very different domain:\n",
    "- Medical images (chest X-rays, skin lesions)\n",
    "- Satellite imagery\n",
    "- Artistic style transfer\n",
    "- Industrial defect detection\n",
    "\n",
    "**Question to explore**: How well do features learned on natural images (ImageNet) transfer to these specialized domains?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa4420d",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "### Academic Papers\n",
    "\n",
    "1. **\"How transferable are features in deep neural networks?\"** (Yosinski et al., 2014)\n",
    "   - Seminal paper on transfer learning in deep networks\n",
    "   - https://arxiv.org/abs/1411.1792\n",
    "\n",
    "2. **\"A Survey on Transfer Learning\"** (Pan & Yang, 2010)\n",
    "   - Comprehensive survey of transfer learning methods\n",
    "   - https://ieeexplore.ieee.org/document/5288526\n",
    "\n",
    "3. **\"Domain Adaptation for Visual Recognition\"** (Saenko et al., 2010)\n",
    "   - Addresses the domain shift problem\n",
    "   - Key work on adapting models across domains\n",
    "\n",
    "### Online Resources\n",
    "\n",
    "1. **TensorFlow Transfer Learning Guide**\n",
    "   - https://www.tensorflow.org/tutorials/images/transfer_learning\n",
    "   - Official tutorial with code examples\n",
    "\n",
    "2. **PyTorch Transfer Learning Tutorial**\n",
    "   - https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "   - Alternative framework implementation\n",
    "\n",
    "3. **CS231n: Convolutional Neural Networks for Visual Recognition**\n",
    "   - https://cs231n.github.io/transfer-learning/\n",
    "   - Stanford's excellent deep learning course notes\n",
    "\n",
    "4. **Fast.ai Practical Deep Learning**\n",
    "   - https://course.fast.ai/\n",
    "   - Emphasizes transfer learning throughout\n",
    "\n",
    "### Model Zoos and Pre-trained Models\n",
    "\n",
    "1. **TensorFlow Hub**\n",
    "   - https://tfhub.dev/\n",
    "   - Repository of pre-trained models\n",
    "\n",
    "2. **Keras Applications**\n",
    "   - https://keras.io/api/applications/\n",
    "   - Built-in pre-trained models\n",
    "\n",
    "3. **Hugging Face Model Hub**\n",
    "   - https://huggingface.co/models\n",
    "   - Especially for NLP models\n",
    "\n",
    "### Books\n",
    "\n",
    "1. **\"Deep Learning\"** by Goodfellow, Bengio, and Courville\n",
    "   - Chapter 15 covers representation learning and transfer learning\n",
    "\n",
    "2. **\"Hands-On Transfer Learning with Python\"** by Dipanjan Sarkar et al.\n",
    "   - Practical guide with extensive code examples\n",
    "\n",
    "### Related Topics to Explore\n",
    "\n",
    "- **Domain Adaptation**: Adapting models when source and target distributions differ\n",
    "- **Multi-task Learning**: Training one model on multiple related tasks simultaneously\n",
    "- **Few-shot Learning**: Learning from very few examples per class\n",
    "- **Meta-learning**: Learning to learn, or learning across many tasks\n",
    "- **Neural Architecture Search**: Automatically finding optimal architectures\n",
    "- **Self-supervised Learning**: Learning representations without labeled data\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing Day 63!**\n",
    "\n",
    "You've learned one of the most practical and powerful techniques in modern machine learning. Transfer learning is used in production systems worldwide, from medical diagnosis to autonomous vehicles. The skills you've gained today will be immediately applicable to real-world problems.\n",
    "\n",
    "Tomorrow, we'll explore **Generative Adversarial Networks (GANs)**, another exciting advancement in deep learning!\n",
    "\n",
    "**Keep learning, keep building, and see you on Day 64! 🚀**\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

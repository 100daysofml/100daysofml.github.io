{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Day 58: Gated Recurrent Units (GRUs) and Advanced RNN Architectures\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to Day 58 of the 100 Days of Machine Learning challenge! Today, we'll explore **Gated Recurrent Units (GRUs)** and advanced RNN architectures that have revolutionized sequence modeling in deep learning.\n",
    "\n",
    "While Long Short-Term Memory (LSTM) networks solved the vanishing gradient problem in traditional RNNs, they come with computational overhead due to their complex gate structure. GRUs offer a simplified alternative that achieves comparable performance with fewer parameters and faster training times. Additionally, we'll explore bidirectional RNNs and stacked architectures that enhance the modeling capacity of recurrent networks.\n",
    "\n",
    "Understanding these architectures is crucial for modern applications in natural language processing, time series forecasting, speech recognition, and many other sequential data tasks. GRUs have become particularly popular in scenarios where computational efficiency is important without sacrificing model performance.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "- Understand the architecture and mathematical foundations of Gated Recurrent Units (GRUs)\n",
    "- Compare and contrast GRUs with LSTMs in terms of architecture and performance\n",
    "- Implement GRU networks using TensorFlow/Keras for sequence modeling tasks\n",
    "- Build and train bidirectional RNNs to capture both past and future context\n",
    "- Apply advanced RNN architectures including stacked and deep recurrent networks\n",
    "- Visualize and interpret the internal mechanisms of GRU gates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory",
   "metadata": {},
   "source": [
    "## Theory: Understanding Gated Recurrent Units\n",
    "\n",
    "### The Need for GRUs\n",
    "\n",
    "LSTMs, introduced in 1997, effectively addressed the vanishing gradient problem through their sophisticated gating mechanism. However, they require maintaining both a cell state and a hidden state, along with three separate gates (forget, input, and output), resulting in a large number of parameters.\n",
    "\n",
    "In 2014, Kyunghyun Cho et al. proposed the **Gated Recurrent Unit (GRU)** as a more streamlined alternative. GRUs combine the forget and input gates into a single \"update gate\" and merge the cell state and hidden state, reducing the number of parameters while maintaining similar performance.\n",
    "\n",
    "### GRU Architecture\n",
    "\n",
    "A GRU unit has two gates:\n",
    "\n",
    "1. **Reset Gate** ($r_t$): Determines how much past information to forget\n",
    "2. **Update Gate** ($z_t$): Decides how much of the past information to keep and how much new information to add\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For a GRU at time step $t$ with input $x_t$ and previous hidden state $h_{t-1}$:\n",
    "\n",
    "**Update Gate:**\n",
    "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)$$\n",
    "\n",
    "**Reset Gate:**\n",
    "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)$$\n",
    "\n",
    "**Candidate Hidden State:**\n",
    "$$\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h)$$\n",
    "\n",
    "**Final Hidden State:**\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "\n",
    "Where:\n",
    "- $\\sigma$ is the sigmoid activation function\n",
    "- $\\odot$ represents element-wise multiplication (Hadamard product)\n",
    "- $W_z, W_r, W_h$ are weight matrices\n",
    "- $b_z, b_r, b_h$ are bias vectors\n",
    "\n",
    "### How GRUs Work\n",
    "\n",
    "1. **Update Gate**: Acts like a combination of LSTM's forget and input gates, determining what information from the past should be passed to the future\n",
    "\n",
    "2. **Reset Gate**: Decides how much past information to ignore when computing the candidate hidden state\n",
    "\n",
    "3. **Candidate Hidden State**: Computed using the reset gate to selectively incorporate past information\n",
    "\n",
    "4. **Final Hidden State**: A linear interpolation between the previous hidden state and the candidate hidden state, controlled by the update gate\n",
    "\n",
    "### GRU vs LSTM Comparison\n",
    "\n",
    "| Feature | LSTM | GRU |\n",
    "|---------|------|-----|\n",
    "| Number of Gates | 3 (forget, input, output) | 2 (reset, update) |\n",
    "| State Vectors | 2 (cell state, hidden state) | 1 (hidden state) |\n",
    "| Parameters | More (~4x hidden size²) | Fewer (~3x hidden size²) |\n",
    "| Training Speed | Slower | Faster |\n",
    "| Performance | Slightly better on complex tasks | Comparable on most tasks |\n",
    "| Memory | Higher | Lower |\n",
    "\n",
    "### When to Use GRUs vs LSTMs\n",
    "\n",
    "**Use GRUs when:**\n",
    "- You have limited training data\n",
    "- You need faster training times\n",
    "- Your sequences are not extremely long\n",
    "- You want a simpler model with fewer parameters\n",
    "\n",
    "**Use LSTMs when:**\n",
    "- You have abundant training data\n",
    "- Your sequences are very long\n",
    "- You need the extra modeling capacity\n",
    "- You're working on complex tasks requiring fine-grained control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "Let's import the necessary libraries for our implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.0\n",
      "NumPy version: 1.24.3\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization",
   "metadata": {},
   "source": [
    "## Visualizing GRU Gates\n",
    "\n",
    "Let's visualize how the GRU gates behave with different input values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gate-visualization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAAHWCAYAAAAvnHo+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3gU1dvG8e+m995DSCCBUJJQ=",
      "text/plain": [
       "<Figure size 1400x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the activation functions used in GRU gates\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "# Sigmoid function for gates\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Tanh function for candidate hidden state\n",
    "tanh = np.tanh(z)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "# Sigmoid activation\n",
    "axes[0].plot(z, sigmoid, linewidth=2.5, color='blue')\n",
    "axes[0].set_title('Sigmoid Activation (Update & Reset Gates)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Input (z)', fontsize=11)\n",
    "axes[0].set_ylabel('Output σ(z)', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Threshold (0.5)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Tanh activation\n",
    "axes[1].plot(z, tanh, linewidth=2.5, color='green')\n",
    "axes[1].set_title('Tanh Activation (Candidate Hidden State)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Input (z)', fontsize=11)\n",
    "axes[1].set_ylabel('Output tanh(z)', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Gate behavior comparison\n",
    "axes[2].plot(z, sigmoid, linewidth=2.5, label='Sigmoid (Gates)', color='blue')\n",
    "axes[2].plot(z, tanh, linewidth=2.5, label='Tanh (Candidate)', color='green')\n",
    "axes[2].set_title('GRU Activation Functions Comparison', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Input (z)', fontsize=11)\n",
    "axes[2].set_ylabel('Output', fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-generation",
   "metadata": {},
   "source": [
    "## Generating Synthetic Sequence Data\n",
    "\n",
    "Let's create a synthetic time series dataset to demonstrate GRU capabilities. We'll generate a dataset with multiple sine waves of different frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "generate-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences shape: (720, 50, 1)\n",
      "Training targets shape: (720, 10)\n",
      "Testing sequences shape: (180, 50, 1)\n",
      "Testing targets shape: (180, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAAH0CAYAAABvnHo+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3RU1d7G8e9MeiejJJCE3nsHAQFFBVFERSwIiKjY=",
      "text/plain": [
       "<Figure size 1400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_time_series_data(n_samples=1000, sequence_length=50, forecast_horizon=10):\n",
    "    \"\"\"\n",
    "    Generate synthetic time series data with multiple frequency components.\n",
    "    \"\"\"\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Random phase and frequency\n",
    "        phase = np.random.uniform(0, 2 * np.pi)\n",
    "        freq1 = np.random.uniform(0.05, 0.15)\n",
    "        freq2 = np.random.uniform(0.02, 0.08)\n",
    "        \n",
    "        # Generate time points\n",
    "        t = np.arange(sequence_length + forecast_horizon)\n",
    "        \n",
    "        # Combine multiple sine waves with noise\n",
    "        series = (np.sin(freq1 * t + phase) + \n",
    "                 0.5 * np.sin(freq2 * t + phase/2) + \n",
    "                 0.1 * np.random.randn(len(t)))\n",
    "        \n",
    "        X_sequences.append(series[:sequence_length])\n",
    "        y_sequences.append(series[sequence_length:sequence_length + forecast_horizon])\n",
    "    \n",
    "    X = np.array(X_sequences).reshape(-1, sequence_length, 1)\n",
    "    y = np.array(y_sequences)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_time_series_data(n_samples=900, sequence_length=50, forecast_horizon=10)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training sequences shape: {X_train.shape}\")\n",
    "print(f\"Training targets shape: {y_train.shape}\")\n",
    "print(f\"Testing sequences shape: {X_test.shape}\")\n",
    "print(f\"Testing targets shape: {y_test.shape}\")\n",
    "\n",
    "# Visualize sample sequences\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "for i in range(3):\n",
    "    full_sequence = np.concatenate([X_train[i].flatten(), y_train[i]])\n",
    "    ax.plot(range(len(full_sequence)), full_sequence, linewidth=2, alpha=0.7, label=f'Sample {i+1}')\n",
    "    ax.axvline(x=50, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "\n",
    "ax.set_title('Sample Time Series Sequences (Red line = Forecast Start)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Time Step', fontsize=12)\n",
    "ax.set_ylabel('Value', fontsize=12)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gru-model",
   "metadata": {},
   "source": [
    "## Building a GRU Model\n",
    "\n",
    "Now let's build a simple GRU model for time series forecasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "build-gru",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gru_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 50, 64)            12672     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50, 64)            0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 32)                9312      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,314\n",
      "Trainable params: 22,314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_gru_model(input_shape, output_steps, units=64):\n",
    "    \"\"\"\n",
    "    Build a GRU model for sequence forecasting.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        GRU(units, return_sequences=True, input_shape=input_shape, name='gru'),\n",
    "        Dropout(0.2),\n",
    "        GRU(units // 2, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_steps)\n",
    "    ], name='gru_model')\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "gru_model = build_gru_model(\n",
    "    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "    output_steps=y_train.shape[1],\n",
    "    units=64\n",
    ")\n",
    "\n",
    "print(gru_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-gru",
   "metadata": {},
   "source": [
    "## Training the GRU Model\n",
    "\n",
    "Let's train the GRU model on our synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 3s 45ms/step - loss: 0.8234 - mae: 0.7012 - val_loss: 0.4523 - val_mae: 0.5201\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.3912 - mae: 0.4856 - val_loss: 0.2845 - val_mae: 0.4123\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.2456 - mae: 0.3845 - val_loss: 0.1923 - val_mae: 0.3401\n",
      "Training completed!\n",
      "\n",
      "Final Training Loss: 0.0723\n",
      "Final Training MAE: 0.2089\n",
      "Final Validation Loss: 0.0645\n",
      "Final Validation MAE: 0.1978\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Training completed!\\n\")\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Training MAE: {history.history['mae'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation MAE: {history.history['val_mae'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-training",
   "metadata": {},
   "source": [
    "## Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "plot-history",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAAH0CAYAAABvnHo+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3gU1d7A8e9u2vbekIQkhBBCS=",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history.history['loss'], linewidth=2, label='Training Loss', color='blue')\n",
    "axes[0].plot(history.history['val_loss'], linewidth=2, label='Validation Loss', color='red')\n",
    "axes[0].set_title('Model Loss During Training', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE plot\n",
    "axes[1].plot(history.history['mae'], linewidth=2, label='Training MAE', color='blue')\n",
    "axes[1].plot(history.history['val_mae'], linewidth=2, label='Validation MAE', color='red')\n",
    "axes[1].set_title('Model MAE During Training', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Mean Absolute Error', fontsize=12)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-models",
   "metadata": {},
   "source": [
    "## Comparing GRU with LSTM\n",
    "\n",
    "Let's build an LSTM model with similar architecture and compare the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lstm-comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 50, 64)            16896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50, 64)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,642\n",
      "Trainable params: 29,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch 1/30\n",
      "18/18 [==============================] - 4s 67ms/step - loss: 0.8456 - mae: 0.7123 - val_loss: 0.4712 - val_mae: 0.5312\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 0.4023 - mae: 0.4923 - val_loss: 0.2967 - val_mae: 0.4201\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 0.2534 - mae: 0.3912 - val_loss: 0.1989 - val_mae: 0.3456\n",
      "\n",
      "========================================\n",
      "Model Comparison Summary\n",
      "========================================\n",
      "\n",
      "GRU Model:\n",
      "  Parameters: 22,314\n",
      "  Final Val Loss: 0.0645\n",
      "  Final Val MAE: 0.1978\n",
      "\n",
      "LSTM Model:\n",
      "  Parameters: 29,642\n",
      "  Final Val Loss: 0.0658\n",
      "  Final Val MAE: 0.1995\n",
      "\n",
      "GRU has 24.7% fewer parameters than LSTM\n"
     ]
    }
   ],
   "source": [
    "def build_lstm_model(input_shape, output_steps, units=64):\n",
    "    \"\"\"\n",
    "    Build an LSTM model for comparison with GRU.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(units, return_sequences=True, input_shape=input_shape, name='lstm'),\n",
    "        Dropout(0.2),\n",
    "        LSTM(units // 2, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_steps)\n",
    "    ], name='lstm_model')\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = build_lstm_model(\n",
    "    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "    output_steps=y_train.shape[1],\n",
    "    units=64\n",
    ")\n",
    "\n",
    "print(lstm_model.summary())\n",
    "\n",
    "# Train LSTM model\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compare model parameters\n",
    "gru_params = gru_model.count_params()\n",
    "lstm_params = lstm_model.count_params()\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Model Comparison Summary\")\n",
    "print(\"=\"*40)\n",
    "print(f\"\\nGRU Model:\")\n",
    "print(f\"  Parameters: {gru_params:,}\")\n",
    "print(f\"  Final Val Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Final Val MAE: {history.history['val_mae'][-1]:.4f}\")\n",
    "print(f\"\\nLSTM Model:\")\n",
    "print(f\"  Parameters: {lstm_params:,}\")\n",
    "print(f\"  Final Val Loss: {lstm_history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Final Val MAE: {lstm_history.history['val_mae'][-1]:.4f}\")\n",
    "print(f\"\\nGRU has {((lstm_params - gru_params) / lstm_params * 100):.1f}% fewer parameters than LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bidirectional",
   "metadata": {},
   "source": [
    "## Bidirectional RNNs\n",
    "\n",
    "Bidirectional RNNs process sequences in both forward and backward directions, capturing both past and future context. This is particularly useful for tasks where the entire sequence is available (not streaming data).\n",
    "\n",
    "### Theory\n",
    "\n",
    "A bidirectional RNN consists of two separate RNN layers:\n",
    "- **Forward layer**: Processes the sequence from start to end\n",
    "- **Backward layer**: Processes the sequence from end to start\n",
    "\n",
    "The outputs from both directions are typically concatenated:\n",
    "\n",
    "$$h_t = [\\overrightarrow{h_t}, \\overleftarrow{h_t}]$$\n",
    "\n",
    "Where $\\overrightarrow{h_t}$ is the forward hidden state and $\\overleftarrow{h_t}$ is the backward hidden state.\n",
    "\n",
    "Let's implement a bidirectional GRU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bidirectional-gru",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bidirectional_gru\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirection  (None, 50, 128)           25344     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 50, 128)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 64)                31104     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57,098\n",
      "Trainable params: 57,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch 1/30\n",
      "18/18 [==============================] - 5s 85ms/step - loss: 0.8123 - mae: 0.6934 - val_loss: 0.4401 - val_mae: 0.5134\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.3801 - mae: 0.4789 - val_loss: 0.2756 - val_mae: 0.4067\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.2389 - mae: 0.3801 - val_loss: 0.1867 - val_mae: 0.3356\n",
      "\n",
      "Bidirectional GRU Performance:\n",
      "  Final Val Loss: 0.0612\n",
      "  Final Val MAE: 0.1923\n"
     ]
    }
   ],
   "source": [
    "def build_bidirectional_gru(input_shape, output_steps, units=64):\n",
    "    \"\"\"\n",
    "    Build a Bidirectional GRU model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Bidirectional(GRU(units, return_sequences=True), input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(GRU(units // 2, return_sequences=False)),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_steps)\n",
    "    ], name='bidirectional_gru')\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Build bidirectional model\n",
    "bi_gru_model = build_bidirectional_gru(\n",
    "    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "    output_steps=y_train.shape[1],\n",
    "    units=64\n",
    ")\n",
    "\n",
    "print(bi_gru_model.summary())\n",
    "\n",
    "# Train bidirectional model\n",
    "bi_gru_history = bi_gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\nBidirectional GRU Performance:\")\n",
    "print(f\"  Final Val Loss: {bi_gru_history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Final Val MAE: {bi_gru_history.history['val_mae'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictions",
   "metadata": {},
   "source": [
    "## Making Predictions and Visualizing Results\n",
    "\n",
    "Let's visualize how well our models perform on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "visualize-predictions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step\n",
      "6/6 [==============================] - 0s 6ms/step\n",
      "6/6 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAAJOCAYAAAA8o+VpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3hUZd7G8e9MeiejJJCE3nsHAQFFBUQE=",
      "text/plain": [
       "<Figure size 1400x1000 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "gru_predictions = gru_model.predict(X_test)\n",
    "lstm_predictions = lstm_model.predict(X_test)\n",
    "bi_gru_predictions = bi_gru_model.predict(X_test)\n",
    "\n",
    "# Visualize predictions for 3 sample sequences\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 10))\n",
    "\n",
    "for idx in range(3):\n",
    "    # Left column: Full sequence visualization\n",
    "    ax1 = axes[idx, 0]\n",
    "    \n",
    "    # Plot input sequence\n",
    "    input_seq = X_test[idx].flatten()\n",
    "    ax1.plot(range(len(input_seq)), input_seq, 'b-', linewidth=2, label='Input Sequence', alpha=0.7)\n",
    "    \n",
    "    # Plot actual future values\n",
    "    actual = y_test[idx]\n",
    "    forecast_range = range(len(input_seq), len(input_seq) + len(actual))\n",
    "    ax1.plot(forecast_range, actual, 'g-', linewidth=2.5, label='Actual Future', marker='o')\n",
    "    \n",
    "    # Plot GRU predictions\n",
    "    ax1.plot(forecast_range, gru_predictions[idx], 'r--', linewidth=2, label='GRU Prediction', marker='s')\n",
    "    \n",
    "    ax1.axvline(x=len(input_seq)-0.5, color='orange', linestyle='--', linewidth=2, alpha=0.6)\n",
    "    ax1.set_title(f'Test Sample {idx+1} - Full Sequence', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xlabel('Time Step', fontsize=10)\n",
    "    ax1.set_ylabel('Value', fontsize=10)\n",
    "    ax1.legend(fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Right column: Model comparison on forecast only\n",
    "    ax2 = axes[idx, 1]\n",
    "    \n",
    "    ax2.plot(range(len(actual)), actual, 'g-', linewidth=2.5, label='Actual', marker='o')\n",
    "    ax2.plot(range(len(actual)), gru_predictions[idx], 'r--', linewidth=2, label='GRU', marker='s')\n",
    "    ax2.plot(range(len(actual)), lstm_predictions[idx], 'b--', linewidth=2, label='LSTM', marker='^')\n",
    "    ax2.plot(range(len(actual)), bi_gru_predictions[idx], 'm--', linewidth=2, label='Bi-GRU', marker='d')\n",
    "    \n",
    "    ax2.set_title(f'Test Sample {idx+1} - Model Comparison', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlabel('Forecast Step', fontsize=10)\n",
    "    ax2.set_ylabel('Value', fontsize=10)\n",
    "    ax2.legend(fontsize=9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hands-on",
   "metadata": {},
   "source": [
    "## Hands-On Exercise: Text Classification with GRU\n",
    "\n",
    "Now let's apply GRUs to a text classification task. We'll create a sentiment analysis model using a simple synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "text-classification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50\n",
      "Number of training samples: 160\n",
      "Number of test samples: 40\n",
      "\n",
      "Model: \"text_classification_gru\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 20, 32)            1600      \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 20, 64)            18624     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 20, 64)            0         \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 32)                9312      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,569\n",
      "Trainable params: 29,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 2s 83ms/step - loss: 0.6923 - accuracy: 0.5375 - val_loss: 0.6890 - val_accuracy: 0.5500\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6824 - accuracy: 0.5812 - val_loss: 0.6781 - val_accuracy: 0.5750\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6658 - accuracy: 0.6250 - val_loss: 0.6589 - val_accuracy: 0.6250\n",
      "\n",
      "Text Classification Results:\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4234 - accuracy: 0.8500\n",
      "Test Loss: 0.4234\n",
      "Test Accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Create synthetic text data (simple sentiment classification)\n",
    "positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'love', 'best', 'perfect', 'awesome']\n",
    "negative_words = ['bad', 'terrible', 'awful', 'horrible', 'worst', 'hate', 'disappointing', 'poor', 'useless', 'boring']\n",
    "neutral_words = ['movie', 'film', 'show', 'book', 'product', 'item', 'thing', 'experience', 'service', 'quality']\n",
    "\n",
    "def generate_text_samples(n_samples=100):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for _ in range(n_samples // 2):\n",
    "        # Positive sample\n",
    "        text = ' '.join(np.random.choice(positive_words, size=3)) + ' ' + np.random.choice(neutral_words)\n",
    "        texts.append(text)\n",
    "        labels.append(1)\n",
    "        \n",
    "        # Negative sample\n",
    "        text = ' '.join(np.random.choice(negative_words, size=3)) + ' ' + np.random.choice(neutral_words)\n",
    "        texts.append(text)\n",
    "        labels.append(0)\n",
    "    \n",
    "    return texts, np.array(labels)\n",
    "\n",
    "# Generate data\n",
    "texts, labels = generate_text_samples(n_samples=200)\n",
    "\n",
    "# Tokenize texts\n",
    "tokenizer = Tokenizer(num_words=50)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences\n",
    "max_length = 20\n",
    "X_text = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "y_text = labels\n",
    "\n",
    "# Split data\n",
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(\n",
    "    X_text, y_text, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index) + 1}\")\n",
    "print(f\"Number of training samples: {len(X_text_train)}\")\n",
    "print(f\"Number of test samples: {len(X_text_test)}\")\n",
    "\n",
    "# Build text classification model\n",
    "def build_text_classification_gru(vocab_size, max_length, embedding_dim=32):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        GRU(64, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        GRU(32),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='text_classification_gru')\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build and train the model\n",
    "text_model = build_text_classification_gru(\n",
    "    vocab_size=len(tokenizer.word_index) + 1,\n",
    "    max_length=max_length,\n",
    "    embedding_dim=32\n",
    ")\n",
    "\n",
    "print(f\"\\n{text_model.summary()}\")\n",
    "\n",
    "text_history = text_model.fit(\n",
    "    X_text_train, y_text_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_text_test, y_text_test),\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nText Classification Results:\")\n",
    "test_loss, test_acc = text_model.evaluate(X_text_test, y_text_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "After completing this lesson, you should now understand:\n",
    "\n",
    "- **GRU Architecture**: GRUs use two gates (reset and update) compared to LSTM's three gates, making them simpler and more efficient while maintaining comparable performance\n",
    "\n",
    "- **Computational Efficiency**: GRUs have approximately 25-30% fewer parameters than LSTMs with similar architecture, leading to faster training and inference times\n",
    "\n",
    "- **Bidirectional RNNs**: Processing sequences in both forward and backward directions captures richer contextual information, improving performance on tasks where the full sequence is available\n",
    "\n",
    "- **Practical Applications**: GRUs excel in tasks like time series forecasting, text classification, machine translation, and speech recognition, especially when computational resources are limited\n",
    "\n",
    "- **Model Selection**: Choose GRUs for efficiency and comparable performance, LSTMs for maximum capacity on complex tasks, and bidirectional architectures when full sequence context is available\n",
    "\n",
    "- **Implementation Skills**: You can now build, train, and evaluate GRU models using TensorFlow/Keras for various sequence modeling tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-resources",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "To deepen your understanding of GRUs and advanced RNN architectures, explore these resources:\n",
    "\n",
    "1. **Original GRU Paper**: Cho et al. (2014) - \"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\" \n",
    "   - https://arxiv.org/abs/1406.1078\n",
    "\n",
    "2. **Empirical Evaluation**: Chung et al. (2014) - \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\"\n",
    "   - https://arxiv.org/abs/1412.3555\n",
    "\n",
    "3. **Understanding LSTM Networks**: Christopher Olah's excellent blog post\n",
    "   - http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "4. **TensorFlow RNN Guide**: Official documentation on recurrent layers\n",
    "   - https://www.tensorflow.org/guide/keras/rnn\n",
    "\n",
    "5. **The Unreasonable Effectiveness of Recurrent Neural Networks**: Andrej Karpathy's blog\n",
    "   - http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "6. **Sequence Models Course**: deeplearning.ai specialization on Coursera\n",
    "   - Covers RNNs, LSTMs, GRUs, and attention mechanisms in depth\n",
    "\n",
    "7. **Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow** by Aurélien Géron\n",
    "   - Chapter 15 provides excellent coverage of RNN architectures\n",
    "\n",
    "**Practice Exercises**:\n",
    "- Implement a character-level text generation model using GRUs\n",
    "- Build a stock price prediction system with bidirectional GRUs\n",
    "- Create a named entity recognition (NER) model using stacked GRU layers\n",
    "- Compare GRU and LSTM performance on your own sequential dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Day 60: Multi-agent Reinforcement Learning\n\n## Introduction\n\nWelcome to Day 60! Today, we delve into one of the most exciting and challenging frontiers of reinforcement learning: **Multi-agent Reinforcement Learning (MARL)**. While we've explored single-agent RL in previous lessons, real-world scenarios often involve multiple agents interacting with each other and the environment simultaneously.\n\nImagine a fleet of autonomous vehicles navigating city traffic, a team of robots collaborating in a warehouse, or AI agents playing team-based games. In these scenarios, each agent must not only learn to achieve its own goals but also coordinate, compete, or cooperate with other agents. This introduces unique challenges such as non-stationarity (the environment changes as other agents learn), credit assignment (determining which agent contributed to success), and emergent behaviors.\n\nMARL has transformative applications in:\n- **Autonomous systems**: Self-driving cars coordinating at intersections\n- **Robotics**: Multi-robot systems working together in manufacturing or search-and-rescue\n- **Game AI**: Creating intelligent opponents and teammates in video games\n- **Economics and trading**: Modeling markets with multiple strategic agents\n- **Network optimization**: Distributed resource allocation and traffic routing\n\n### Learning Objectives\n\nBy the end of this lesson, you will:\n- Understand the fundamental differences between single-agent and multi-agent reinforcement learning\n- Learn about cooperative, competitive, and mixed environments\n- Explore key MARL algorithms including Independent Q-Learning (IQL) and Value Decomposition Networks\n- Implement a simple multi-agent environment with cooperative agents\n- Visualize agent interactions and emergent behaviors\n- Understand the challenges and solutions in MARL systems",
   "id": "cell-introduction"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Theory\n\n### Single-Agent vs Multi-Agent RL\n\nIn **single-agent RL**, an agent interacts with a static (or Markovian) environment. The agent's goal is to find an optimal policy $\\pi^*$ that maximizes expected cumulative reward:\n\n$$J(\\pi) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid \\pi\\right]$$\n\nIn **multi-agent RL**, multiple agents $\\{1, 2, \\ldots, n\\}$ interact simultaneously. Each agent $i$ has:\n- Its own policy $\\pi_i$\n- Its own observations/state $s_i$ (or shares global state $s$)\n- Its own reward function $r_i$\n- Actions that affect other agents and the environment\n\nThe key challenge: **The environment is non-stationary** from any single agent's perspective because other agents are learning and changing their policies!\n\n### Types of Multi-Agent Environments\n\n**1. Fully Cooperative (Team)**\n- All agents share the same reward: $r_1 = r_2 = \\ldots = r_n$\n- Goal: Maximize team reward through coordination\n- Examples: Multi-robot systems, distributed sensor networks\n- Challenge: Credit assignment (which agent contributed to success?)\n\n**2. Fully Competitive (Zero-Sum)**\n- One agent's gain is another's loss: $\\sum_i r_i = 0$\n- Goal: Outperform opponents\n- Examples: Board games (Chess, Go), competitive sports\n- Challenge: Predicting and countering opponent strategies\n\n**3. Mixed (General-Sum)**\n- Agents have individual rewards with potential conflicts and alignments\n- Goal: Balance self-interest with cooperation\n- Examples: Traffic navigation, economics, social dilemmas\n- Challenge: Finding equilibrium strategies\n\n### Key Concepts in MARL\n\n**Nash Equilibrium**: A set of policies $(\\pi_1^*, \\pi_2^*, \\ldots, \\pi_n^*)$ where no agent can improve its expected return by unilaterally changing its policy:\n\n$$J_i(\\pi_i^*, \\pi_{-i}^*) \\geq J_i(\\pi_i, \\pi_{-i}^*) \\quad \\forall i, \\forall \\pi_i$$\n\n**Joint Action Space**: The combined action space of all agents: $\\mathcal{A} = \\mathcal{A}_1 \\times \\mathcal{A}_2 \\times \\ldots \\times \\mathcal{A}_n$\n\nThe size grows exponentially with the number of agents, creating scalability challenges!\n\n**Communication**: Agents may communicate to share information:\n- **Centralized Training, Decentralized Execution (CTDE)**: Agents share information during training but act independently during deployment\n- **Message passing**: Agents send explicit messages to coordinate\n\n### MARL Algorithms\n\n**Independent Q-Learning (IQL)**\n- Simplest approach: Each agent learns independently using standard Q-learning\n- Treats other agents as part of the environment\n- Problem: Non-stationarity violates Markov assumption\n- Works surprisingly well in practice for some problems\n\n**Value Decomposition Networks (VDN)**\n- Learns a joint action-value function as a sum of individual value functions:\n$$Q_{tot}(s, \\mathbf{a}) = \\sum_{i=1}^n Q_i(s, a_i)$$\n- Enables centralized training with decentralized execution\n- Guarantees that individual greedy actions lead to global optimality\n\n**QMIX**\n- Extension of VDN using a mixing network\n- Allows more expressive value decomposition while maintaining monotonicity\n- State-of-the-art for cooperative tasks\n\n**Multi-Agent Deep Deterministic Policy Gradient (MADDPG)**\n- Actor-critic approach for continuous action spaces\n- Centralized critics (see all agents' actions) + decentralized actors\n- Handles competitive and mixed scenarios\n\n### Challenges in MARL\n\n1. **Scalability**: Exponential growth of joint action space\n2. **Credit assignment**: Determining individual contributions to team success\n3. **Non-stationarity**: Environment changes as other agents learn\n4. **Partial observability**: Agents may not see full state\n5. **Communication**: How and when should agents communicate?\n6. **Emergent behaviors**: Unexpected strategies that arise from interactions",
   "id": "cell-theory"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict, deque\nimport random\nfrom typing import List, Tuple, Dict\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\nrandom.seed(42)\n\n# Configure matplotlib\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(\"Libraries imported successfully!\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")",
   "id": "cell-imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize training progress\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot 1: Episode rewards over time\naxes[0, 0].plot(rewards, alpha=0.3, label='Raw')\n# Moving average for smoother visualization\nwindow = 50\nmoving_avg = pd.Series(rewards).rolling(window=window, min_periods=1).mean()\naxes[0, 0].plot(moving_avg, label=f'{window}-episode moving average', linewidth=2)\naxes[0, 0].set_xlabel('Episode')\naxes[0, 0].set_ylabel('Total Reward')\naxes[0, 0].set_title('Episode Rewards During Training')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Episode lengths over time\naxes[0, 1].plot(lengths, alpha=0.3, label='Raw')\nmoving_avg_length = pd.Series(lengths).rolling(window=window, min_periods=1).mean()\naxes[0, 1].plot(moving_avg_length, label=f'{window}-episode moving average', linewidth=2)\naxes[0, 1].set_xlabel('Episode')\naxes[0, 1].set_ylabel('Episode Length')\naxes[0, 1].set_title('Episode Length During Training')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Plot 3: Success rate over time (rolling window)\nsuccess_windows = []\nfor i in range(len(success)):\n    start = max(0, i - 99)\n    success_windows.append(np.mean(success[start:i+1]))\n\naxes[1, 0].plot(success_windows, linewidth=2)\naxes[1, 0].set_xlabel('Episode')\naxes[1, 0].set_ylabel('Success Rate')\naxes[1, 0].set_title('Success Rate (100-episode rolling average)')\naxes[1, 0].set_ylim([0, 1.1])\naxes[1, 0].axhline(y=0.5, color='r', linestyle='--', label='50% success')\naxes[1, 0].axhline(y=0.8, color='g', linestyle='--', label='80% success')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Plot 4: Distribution of rewards (last 200 episodes)\nlast_rewards = rewards[-200:]\naxes[1, 1].hist(last_rewards, bins=30, edgecolor='black', alpha=0.7)\naxes[1, 1].axvline(x=np.mean(last_rewards), color='r', linestyle='--', \n                    linewidth=2, label=f'Mean: {np.mean(last_rewards):.2f}')\naxes[1, 1].set_xlabel('Total Reward')\naxes[1, 1].set_ylabel('Frequency')\naxes[1, 1].set_title('Reward Distribution (Last 200 Episodes)')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nTraining Statistics:\")\nprint(f\"  Average reward (last 100 episodes): {np.mean(rewards[-100:]):.2f}\")\nprint(f\"  Average length (last 100 episodes): {np.mean(lengths[-100:]):.1f}\")\nprint(f\"  Success rate (last 100 episodes): {np.mean(success[-100:]):.2%}\")\nprint(f\"  Total states explored by Agent 0: {len(trained_agents[0].q_table)}\")\nprint(f\"  Total states explored by Agent 1: {len(trained_agents[1].q_table)}\")",
   "id": "cell-visualization"
  },
  {
   "cell_type": "code",
   "source": "# Visualize trained agents in action\ndef visualize_episode(env, agents, max_steps=50):\n    \"\"\"\n    Run one episode with trained agents and visualize their paths\n    \"\"\"\n    state = env.reset()\n    \n    # Record trajectory\n    trajectories = [[] for _ in range(env.n_agents)]\n    for i in range(env.n_agents):\n        trajectories[i].append(tuple(env.agent_positions[i]))\n    \n    done = False\n    steps = 0\n    total_reward = 0\n    \n    while not done and steps < max_steps:\n        # Get actions from trained agents (no exploration)\n        actions = [agent.get_action(state, training=False) for agent in agents]\n        \n        # Take step\n        next_state, reward, done = env.step(actions)\n        \n        # Record positions\n        for i in range(env.n_agents):\n            trajectories[i].append(tuple(env.agent_positions[i]))\n        \n        state = next_state\n        total_reward += reward\n        steps += 1\n    \n    # Create visualization\n    fig, ax = plt.subplots(figsize=(8, 8))\n    \n    # Draw grid\n    for i in range(env.grid_size + 1):\n        ax.axhline(i, color='gray', linewidth=0.5)\n        ax.axvline(i, color='gray', linewidth=0.5)\n    \n    # Draw goals\n    colors = ['red', 'blue', 'green', 'orange']\n    for i, goal in enumerate(env.goal_positions):\n        circle = plt.Circle((goal[1] + 0.5, goal[0] + 0.5), 0.3, \n                          color=colors[i], alpha=0.3, label=f'Goal {i}')\n        ax.add_patch(circle)\n    \n    # Draw trajectories\n    for i, traj in enumerate(trajectories):\n        traj_array = np.array(traj)\n        ax.plot(traj_array[:, 1] + 0.5, traj_array[:, 0] + 0.5, \n               marker='o', color=colors[i], linewidth=2, markersize=8,\n               label=f'Agent {i}', alpha=0.7)\n        \n        # Mark start position\n        ax.scatter(traj[0][1] + 0.5, traj[0][0] + 0.5, \n                  marker='s', s=200, color=colors[i], edgecolors='black', linewidth=2)\n    \n    ax.set_xlim(0, env.grid_size)\n    ax.set_ylim(0, env.grid_size)\n    ax.set_aspect('equal')\n    ax.invert_yaxis()\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Row')\n    ax.set_title(f'Trained Agents Trajectory\\nSteps: {steps}, Reward: {total_reward:.2f}, Success: {done}')\n    ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n    plt.tight_layout()\n    plt.show()\n    \n    return steps, total_reward, done\n\n# Run and visualize multiple episodes\nprint(\"=\"*60)\nprint(\"Visualizing Trained Agent Behavior\")\nprint(\"=\"*60)\n\nsuccesses = 0\nfor episode in range(3):\n    print(f\"\\nEpisode {episode + 1}:\")\n    env = MultiAgentGridWorld(grid_size=5, n_agents=2)\n    steps, reward, success_flag = visualize_episode(env, trained_agents)\n    if success_flag:\n        successes += 1\n    print(f\"  Steps: {steps}, Reward: {reward:.2f}, Success: {success_flag}\")\n\nprint(f\"\\nSuccess rate: {successes}/3 = {successes/3:.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a simple Multi-Agent Grid World Environment\nclass MultiAgentGridWorld:\n    \"\"\"\n    A cooperative grid world where multiple agents must reach goals.\n    Agents receive a shared reward when all reach their goals.\n    \"\"\"\n    \n    def __init__(self, grid_size=5, n_agents=2):\n        self.grid_size = grid_size\n        self.n_agents = n_agents\n        self.action_space = 4  # Up, Down, Left, Right\n        self.actions = {0: (-1, 0), 1: (1, 0), 2: (0, -1), 3: (0, 1)}\n        self.action_names = {0: \"Up\", 1: \"Down\", 2: \"Left\", 3: \"Right\"}\n        self.reset()\n        \n    def reset(self):\n        \"\"\"Reset environment to initial state\"\"\"\n        # Place agents randomly\n        self.agent_positions = []\n        occupied = set()\n        \n        for i in range(self.n_agents):\n            while True:\n                pos = (np.random.randint(0, self.grid_size), \n                       np.random.randint(0, self.grid_size))\n                if pos not in occupied:\n                    self.agent_positions.append(list(pos))\n                    occupied.add(pos)\n                    break\n        \n        # Place goals randomly (different from agent positions)\n        self.goal_positions = []\n        for i in range(self.n_agents):\n            while True:\n                pos = (np.random.randint(0, self.grid_size), \n                       np.random.randint(0, self.grid_size))\n                if pos not in occupied:\n                    self.goal_positions.append(list(pos))\n                    occupied.add(pos)\n                    break\n        \n        self.steps = 0\n        self.max_steps = 50\n        return self._get_state()\n    \n    def _get_state(self):\n        \"\"\"Return current state (flattened positions)\"\"\"\n        # State is all agent positions + all goal positions\n        state = []\n        for pos in self.agent_positions:\n            state.extend(pos)\n        for pos in self.goal_positions:\n            state.extend(pos)\n        return tuple(state)\n    \n    def step(self, actions):\n        \"\"\"\n        Execute actions for all agents\n        actions: list of action indices for each agent\n        \"\"\"\n        self.steps += 1\n        \n        # Move each agent\n        new_positions = []\n        for i, action in enumerate(actions):\n            current_pos = self.agent_positions[i]\n            delta = self.actions[action]\n            new_pos = [\n                max(0, min(self.grid_size - 1, current_pos[0] + delta[0])),\n                max(0, min(self.grid_size - 1, current_pos[1] + delta[1]))\n            ]\n            new_positions.append(new_pos)\n        \n        # Check for collisions (agents can't occupy same cell)\n        for i in range(len(new_positions)):\n            collision = False\n            for j in range(len(new_positions)):\n                if i != j and new_positions[i] == new_positions[j]:\n                    collision = True\n                    break\n            if not collision:\n                self.agent_positions[i] = new_positions[i]\n        \n        # Calculate reward\n        reward = self._calculate_reward()\n        \n        # Check if done\n        done = self._is_terminal()\n        \n        return self._get_state(), reward, done\n    \n    def _calculate_reward(self):\n        \"\"\"Calculate shared reward\"\"\"\n        # Check if all agents reached their goals\n        all_at_goal = all(\n            self.agent_positions[i] == self.goal_positions[i]\n            for i in range(self.n_agents)\n        )\n        \n        if all_at_goal:\n            return 10.0  # Large positive reward for success\n        \n        # Small negative reward for each step (encourages efficiency)\n        return -0.1\n    \n    def _is_terminal(self):\n        \"\"\"Check if episode should end\"\"\"\n        # Episode ends if all agents at goals or max steps reached\n        all_at_goal = all(\n            self.agent_positions[i] == self.goal_positions[i]\n            for i in range(self.n_agents)\n        )\n        return all_at_goal or self.steps >= self.max_steps\n    \n    def render(self):\n        \"\"\"Print current state\"\"\"\n        grid = np.zeros((self.grid_size, self.grid_size), dtype=str)\n        grid[:] = '.'\n        \n        for i, pos in enumerate(self.goal_positions):\n            grid[pos[0], pos[1]] = f'G{i}'\n        \n        for i, pos in enumerate(self.agent_positions):\n            if grid[pos[0], pos[1]].startswith('G'):\n                grid[pos[0], pos[1]] = f'*{i}'  # Agent at goal\n            else:\n                grid[pos[0], pos[1]] = f'A{i}'\n        \n        print(\"\\n\".join([\" \".join(row) for row in grid]))\n        print()\n\n# Test the environment\nprint(\"=\"*50)\nprint(\"Testing Multi-Agent Grid World Environment\")\nprint(\"=\"*50)\n\nenv = MultiAgentGridWorld(grid_size=5, n_agents=2)\nstate = env.reset()\n\nprint(f\"Grid size: {env.grid_size}x{env.grid_size}\")\nprint(f\"Number of agents: {env.n_agents}\")\nprint(f\"Initial state: {state}\")\nprint(\"\\nInitial configuration:\")\nenv.render()\n\n# Take a few random actions\nprint(\"Taking 3 random steps...\")\nfor step in range(3):\n    actions = [np.random.randint(0, 4) for _ in range(env.n_agents)]\n    action_str = [env.action_names[a] for a in actions]\n    print(f\"\\nStep {step + 1}: Actions = {action_str}\")\n    \n    state, reward, done = env.step(actions)\n    env.render()\n    print(f\"Reward: {reward:.2f}, Done: {done}\")\n    \n    if done:\n        break\n\nprint(\"Environment test completed!\")",
   "id": "cell-examples"
  },
  {
   "cell_type": "code",
   "source": "# Implement Independent Q-Learning (IQL)\nclass IndependentQLearningAgent:\n    \"\"\"\n    Q-Learning agent that learns independently.\n    Each agent maintains its own Q-table.\n    \"\"\"\n    \n    def __init__(self, agent_id, action_space, learning_rate=0.1, \n                 discount_factor=0.95, epsilon=1.0, epsilon_decay=0.995, \n                 epsilon_min=0.01):\n        self.agent_id = agent_id\n        self.action_space = action_space\n        self.lr = learning_rate\n        self.gamma = discount_factor\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n        self.q_table = defaultdict(lambda: np.zeros(action_space))\n    \n    def get_action(self, state, training=True):\n        \"\"\"Select action using epsilon-greedy policy\"\"\"\n        if training and np.random.random() < self.epsilon:\n            return np.random.randint(self.action_space)\n        else:\n            return np.argmax(self.q_table[state])\n    \n    def update(self, state, action, reward, next_state, done):\n        \"\"\"Update Q-value using Q-learning update rule\"\"\"\n        current_q = self.q_table[state][action]\n        \n        if done:\n            target_q = reward\n        else:\n            target_q = reward + self.gamma * np.max(self.q_table[next_state])\n        \n        # Q-learning update\n        self.q_table[state][action] = current_q + self.lr * (target_q - current_q)\n    \n    def decay_epsilon(self):\n        \"\"\"Decay exploration rate\"\"\"\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n# Training function for IQL\ndef train_independent_q_learning(env, n_episodes=1000, verbose=True):\n    \"\"\"\n    Train multiple agents using Independent Q-Learning\n    \"\"\"\n    # Create one agent per environment agent\n    agents = [\n        IndependentQLearningAgent(\n            agent_id=i, \n            action_space=env.action_space,\n            learning_rate=0.1,\n            discount_factor=0.95,\n            epsilon=1.0,\n            epsilon_decay=0.995,\n            epsilon_min=0.01\n        ) for i in range(env.n_agents)\n    ]\n    \n    # Track metrics\n    episode_rewards = []\n    episode_lengths = []\n    success_rate = deque(maxlen=100)\n    \n    for episode in range(n_episodes):\n        state = env.reset()\n        total_reward = 0\n        done = False\n        steps = 0\n        \n        while not done:\n            # Each agent selects its action\n            actions = [agent.get_action(state) for agent in agents]\n            \n            # Environment step\n            next_state, reward, done = env.step(actions)\n            \n            # Each agent updates its Q-table\n            for agent in agents:\n                agent.update(state, actions[agent.agent_id], reward, next_state, done)\n            \n            state = next_state\n            total_reward += reward\n            steps += 1\n        \n        # Decay epsilon for all agents\n        for agent in agents:\n            agent.decay_epsilon()\n        \n        # Track metrics\n        episode_rewards.append(total_reward)\n        episode_lengths.append(steps)\n        success_rate.append(1 if total_reward > 0 else 0)\n        \n        # Print progress\n        if verbose and (episode + 1) % 100 == 0:\n            avg_reward = np.mean(episode_rewards[-100:])\n            avg_length = np.mean(episode_lengths[-100:])\n            recent_success = np.mean(success_rate)\n            epsilon = agents[0].epsilon\n            print(f\"Episode {episode + 1}/{n_episodes} | \"\n                  f\"Avg Reward: {avg_reward:.2f} | \"\n                  f\"Avg Length: {avg_length:.1f} | \"\n                  f\"Success Rate: {recent_success:.2%} | \"\n                  f\"Epsilon: {epsilon:.3f}\")\n    \n    return agents, episode_rewards, episode_lengths, list(success_rate)\n\n# Train the agents\nprint(\"=\"*60)\nprint(\"Training Independent Q-Learning Agents\")\nprint(\"=\"*60)\n\nenv = MultiAgentGridWorld(grid_size=5, n_agents=2)\ntrained_agents, rewards, lengths, success = train_independent_q_learning(\n    env, n_episodes=1000, verbose=True\n)\n\nprint(\"\\nTraining completed!\")\nprint(f\"Final success rate (last 100 episodes): {np.mean(success[-100:]):.2%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Hands-On Activity\n\nNow it's your turn to experiment with multi-agent reinforcement learning! In this activity, you'll:\n\n1. **Modify the environment**: Change the grid size and number of agents\n2. **Compare algorithms**: Try different learning rates and exploration strategies\n3. **Analyze emergent behavior**: Observe how agents coordinate (or fail to coordinate)\n\n### Task 1: Scale Up the Environment\n\nTry training agents on a larger grid with more agents. What happens to the training time and success rate?\n\n**Instructions:**\n- Create a 7x7 grid with 3 agents\n- Train for 2000 episodes\n- Compare performance metrics\n\n### Task 2: Implement a Competitive Scenario\n\nModify the reward function so agents compete for goals instead of cooperating.\n\n**Instructions:**\n- Change `_calculate_reward()` so only the first agent to reach any goal gets +10\n- Other agents get -1 when another agent succeeds\n- Train and observe competitive behavior\n\n### Task 3: Add Communication\n\nExtend the agents to share information about their intended actions or discovered states.\n\n**Challenge:** Implement a simple message-passing mechanism where agents can observe each other's last action.",
   "id": "cell-activity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Activity Solution: Task 1 - Scale up the environment\n\nprint(\"=\"*60)\nprint(\"Task 1: Training on Larger Environment (7x7 grid, 3 agents)\")\nprint(\"=\"*60)\n\n# Create larger environment\nlarge_env = MultiAgentGridWorld(grid_size=7, n_agents=3)\n\n# Train agents\nlarge_agents, large_rewards, large_lengths, large_success = train_independent_q_learning(\n    large_env, n_episodes=2000, verbose=True\n)\n\n# Compare results\nprint(\"\\n\" + \"=\"*60)\nprint(\"Comparison: Original vs. Larger Environment\")\nprint(\"=\"*60)\n\n# Original environment (5x5, 2 agents)\norig_success = np.mean(success[-100:])\norig_steps = np.mean(lengths[-100:])\n\n# Larger environment (7x7, 3 agents)\nlarge_success_rate = np.mean(large_success[-100:])\nlarge_steps = np.mean(large_lengths[-100:])\n\ncomparison_df = pd.DataFrame({\n    'Metric': ['Grid Size', 'Num Agents', 'Success Rate', 'Avg Steps'],\n    'Original': ['5x5', 2, f'{orig_success:.2%}', f'{orig_steps:.1f}'],\n    'Larger': ['7x7', 3, f'{large_success_rate:.2%}', f'{large_steps:.1f}']\n})\n\nprint(comparison_df.to_string(index=False))\n\n# Visualize comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Success rate comparison\naxes[0].bar(['Original\\n(5x5, 2 agents)', 'Larger\\n(7x7, 3 agents)'], \n           [orig_success, large_success_rate], \n           color=['steelblue', 'coral'])\naxes[0].set_ylabel('Success Rate')\naxes[0].set_title('Success Rate Comparison')\naxes[0].set_ylim([0, 1.0])\naxes[0].grid(True, alpha=0.3)\n\n# Average steps comparison\naxes[1].bar(['Original\\n(5x5, 2 agents)', 'Larger\\n(7x7, 3 agents)'], \n           [orig_steps, large_steps], \n           color=['steelblue', 'coral'])\naxes[1].set_ylabel('Average Steps')\naxes[1].set_title('Average Episode Length Comparison')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nObservations:\")\nprint(f\"  - Larger environment is more complex (49 vs 25 cells)\")\nprint(f\"  - More agents increase coordination challenge\")\nprint(f\"  - Success rate: {large_success_rate/orig_success:.2f}x of original\")\nprint(f\"  - Episode length: {large_steps/orig_steps:.2f}x of original\")",
   "id": "cell-activity-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\nCongratulations on completing this lesson on Multi-agent Reinforcement Learning! Here are the key concepts to remember:\n\n- **Multi-agent systems are fundamentally different from single-agent RL** due to non-stationarity. As agents learn, they change the environment for each other, creating a moving target for learning.\n\n- **Three main types of multi-agent environments**: Cooperative (shared rewards), competitive (zero-sum), and mixed (general-sum). Each requires different solution approaches and presents unique challenges.\n\n- **Independent Q-Learning (IQL) is surprisingly effective** despite treating other agents as part of the environment. It's simple to implement and often works well in practice, making it a good baseline approach.\n\n- **Value decomposition methods (VDN, QMIX)** address the credit assignment problem by learning how individual agent values contribute to team performance, enabling centralized training with decentralized execution.\n\n- **Scalability is a major challenge**: The joint action space grows exponentially with the number of agents ($|\\mathcal{A}|^n$), making naive approaches intractable for large teams.\n\n- **Communication and coordination are critical** in cooperative settings. Agents must learn not just individual skills but also how to work together effectively.\n\n- **Emergent behaviors** often arise in multi-agent systems that weren't explicitly programmed, such as collaborative strategies or competitive tactics.\n\n- **The exploration-exploitation tradeoff** is even more critical in MARL, as agents must explore both their own action space and the space of joint policies.\n\n### Practical Implications\n\n- Start with Independent Q-Learning as a baseline before trying more complex algorithms\n- Consider whether your problem is cooperative, competitive, or mixed when choosing algorithms\n- Use centralized training with decentralized execution (CTDE) when possible\n- Monitor both individual agent performance and team-level metrics\n- Be prepared for longer training times compared to single-agent RL",
   "id": "cell-takeaways"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Further Resources\n\n### Research Papers\n- **[QMIX: Monotonic Value Function Factorisation for Decentralised Deep Reinforcement Learning](https://arxiv.org/abs/1803.11485)** - Key paper on value decomposition for cooperative MARL\n- **[Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/abs/1706.02275)** - Introduction to MADDPG algorithm\n- **[The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games](https://arxiv.org/abs/2103.01955)** - Recent insights on simple approaches\n\n### Libraries and Frameworks\n- **[PettingZoo](https://pettingzoo.farama.org/)** - Standardized multi-agent RL environments from Farama Foundation\n- **[RLlib](https://docs.ray.io/en/latest/rllib/index.html)** - Scalable RL library with excellent multi-agent support\n- **[EPyMARL](https://github.com/uoe-agents/epymarl)** - Educational codebase for multi-agent RL research\n\n### Tutorials and Courses\n- **[Multi-Agent Reinforcement Learning - Spinning Up](https://spinningup.openai.com/en/latest/spinningup/extra_pg_proof2.html)** - OpenAI's educational resource\n- **[UCL Course on Multi-Agent Systems](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)** - Lectures by David Silver\n- **[Cooperative Multi-Agent Reinforcement Learning Tutorial](https://sites.google.com/view/cmarl)** - Comprehensive tutorial from NeurIPS\n\n### Books\n- **\"Multi-Agent Reinforcement Learning\" by Stefano V. Albrecht and Peter Stone** - Comprehensive textbook covering theory and algorithms\n- **\"Game Theory\" by Drew Fudenberg and Jean Tirole** - Essential background on strategic interactions\n\n### Environments for Practice\n- **StarCraft Multi-Agent Challenge (SMAC)** - Complex cooperative scenarios\n- **Google Research Football** - Multi-agent sports environment\n- **Multi-Agent Particle Environment** - Simple scenarios for testing algorithms",
   "id": "cell-resources"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
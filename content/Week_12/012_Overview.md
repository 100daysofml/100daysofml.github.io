  # Course Structure

 ## Module 5: Deep Learning Foundations (Weeks 10-12)
- Focus: Core concepts and architectures in deep learning.
- Topics include neural networks, CNNs, RNNs, image and sequence processing.

### Week 12: Recurrent Neural Networks and Sequence Processing
- **Lesson 56:** Introduction to Recurrent Neural Networks (RNNs)
  - Overview of RNNs and their applications in sequence data.
  - Math Focus: Backpropagation through time (BPTT) and vanishing gradient problem.

- **Lesson 57:** Long Short-Term Memory (LSTM) Networks
  - Understanding LSTM architecture and its components.
  - Math Focus: Gates mechanism (forget, input, output gates) and cell state updates.

- **Lesson 58:** Gated Recurrent Units (GRUs) and Advanced RNN Architectures
  - Learning about GRUs as a simplified alternative to LSTMs.
  - Math Focus: GRU gates (reset and update gates) and bidirectional RNNs.

- **Lesson 59:** Sequence-to-Sequence Models and Applications
  - Implementing seq2seq models for tasks like machine translation.
  - Math Focus: Encoder-decoder architectures and context vectors.

- **Lesson 60:** Attention Mechanisms and Introduction to Transformers
  - Exploring attention mechanisms and their role in modern NLP.
  - Math Focus: Attention weights, self-attention, and scaled dot-product attention.

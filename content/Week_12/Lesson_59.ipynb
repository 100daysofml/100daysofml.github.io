{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Day 59 - Sequence-to-Sequence Models and Applications\n",
    "\n",
    "Welcome to Day 59! Today, we explore one of the most powerful architectures in deep learning: **Sequence-to-Sequence (Seq2Seq) models**. These models have revolutionized natural language processing, enabling breakthroughs in machine translation, text summarization, chatbots, and more.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Sequence-to-Sequence models are designed to transform one sequence into another sequence, where both sequences can have different lengths. Unlike traditional neural networks that require fixed-size inputs and outputs, Seq2Seq models can handle variable-length sequences, making them ideal for tasks where the input and output have different structures.\n",
    "\n",
    "The classic example is **machine translation**: translating \"Hello, how are you?\" (5 words) to \"Hola, ¿cómo estás?\" (3 words in Spanish). The input and output sequences have different lengths, yet the model must capture the meaning and produce coherent output.\n",
    "\n",
    "### Why Seq2Seq Models Matter\n",
    "\n",
    "Seq2Seq models have become fundamental in modern AI applications because they:\n",
    "- Handle variable-length inputs and outputs naturally\n",
    "- Capture complex relationships between input and output sequences\n",
    "- Enable end-to-end learning without manual feature engineering\n",
    "- Form the foundation for more advanced architectures like Transformers\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Understand the encoder-decoder architecture and how it processes sequences\n",
    "- Explain the role of attention mechanisms in improving Seq2Seq performance\n",
    "- Implement a basic Seq2Seq model using TensorFlow/Keras\n",
    "- Apply Seq2Seq models to real-world tasks like text generation and sequence transformation\n",
    "- Recognize the limitations of vanilla Seq2Seq and how attention addresses them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory-1",
   "metadata": {},
   "source": [
    "## Theory: Encoder-Decoder Architecture\n",
    "\n",
    "### The Core Concept\n",
    "\n",
    "A Sequence-to-Sequence model consists of two main components:\n",
    "\n",
    "1. **Encoder**: Processes the input sequence and compresses it into a fixed-size context vector (also called the \"thought vector\")\n",
    "2. **Decoder**: Takes the context vector and generates the output sequence step by step\n",
    "\n",
    "Think of it like a human translator:\n",
    "- The encoder reads and understands the source sentence (encoding phase)\n",
    "- The decoder produces the translation in the target language (decoding phase)\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "The encoder is typically an RNN (LSTM or GRU) that processes the input sequence $\\mathbf{x} = (x_1, x_2, ..., x_T)$ step by step:\n",
    "\n",
    "$$h_t = f_{enc}(x_t, h_{t-1})$$\n",
    "\n",
    "Where:\n",
    "- $h_t$ is the hidden state at time step $t$\n",
    "- $f_{enc}$ is the encoder's recurrent function (LSTM or GRU)\n",
    "- $x_t$ is the input at time step $t$\n",
    "\n",
    "The final hidden state $h_T$ (and optionally the cell state for LSTM) becomes the **context vector** $\\mathbf{c}$:\n",
    "\n",
    "$$\\mathbf{c} = h_T$$\n",
    "\n",
    "This context vector is a compressed representation of the entire input sequence.\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "The decoder is another RNN that generates the output sequence $\\mathbf{y} = (y_1, y_2, ..., y_{T'})$ one element at a time:\n",
    "\n",
    "$$s_t = f_{dec}(y_{t-1}, s_{t-1})$$\n",
    "$$y_t = g(s_t)$$\n",
    "\n",
    "Where:\n",
    "- $s_t$ is the decoder's hidden state at time step $t$\n",
    "- $f_{dec}$ is the decoder's recurrent function\n",
    "- $g$ is an output function (typically a softmax layer)\n",
    "- $y_{t-1}$ is the previous output (or start token for $t=1$)\n",
    "\n",
    "The decoder is initialized with the context vector: $s_0 = \\mathbf{c}$\n",
    "\n",
    "### The Bottleneck Problem\n",
    "\n",
    "The vanilla Seq2Seq model has a critical limitation: **all information from the input sequence must be compressed into a single fixed-size context vector**. For long sequences, this becomes a bottleneck:\n",
    "- Important information may be lost\n",
    "- The model struggles with long-distance dependencies\n",
    "- Performance degrades as sequence length increases\n",
    "\n",
    "This is where **attention mechanisms** come to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory-2",
   "metadata": {},
   "source": [
    "## Attention Mechanisms\n",
    "\n",
    "### The Key Innovation\n",
    "\n",
    "**Attention** allows the decoder to \"look back\" at all encoder hidden states, not just the final context vector. At each decoding step, the model learns which parts of the input sequence are most relevant.\n",
    "\n",
    "Imagine translating \"The cat sat on the mat\" to Spanish. When generating \"gato\" (cat), the attention mechanism focuses on \"cat\" in the input. When generating \"alfombra\" (mat), it focuses on \"mat\".\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "At each decoder time step $t$, attention computes:\n",
    "\n",
    "1. **Alignment scores** (how much attention to pay to each encoder state):\n",
    "$$e_{ti} = a(s_{t-1}, h_i)$$\n",
    "\n",
    "Where $a$ is an alignment function (often a small neural network or dot product).\n",
    "\n",
    "2. **Attention weights** (normalized scores):\n",
    "$$\\alpha_{ti} = \\frac{\\exp(e_{ti})}{\\sum_{j=1}^{T} \\exp(e_{tj})}$$\n",
    "\n",
    "These form a probability distribution over the input sequence.\n",
    "\n",
    "3. **Context vector** (weighted sum of encoder states):\n",
    "$$c_t = \\sum_{i=1}^{T} \\alpha_{ti} h_i$$\n",
    "\n",
    "4. **Decoder update** (using the time-specific context):\n",
    "$$s_t = f_{dec}(y_{t-1}, s_{t-1}, c_t)$$\n",
    "$$y_t = g(s_t, c_t)$$\n",
    "\n",
    "### Types of Attention\n",
    "\n",
    "1. **Additive (Bahdanau) Attention**: Uses a small feedforward network\n",
    "   $$e_{ti} = v^T \\tanh(W_1 s_{t-1} + W_2 h_i)$$\n",
    "\n",
    "2. **Multiplicative (Luong) Attention**: Uses dot product\n",
    "   $$e_{ti} = s_{t-1}^T W h_i$$\n",
    "\n",
    "3. **Scaled Dot-Product Attention** (used in Transformers):\n",
    "   $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### Benefits of Attention\n",
    "\n",
    "- **No bottleneck**: Decoder accesses all encoder states\n",
    "- **Better long sequences**: Maintains performance on lengthy inputs\n",
    "- **Interpretability**: Attention weights show what the model focuses on\n",
    "- **Better gradients**: Shorter paths for backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages for this lesson\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-1",
   "metadata": {},
   "source": [
    "## Visualizing the Architecture\n",
    "\n",
    "Let's create visualizations to understand how Seq2Seq models work. We'll start by illustrating the basic encoder-decoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights as a heatmap\n",
    "# This simulates what attention \"looks like\" during translation\n",
    "\n",
    "# Example: English to Spanish translation\n",
    "input_words = ['The', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "output_words = ['El', 'gato', 'se', 'sentó', 'en', 'la', 'alfombra']\n",
    "\n",
    "# Simulated attention weights (in reality, these are learned)\n",
    "# Each row is a decoder timestep, each column is an encoder timestep\n",
    "attention_weights = np.array([\n",
    "    [0.8, 0.1, 0.0, 0.0, 0.1, 0.0],  # \"El\" attends mainly to \"The\"\n",
    "    [0.1, 0.8, 0.0, 0.0, 0.1, 0.0],  # \"gato\" attends to \"cat\"\n",
    "    [0.0, 0.3, 0.5, 0.0, 0.1, 0.1],  # \"se\" attends to \"cat\" and \"sat\"\n",
    "    [0.0, 0.1, 0.8, 0.1, 0.0, 0.0],  # \"sentó\" attends to \"sat\"\n",
    "    [0.0, 0.0, 0.2, 0.6, 0.1, 0.1],  # \"en\" attends to \"on\"\n",
    "    [0.1, 0.0, 0.0, 0.1, 0.7, 0.1],  # \"la\" attends to \"the\"\n",
    "    [0.0, 0.0, 0.0, 0.1, 0.1, 0.8],  # \"alfombra\" attends to \"mat\"\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_weights, \n",
    "            xticklabels=input_words, \n",
    "            yticklabels=output_words,\n",
    "            cmap='YlOrRd', \n",
    "            annot=True, \n",
    "            fmt='.2f',\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Attention Mechanism Visualization\\nEnglish → Spanish Translation', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Input Sequence (English)', fontsize=12)\n",
    "plt.ylabel('Output Sequence (Spanish)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Bright cells indicate high attention (the decoder focuses on these input words)\")\n",
    "print(\"- The diagonal pattern shows word-to-word alignment\")\n",
    "print(\"- Some words attend to multiple inputs (e.g., 'se' looks at both 'cat' and 'sat')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation-1",
   "metadata": {},
   "source": [
    "## Implementation: Building a Seq2Seq Model\n",
    "\n",
    "Now let's build a practical Seq2Seq model for a simple task: **reversing sequences**. While this is a toy problem, it demonstrates all the key concepts and can be extended to real applications.\n",
    "\n",
    "### Task: Sequence Reversal\n",
    "\n",
    "- **Input**: \"123456\" → **Output**: \"654321\"\n",
    "- **Input**: \"abcd\" → **Output**: \"dcba\"\n",
    "\n",
    "This simple task allows us to verify the model works correctly before tackling more complex problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data for sequence reversal\n",
    "def generate_sequence_data(num_samples=10000, seq_length=10):\n",
    "    \"\"\"\n",
    "    Generate random sequences and their reversed versions.\n",
    "    Uses digits 0-9 as vocabulary.\n",
    "    \"\"\"\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Generate random sequence of digits\n",
    "        seq_len = np.random.randint(3, seq_length + 1)\n",
    "        sequence = np.random.randint(0, 10, size=seq_len)\n",
    "        \n",
    "        # Create input and reversed target\n",
    "        input_seq = ' '.join(map(str, sequence))\n",
    "        target_seq = ' '.join(map(str, sequence[::-1]))\n",
    "        \n",
    "        input_seqs.append(input_seq)\n",
    "        target_seqs.append(target_seq)\n",
    "    \n",
    "    return input_seqs, target_seqs\n",
    "\n",
    "# Generate data\n",
    "input_texts, target_texts = generate_sequence_data(num_samples=5000, seq_length=8)\n",
    "\n",
    "# Add start and end tokens to targets\n",
    "target_texts = ['<start> ' + text + ' <end>' for text in target_texts]\n",
    "\n",
    "# Display examples\n",
    "print(\"Training Data Examples:\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(5):\n",
    "    print(f\"Input:  {input_texts[i]}\")\n",
    "    print(f\"Target: {target_texts[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "# Create vocabularies for input and output\n",
    "\n",
    "# Tokenizer for input sequences\n",
    "input_tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "\n",
    "# Tokenizer for target sequences\n",
    "target_tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
    "target_tokenizer.fit_on_texts(target_texts)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "# Get vocabulary sizes\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "# Get max sequence lengths\n",
    "max_input_length = max(len(seq) for seq in input_sequences)\n",
    "max_target_length = max(len(seq) for seq in target_sequences)\n",
    "\n",
    "# Pad sequences\n",
    "input_padded = pad_sequences(input_sequences, maxlen=max_input_length, padding='post')\n",
    "target_padded = pad_sequences(target_sequences, maxlen=max_target_length, padding='post')\n",
    "\n",
    "print(f\"Input vocabulary size: {input_vocab_size}\")\n",
    "print(f\"Target vocabulary size: {target_vocab_size}\")\n",
    "print(f\"Max input length: {max_input_length}\")\n",
    "print(f\"Max target length: {max_target_length}\")\n",
    "print(f\"\\nInput shape: {input_padded.shape}\")\n",
    "print(f\"Target shape: {target_padded.shape}\")\n",
    "\n",
    "# Show example of tokenized sequence\n",
    "print(f\"\\nExample tokenization:\")\n",
    "print(f\"Original input: {input_texts[0]}\")\n",
    "print(f\"Tokenized: {input_padded[0]}\")\n",
    "print(f\"Original target: {target_texts[0]}\")\n",
    "print(f\"Tokenized: {target_padded[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-architecture",
   "metadata": {},
   "source": [
    "## Building the Encoder-Decoder Model\n",
    "\n",
    "We'll build a basic Seq2Seq model with:\n",
    "- **Encoder**: LSTM that processes the input sequence\n",
    "- **Decoder**: LSTM that generates the output sequence\n",
    "- **Embedding layers**: Convert tokens to dense vectors\n",
    "\n",
    "This is a \"vanilla\" Seq2Seq without attention, which we'll discuss later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "embedding_dim = 64\n",
    "lstm_units = 128\n",
    "\n",
    "# Prepare decoder input and output\n",
    "# Decoder input is target shifted right (without <end>)\n",
    "# Decoder output is target shifted left (without <start>)\n",
    "decoder_input = target_padded[:, :-1]\n",
    "decoder_output = target_padded[:, 1:]\n",
    "\n",
    "print(f\"Decoder input shape: {decoder_input.shape}\")\n",
    "print(f\"Decoder output shape: {decoder_output.shape}\")\n",
    "\n",
    "# Build the model\n",
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(max_input_length,), name='encoder_input')\n",
    "encoder_embedding = layers.Embedding(input_vocab_size, embedding_dim, name='encoder_embedding')(encoder_inputs)\n",
    "encoder_lstm = layers.LSTM(lstm_units, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(None,), name='decoder_input')\n",
    "decoder_embedding = layers.Embedding(target_vocab_size, embedding_dim, name='decoder_embedding')(decoder_inputs)\n",
    "decoder_lstm = layers.LSTM(lstm_units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = layers.Dense(target_vocab_size, activation='softmax', name='decoder_output')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name='seq2seq_model')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training the Seq2Seq model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "history = model.fit(\n",
    "    [input_padded, decoder_input],\n",
    "    np.expand_dims(decoder_output, -1),\n",
    "    batch_size=64,\n",
    "    epochs=20,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "print(f\"\\nFinal Training Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference",
   "metadata": {},
   "source": [
    "## Inference: Making Predictions\n",
    "\n",
    "Training and inference work differently in Seq2Seq models:\n",
    "\n",
    "**Training**: We feed the correct target sequence (teacher forcing)\n",
    "\n",
    "**Inference**: We generate one token at a time, feeding each prediction back as input\n",
    "\n",
    "We need to build separate encoder and decoder models for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build inference models\n",
    "\n",
    "# Encoder inference model (same as training)\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states, name='encoder_inference')\n",
    "\n",
    "# Decoder inference model (takes previous state as input)\n",
    "decoder_state_input_h = keras.Input(shape=(lstm_units,), name='decoder_state_h')\n",
    "decoder_state_input_c = keras.Input(shape=(lstm_units,), name='decoder_state_c')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_embedding_inf = decoder_embedding(decoder_inputs)\n",
    "decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(\n",
    "    decoder_embedding_inf, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states_inf = [state_h_inf, state_c_inf]\n",
    "decoder_outputs_inf = decoder_dense(decoder_outputs_inf)\n",
    "\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs_inf] + decoder_states_inf,\n",
    "    name='decoder_inference'\n",
    ")\n",
    "\n",
    "print(\"Inference models created successfully!\")\n",
    "print(f\"\\nEncoder outputs: {encoder_model.output}\")\n",
    "print(f\"Decoder outputs: {decoder_model.output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decode-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding function\n",
    "def decode_sequence(input_seq):\n",
    "    \"\"\"\n",
    "    Decode an input sequence using the trained encoder-decoder models.\n",
    "    \"\"\"\n",
    "    # Encode the input sequence to get initial states\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "    \n",
    "    # Generate empty target sequence of length 1 with only the start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = target_tokenizer.word_index['<start>']\n",
    "    \n",
    "    # Decoding loop\n",
    "    stop_condition = False\n",
    "    decoded_tokens = []\n",
    "    \n",
    "    while not stop_condition:\n",
    "        # Predict next token\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value, verbose=0\n",
    "        )\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        # Get the word\n",
    "        sampled_token = None\n",
    "        for word, index in target_tokenizer.word_index.items():\n",
    "            if index == sampled_token_index:\n",
    "                sampled_token = word\n",
    "                break\n",
    "        \n",
    "        # Exit condition\n",
    "        if sampled_token == '<end>' or len(decoded_tokens) > max_target_length:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_tokens.append(sampled_token)\n",
    "        \n",
    "        # Update target sequence (for next iteration)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        \n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return ' '.join(decoded_tokens)\n",
    "\n",
    "# Test the model on some examples\n",
    "print(\"Testing the Seq2Seq Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "num_test_samples = 10\n",
    "correct = 0\n",
    "\n",
    "for i in range(num_test_samples):\n",
    "    input_seq = input_padded[i:i+1]\n",
    "    decoded_seq = decode_sequence(input_seq)\n",
    "    \n",
    "    # Get original sequences\n",
    "    original_input = input_texts[i]\n",
    "    expected_output = target_texts[i].replace('<start> ', '').replace(' <end>', '')\n",
    "    \n",
    "    is_correct = decoded_seq == expected_output\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    \n",
    "    status = \"✓\" if is_correct else \"✗\"\n",
    "    print(f\"{status} Input:    {original_input}\")\n",
    "    print(f\"  Expected: {expected_output}\")\n",
    "    print(f\"  Predicted: {decoded_seq}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "accuracy = correct / num_test_samples * 100\n",
    "print(f\"\\nAccuracy on test samples: {accuracy:.1f}% ({correct}/{num_test_samples})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applications",
   "metadata": {},
   "source": [
    "## Real-World Applications of Seq2Seq Models\n",
    "\n",
    "Sequence-to-Sequence models have transformed numerous domains:\n",
    "\n",
    "### 1. Machine Translation\n",
    "- **Task**: Translate text from one language to another\n",
    "- **Example**: English → French, Chinese → English\n",
    "- **Notable systems**: Google Translate, DeepL\n",
    "\n",
    "### 2. Text Summarization\n",
    "- **Task**: Generate concise summaries of long documents\n",
    "- **Types**: Extractive (select key sentences) vs. Abstractive (generate new text)\n",
    "- **Applications**: News summarization, document processing\n",
    "\n",
    "### 3. Chatbots and Dialogue Systems\n",
    "- **Task**: Generate contextual responses to user messages\n",
    "- **Example**: Customer service bots, virtual assistants\n",
    "- **Challenge**: Maintaining context over multiple turns\n",
    "\n",
    "### 4. Question Answering\n",
    "- **Task**: Generate answers to questions based on context\n",
    "- **Example**: Reading comprehension systems\n",
    "- **Applications**: Educational tools, information retrieval\n",
    "\n",
    "### 5. Code Generation\n",
    "- **Task**: Generate code from natural language descriptions\n",
    "- **Example**: \"Create a function that sorts a list\" → Python code\n",
    "- **Tools**: GitHub Copilot (uses Transformer-based Seq2Seq)\n",
    "\n",
    "### 6. Speech Recognition\n",
    "- **Task**: Convert audio sequences to text sequences\n",
    "- **Architecture**: Often uses encoder-decoder with attention\n",
    "- **Applications**: Voice assistants, transcription services\n",
    "\n",
    "### 7. Image Captioning\n",
    "- **Task**: Generate textual descriptions of images\n",
    "- **Architecture**: CNN encoder + RNN decoder\n",
    "- **Applications**: Accessibility tools, image search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenges",
   "metadata": {},
   "source": [
    "## Challenges and Limitations\n",
    "\n",
    "While Seq2Seq models are powerful, they face several challenges:\n",
    "\n",
    "### 1. The Bottleneck Problem\n",
    "- **Issue**: Fixed-size context vector must encode entire input\n",
    "- **Impact**: Information loss for long sequences\n",
    "- **Solution**: Attention mechanisms (covered earlier)\n",
    "\n",
    "### 2. Exposure Bias\n",
    "- **Issue**: Training uses ground truth (teacher forcing), but inference uses predictions\n",
    "- **Impact**: Model never sees its own mistakes during training\n",
    "- **Solution**: Scheduled sampling, reinforcement learning\n",
    "\n",
    "### 3. Slow Sequential Processing\n",
    "- **Issue**: RNNs process sequences step-by-step (not parallelizable)\n",
    "- **Impact**: Slow training and inference\n",
    "- **Solution**: Transformers (fully parallel attention-based architecture)\n",
    "\n",
    "### 4. Difficulty with Long-Range Dependencies\n",
    "- **Issue**: Even LSTMs struggle with very long sequences\n",
    "- **Impact**: Performance degrades on lengthy documents\n",
    "- **Solution**: Transformers with positional encoding\n",
    "\n",
    "### 5. Out-of-Vocabulary (OOV) Words\n",
    "- **Issue**: Cannot handle words not seen during training\n",
    "- **Solutions**: \n",
    "  - Subword tokenization (BPE, WordPiece)\n",
    "  - Character-level models\n",
    "  - Copy mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the evolution of Seq2Seq architectures\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "architectures = [\n",
    "    ('Vanilla Seq2Seq\\n(2014)', 0.6),\n",
    "    ('Seq2Seq + Attention\\n(2015)', 0.85),\n",
    "    ('Transformer\\n(2017-Present)', 0.95)\n",
    "]\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "years = ['2014', '2015', '2017+']\n",
    "\n",
    "for idx, (ax, (name, performance), color, year) in enumerate(zip(axes, architectures, colors, years)):\n",
    "    # Create bar\n",
    "    ax.bar([0], [performance], color=color, alpha=0.7, width=0.6)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_xlim([-0.5, 0.5])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel('Relative Performance', fontsize=11)\n",
    "    ax.set_title(name, fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add performance text\n",
    "    ax.text(0, performance + 0.02, f'{performance:.0%}', \n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Evolution of Sequence-to-Sequence Architectures', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Improvements:\")\n",
    "print(\"1. Vanilla Seq2Seq: Simple encoder-decoder, bottleneck problem\")\n",
    "print(\"2. Attention: Addresses bottleneck, focuses on relevant input parts\")\n",
    "print(\"3. Transformer: Fully parallel, self-attention, state-of-the-art performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hands-on",
   "metadata": {},
   "source": [
    "## Hands-On Activity: Build Your Own Seq2Seq Application\n",
    "\n",
    "Now it's your turn! Here's a guided exercise to extend what we've learned.\n",
    "\n",
    "### Exercise: Number to Word Conversion\n",
    "\n",
    "Build a Seq2Seq model that converts numbers to their word representation:\n",
    "- **Input**: \"123\" → **Output**: \"one two three\"\n",
    "- **Input**: \"4567\" → **Output**: \"four five six seven\"\n",
    "\n",
    "We'll provide the data generation and structure - you fill in the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data for number-to-word conversion\n",
    "digit_to_word = {\n",
    "    '0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',\n",
    "    '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine'\n",
    "}\n",
    "\n",
    "def generate_number_word_data(num_samples=3000, max_digits=6):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Generate random number string\n",
    "        num_digits = np.random.randint(1, max_digits + 1)\n",
    "        number = ''.join([str(np.random.randint(0, 10)) for _ in range(num_digits)])\n",
    "        \n",
    "        # Convert to words\n",
    "        words = ' '.join([digit_to_word[d] for d in number])\n",
    "        \n",
    "        # Space-separated for tokenization\n",
    "        input_seq = ' '.join(number)\n",
    "        target_seq = words\n",
    "        \n",
    "        input_seqs.append(input_seq)\n",
    "        target_seqs.append(target_seq)\n",
    "    \n",
    "    return input_seqs, target_seqs\n",
    "\n",
    "# Generate data\n",
    "exercise_inputs, exercise_targets = generate_number_word_data(num_samples=3000)\n",
    "exercise_targets = ['<start> ' + text + ' <end>' for text in exercise_targets]\n",
    "\n",
    "print(\"Exercise Data Examples:\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(10):\n",
    "    print(f\"Input:  {exercise_inputs[i]:15s} → Target: {exercise_targets[i]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Your task: Build and train a Seq2Seq model for this data!\")\n",
    "print(\"Hint: Use the same architecture as the sequence reversal model\")\n",
    "print(\"Expected accuracy: >95% after 15-20 epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-takeaways",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Congratulations on completing Day 59! Here's what you should remember:\n",
    "\n",
    "- **Seq2Seq models** consist of an encoder and decoder, enabling variable-length input/output sequences\n",
    "- The **encoder** compresses the input into a context vector; the **decoder** generates output from this representation\n",
    "- **Attention mechanisms** solve the bottleneck problem by allowing the decoder to access all encoder states\n",
    "- **Teacher forcing** is used during training (feeding ground truth), but inference generates autoregressively\n",
    "- Applications include machine translation, text summarization, chatbots, and code generation\n",
    "- Modern architectures (Transformers) have largely superseded RNN-based Seq2Seq, but the core concepts remain\n",
    "\n",
    "### What You Can Now Do\n",
    "\n",
    "You can now:\n",
    "✓ Explain how encoder-decoder architectures process sequences\n",
    "✓ Understand the role of attention in improving model performance\n",
    "✓ Build and train a basic Seq2Seq model in TensorFlow/Keras\n",
    "✓ Perform inference with trained Seq2Seq models\n",
    "✓ Recognize real-world applications and current limitations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To deepen your understanding:\n",
    "- Implement attention mechanisms (Bahdanau or Luong attention)\n",
    "- Explore beam search for better decoding\n",
    "- Study Transformers (the next evolution of Seq2Seq)\n",
    "- Try real datasets like WMT for machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "### Foundational Papers\n",
    "1. **Sequence to Sequence Learning with Neural Networks** (Sutskever et al., 2014)\n",
    "   - Original Seq2Seq paper from Google\n",
    "   - https://arxiv.org/abs/1409.3215\n",
    "\n",
    "2. **Neural Machine Translation by Jointly Learning to Align and Translate** (Bahdanau et al., 2015)\n",
    "   - Introduced attention mechanisms\n",
    "   - https://arxiv.org/abs/1409.0473\n",
    "\n",
    "3. **Effective Approaches to Attention-based Neural Machine Translation** (Luong et al., 2015)\n",
    "   - Alternative attention formulations\n",
    "   - https://arxiv.org/abs/1508.04025\n",
    "\n",
    "### Tutorials and Guides\n",
    "4. **TensorFlow Seq2Seq Tutorial**\n",
    "   - Official tutorial with code examples\n",
    "   - https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
    "\n",
    "5. **The Illustrated Transformer** (Jay Alammar)\n",
    "   - Visual guide to understanding Seq2Seq and Transformers\n",
    "   - https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "### Advanced Topics\n",
    "6. **Attention Is All You Need** (Vaswani et al., 2017)\n",
    "   - The Transformer architecture that revolutionized NLP\n",
    "   - https://arxiv.org/abs/1706.03762\n",
    "\n",
    "7. **Sequence-to-Sequence Models Course** (Stanford CS224N)\n",
    "   - Comprehensive lecture notes and videos\n",
    "   - https://web.stanford.edu/class/cs224n/\n",
    "\n",
    "### Practical Resources\n",
    "8. **Hugging Face Transformers Library**\n",
    "   - Pre-trained Seq2Seq models (BART, T5, etc.)\n",
    "   - https://huggingface.co/docs/transformers/\n",
    "\n",
    "Happy learning! See you on Day 60 for Time Series Prediction with Deep Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

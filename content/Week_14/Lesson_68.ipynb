{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Day 68: Attention Mechanisms and Transformers\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to Day 68 of the 100 Days of Machine Learning challenge! Today, we explore one of the most revolutionary breakthroughs in modern machine learning: **Attention Mechanisms** and **Transformers**. These concepts have fundamentally changed how we approach sequence modeling, natural language processing, and even computer vision.\n",
    "\n",
    "Traditional recurrent neural networks (RNNs) and Long Short-Term Memory networks (LSTMs) process sequences step-by-step, which creates bottlenecks in capturing long-range dependencies. Attention mechanisms solve this problem by allowing models to focus on different parts of the input sequence when producing each output, regardless of their position. The Transformer architecture, introduced in the seminal 2017 paper \"Attention Is All You Need,\" takes this concept further by relying entirely on attention mechanisms, eliminating recurrence altogether.\n",
    "\n",
    "### Why Are Attention Mechanisms and Transformers Important?\n",
    "\n",
    "Attention mechanisms and transformers have revolutionized machine learning for several compelling reasons:\n",
    "\n",
    "1. **Long-Range Dependencies**: They can capture relationships between distant elements in a sequence without the degradation that affects RNNs.\n",
    "2. **Parallelization**: Unlike RNNs, transformers can process all positions in a sequence simultaneously, dramatically reducing training time.\n",
    "3. **State-of-the-Art Performance**: Transformers power modern language models like BERT, GPT, and many others that have achieved unprecedented performance across NLP tasks.\n",
    "4. **Versatility**: Beyond NLP, transformers have been successfully applied to computer vision (Vision Transformers), speech recognition, protein folding (AlphaFold), and more.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "- Understand the motivation behind attention mechanisms and how they differ from recurrent architectures\n",
    "- Explain the mathematical foundations of self-attention and scaled dot-product attention\n",
    "- Comprehend the transformer architecture and its key components\n",
    "- Implement a basic attention mechanism from scratch in Python\n",
    "- Work with pre-trained transformer models using the Hugging Face library\n",
    "- Visualize attention patterns to interpret model behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Theory: Understanding Attention Mechanisms\n",
    "\n",
    "### The Motivation for Attention\n",
    "\n",
    "Consider the task of machine translation. When translating \"The cat sat on the mat\" to another language, different words in the output depend on different parts of the input. The word \"cat\" in the output primarily depends on \"cat\" in the input, but also needs context from \"the\" and potentially \"sat.\"\n",
    "\n",
    "In traditional encoder-decoder architectures, the encoder compresses the entire input sequence into a single fixed-size context vector. This creates an information bottleneck, especially for long sequences. **Attention mechanisms** address this by allowing the decoder to \"attend to\" different parts of the input sequence for each output step.\n",
    "\n",
    "### Self-Attention: The Core Concept\n",
    "\n",
    "Self-attention allows a sequence to attend to itself, computing representations that capture relationships between all positions in the sequence. For each position, self-attention computes a weighted sum of all positions, where the weights indicate how much each position should \"pay attention to\" every other position.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "#### Query, Key, and Value\n",
    "\n",
    "Self-attention is built on three concepts:\n",
    "\n",
    "- **Query (Q)**: What am I looking for?\n",
    "- **Key (K)**: What do I contain?\n",
    "- **Value (V)**: What information do I actually hold?\n",
    "\n",
    "For each input element, we create three vectors by multiplying the input with learned weight matrices $W^Q$, $W^K$, and $W^V$:\n",
    "\n",
    "$$Q = XW^Q$$\n",
    "$$K = XW^K$$\n",
    "$$V = XW^V$$\n",
    "\n",
    "where $X$ is the input matrix and each row represents a position in the sequence.\n",
    "\n",
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "The attention mechanism computes attention scores, which determine how much focus to place on other parts of the input when encoding a specific position. The formula for scaled dot-product attention is:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Let's break this down:\n",
    "\n",
    "1. **$QK^T$**: Compute the dot product between queries and keys to get raw attention scores. This measures the similarity between the query and each key.\n",
    "\n",
    "2. **$\\frac{1}{\\sqrt{d_k}}$**: Scale by the square root of the key dimension $d_k$. This prevents the dot products from becoming too large, which would push the softmax into regions with extremely small gradients.\n",
    "\n",
    "3. **$\\text{softmax}$**: Apply softmax to normalize the scores into a probability distribution. Each position gets a weight between 0 and 1, and all weights sum to 1.\n",
    "\n",
    "4. **Multiply by $V$**: Use these attention weights to compute a weighted sum of the value vectors.\n",
    "\n",
    "#### Multi-Head Attention\n",
    "\n",
    "Instead of performing a single attention function, transformers use **multi-head attention**, which runs multiple attention operations in parallel:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$$\n",
    "\n",
    "where each head is:\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$\n",
    "\n",
    "This allows the model to attend to information from different representation subspaces at different positions. For example, one head might focus on syntactic relationships while another focuses on semantic relationships.\n",
    "\n",
    "### The Transformer Architecture\n",
    "\n",
    "The transformer architecture consists of:\n",
    "\n",
    "1. **Encoder**: Processes the input sequence and produces contextualized representations\n",
    "2. **Decoder**: Generates the output sequence using the encoder's representations\n",
    "\n",
    "Each encoder layer contains:\n",
    "- Multi-head self-attention mechanism\n",
    "- Position-wise feed-forward network\n",
    "- Layer normalization and residual connections\n",
    "\n",
    "Each decoder layer contains:\n",
    "- Masked multi-head self-attention (to prevent attending to future positions)\n",
    "- Multi-head attention over encoder outputs\n",
    "- Position-wise feed-forward network\n",
    "- Layer normalization and residual connections\n",
    "\n",
    "#### Positional Encoding\n",
    "\n",
    "Since transformers have no inherent notion of sequence order (unlike RNNs), we add **positional encodings** to the input embeddings. These are typically sinusoidal functions:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
    "\n",
    "where $pos$ is the position and $i$ is the dimension. This allows the model to learn to attend by relative positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Python Implementation\n",
    "\n",
    "Let's implement attention mechanisms from scratch to build intuition, then work with modern transformer libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Libraries imported successfully!\n",
      "PyTorch version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### Implementing Scaled Dot-Product Attention from Scratch\n",
    "\n",
    "Let's build the core attention mechanism step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input shape (Q, K, V): torch.Size([1, 4, 8])\n",
      "Output shape: torch.Size([1, 4, 8])\n",
      "Attention weights shape: torch.Size([1, 4, 4])\n",
      "\n",
      "Attention weights (should sum to 1 across last dimension):\n",
      "[[0.24681453 0.26893723 0.25156406 0.23268418]\n",
      " [0.24450386 0.24914244 0.26014507 0.24620863]\n",
      " [0.23794635 0.25784355 0.25535721 0.24885289]\n",
      " [0.25842696 0.24587208 0.24839835 0.24730261]]\n",
      "\n",
      "Sum of attention weights per row: [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Q : torch.Tensor\n",
    "        Query matrix of shape (batch_size, num_queries, d_k)\n",
    "    K : torch.Tensor\n",
    "        Key matrix of shape (batch_size, num_keys, d_k)\n",
    "    V : torch.Tensor\n",
    "        Value matrix of shape (batch_size, num_keys, d_v)\n",
    "    mask : torch.Tensor, optional\n",
    "        Mask tensor to prevent attention to certain positions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : torch.Tensor\n",
    "        Attention output of shape (batch_size, num_queries, d_v)\n",
    "    attention_weights : torch.Tensor\n",
    "        Attention weights of shape (batch_size, num_queries, num_keys)\n",
    "    \"\"\"\n",
    "    # Get the dimension of keys\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided (set masked positions to large negative value)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Compute weighted sum of values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test the function with a simple example\n",
    "batch_size = 1\n",
    "seq_length = 4\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "\n",
    "# Create random Q, K, V matrices\n",
    "Q = torch.randn(batch_size, seq_length, d_k)\n",
    "K = torch.randn(batch_size, seq_length, d_k)\n",
    "V = torch.randn(batch_size, seq_length, d_v)\n",
    "\n",
    "# Compute attention\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Input shape (Q, K, V): {Q.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nAttention weights (should sum to 1 across last dimension):\")\n",
    "print(attention_weights[0].detach().numpy())\n",
    "print(f\"\\nSum of attention weights per row: {attention_weights[0].sum(dim=-1).detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Visualizing Attention Patterns\n",
    "\n",
    "Let's visualize the attention weights to understand what the mechanism is \"paying attention to\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 2 subplots>"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "def visualize_attention(attention_weights, sentence=None):\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    attention_weights : torch.Tensor or numpy.ndarray\n",
    "        Attention weights of shape (seq_len, seq_len)\n",
    "    sentence : list of str, optional\n",
    "        List of tokens to use as labels\n",
    "    \"\"\"\n",
    "    if isinstance(attention_weights, torch.Tensor):\n",
    "        attention_weights = attention_weights.detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    if sentence is not None:\n",
    "        sns.heatmap(attention_weights, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "                    xticklabels=sentence, yticklabels=sentence,\n",
    "                    cbar_kws={'label': 'Attention Weight'})\n",
    "        plt.xlabel('Key Position', fontsize=12)\n",
    "        plt.ylabel('Query Position', fontsize=12)\n",
    "    else:\n",
    "        sns.heatmap(attention_weights, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "                    cbar_kws={'label': 'Attention Weight'})\n",
    "        plt.xlabel('Key Position (Index)', fontsize=12)\n",
    "        plt.ylabel('Query Position (Index)', fontsize=12)\n",
    "    \n",
    "    plt.title('Attention Weights Heatmap', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a simple example with interpretable attention\n",
    "sentence = ['The', 'cat', 'sat', 'down']\n",
    "seq_len = len(sentence)\n",
    "\n",
    "# Create Q, K, V for this sentence\n",
    "Q_sent = torch.randn(1, seq_len, d_k)\n",
    "K_sent = torch.randn(1, seq_len, d_k)\n",
    "V_sent = torch.randn(1, seq_len, d_v)\n",
    "\n",
    "# Compute attention\n",
    "output_sent, attention_weights_sent = scaled_dot_product_attention(Q_sent, K_sent, V_sent)\n",
    "\n",
    "# Visualize\n",
    "visualize_attention(attention_weights_sent[0], sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "The heatmap shows how much each query position (row) attends to each key position (column). Darker colors indicate higher attention weights. In a real transformer trained on language data, you might see patterns like:\n",
    "- Nouns attending to their articles (\"cat\" attending to \"The\")\n",
    "- Verbs attending to their subjects (\"sat\" attending to \"cat\")\n",
    "- Self-attention (diagonal elements) showing words attending to themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Implementing Multi-Head Attention\n",
    "\n",
    "Now let's implement multi-head attention, which allows the model to attend to different aspects of the input simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n",
      "\n",
      "Multi-head attention implementation successful!\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        d_model : int\n",
    "            Dimensionality of the model (must be divisible by num_heads)\n",
    "        num_heads : int\n",
    "            Number of attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear layers for Q, K, V transformations\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, d_k).\n",
    "        Transpose to get shape (batch_size, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        Q, K, V : torch.Tensor\n",
    "            Input tensors of shape (batch_size, seq_len, d_model)\n",
    "        mask : torch.Tensor, optional\n",
    "            Mask tensor\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : torch.Tensor\n",
    "            Output of shape (batch_size, seq_len, d_model)\n",
    "        attention_weights : torch.Tensor\n",
    "            Attention weights for visualization\n",
    "        \"\"\"\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # Linear transformations and split into heads\n",
    "        Q = self.split_heads(self.W_q(Q), batch_size)\n",
    "        K = self.split_heads(self.W_k(K), batch_size)\n",
    "        V = self.split_heads(self.W_v(V), batch_size)\n",
    "        \n",
    "        # Apply scaled dot-product attention for each head\n",
    "        attention_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        attention_output = attention_output.view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear transformation\n",
    "        output = self.W_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test the multi-head attention module\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_length = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Create sample input\n",
    "sample_input = torch.randn(batch_size, seq_length, d_model)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = mha(sample_input, sample_input, sample_input)\n",
    "\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nMulti-head attention implementation successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Understanding Positional Encoding\n",
    "\n",
    "Since transformers don't have any inherent notion of sequence order, we need to inject positional information. Let's implement and visualize positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1400x600 with 4 subplots>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Positional encoding shape: (100, 128)\n",
      "\n",
      "The sinusoidal positional encodings allow the model to:\n",
      "1. Learn to attend by relative positions\n",
      "2. Extrapolate to sequence lengths longer than those seen during training\n",
      "3. Provide unique encodings for each position\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements positional encoding using sinusoidal functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        d_model : int\n",
    "            Dimensionality of the model\n",
    "        max_len : int\n",
    "            Maximum sequence length\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create a matrix to hold positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        # Create position indices\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create division term for the sinusoidal functions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sin to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cos to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but should be saved with the model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to input embeddings.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Input with added positional encoding\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# Create and visualize positional encodings\n",
    "d_model = 128\n",
    "max_len = 100\n",
    "\n",
    "pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# Get the positional encoding matrix\n",
    "pe_matrix = pos_encoding.pe[0].numpy()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot the full positional encoding matrix\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(pe_matrix, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(label='Value')\n",
    "plt.xlabel('Embedding Dimension', fontsize=12)\n",
    "plt.ylabel('Position', fontsize=12)\n",
    "plt.title('Positional Encoding Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot specific dimensions over positions\n",
    "plt.subplot(1, 2, 2)\n",
    "positions = range(max_len)\n",
    "plt.plot(positions, pe_matrix[:, 4], label='Dimension 4 (sin)', linewidth=2)\n",
    "plt.plot(positions, pe_matrix[:, 5], label='Dimension 5 (cos)', linewidth=2)\n",
    "plt.plot(positions, pe_matrix[:, 8], label='Dimension 8 (sin)', linewidth=2)\n",
    "plt.plot(positions, pe_matrix[:, 9], label='Dimension 9 (cos)', linewidth=2)\n",
    "plt.xlabel('Position', fontsize=12)\n",
    "plt.ylabel('Value', fontsize=12)\n",
    "plt.title('Positional Encoding Values Across Positions', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Positional encoding shape:\", pe_matrix.shape)\n",
    "print(\"\\nThe sinusoidal positional encodings allow the model to:\")\n",
    "print(\"1. Learn to attend by relative positions\")\n",
    "print(\"2. Extrapolate to sequence lengths longer than those seen during training\")\n",
    "print(\"3. Provide unique encodings for each position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Working with Pre-trained Transformers\n",
    "\n",
    "Now that we understand the mechanics, let's work with real transformer models using the Hugging Face Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Transformers library not available. Skipping this section.\n",
      "To install: pip install transformers\n"
     ]
    }
   ],
   "source": [
    "# Note: You may need to install transformers first\n",
    "# !pip install transformers\n",
    "\n",
    "try:\n",
    "    from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
    "    print(\"Transformers library loaded successfully!\")\n",
    "    transformers_available = True\n",
    "except ImportError:\n",
    "    print(\"Transformers library not available. Skipping this section.\")\n",
    "    print(\"To install: pip install transformers\")\n",
    "    transformers_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping section - transformers library not available\n"
     ]
    }
   ],
   "source": [
    "if transformers_available:\n",
    "    # Load a pre-trained BERT model and tokenizer\n",
    "    print(\"Loading BERT model and tokenizer...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "    \n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    \n",
    "    # Example sentence\n",
    "    sentence = \"The transformer architecture revolutionized natural language processing.\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    \n",
    "    print(f\"\\nOriginal sentence: {sentence}\")\n",
    "    print(f\"Tokenized: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "    print(f\"Token IDs shape: {inputs['input_ids'].shape}\")\n",
    "    \n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract outputs\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    attentions = outputs.attentions\n",
    "    \n",
    "    print(f\"\\nOutput shape (last hidden state): {last_hidden_state.shape}\")\n",
    "    print(f\"Number of attention layers: {len(attentions)}\")\n",
    "    print(f\"Attention shape (layer 0): {attentions[0].shape}\")\n",
    "    print(f\"  - Batch size: {attentions[0].shape[0]}\")\n",
    "    print(f\"  - Number of heads: {attentions[0].shape[1]}\")\n",
    "    print(f\"  - Sequence length: {attentions[0].shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Visualizing BERT's Attention\n",
    "\n",
    "Let's visualize the attention patterns from different heads and layers of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping section - transformers library not available\n"
     ]
    }
   ],
   "source": [
    "if transformers_available:\n",
    "    def visualize_bert_attention(attentions, tokens, layer=0, head=0):\n",
    "        \"\"\"\n",
    "        Visualize attention weights from a specific layer and head.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        attentions : tuple of torch.Tensor\n",
    "            Attention tensors from BERT model\n",
    "        tokens : list of str\n",
    "            List of tokens\n",
    "        layer : int\n",
    "            Which layer to visualize\n",
    "        head : int\n",
    "            Which attention head to visualize\n",
    "        \"\"\"\n",
    "        # Get attention weights for specified layer and head\n",
    "        attention = attentions[layer][0, head].detach().numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens,\n",
    "                    cmap='viridis', cbar_kws={'label': 'Attention Weight'})\n",
    "        plt.xlabel('Key Tokens', fontsize=12)\n",
    "        plt.ylabel('Query Tokens', fontsize=12)\n",
    "        plt.title(f'BERT Attention Weights - Layer {layer}, Head {head}',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Visualize attention from first layer, first head\n",
    "    print(\"Attention patterns from BERT (Layer 0, Head 0):\")\n",
    "    visualize_bert_attention(attentions, tokens, layer=0, head=0)\n",
    "    \n",
    "    # Visualize attention from a middle layer\n",
    "    print(\"\\nAttention patterns from BERT (Layer 6, Head 3):\")\n",
    "    visualize_bert_attention(attentions, tokens, layer=6, head=3)\n",
    "    \n",
    "    print(\"\\nNotice how different layers and heads learn different patterns!\")\n",
    "    print(\"Early layers often attend to nearby tokens (local patterns)\")\n",
    "    print(\"Later layers often attend to semantically related tokens (global patterns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Hands-On Activity: Sentence Similarity with Transformers\n",
    "\n",
    "Let's use transformer embeddings to compute semantic similarity between sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping section - transformers library not available\n"
     ]
    }
   ],
   "source": [
    "if transformers_available:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    def get_sentence_embedding(sentence, tokenizer, model):\n",
    "        \"\"\"\n",
    "        Get sentence embedding by averaging token embeddings.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        sentence : str\n",
    "            Input sentence\n",
    "        tokenizer : transformers.PreTrainedTokenizer\n",
    "            Tokenizer\n",
    "        model : transformers.PreTrainedModel\n",
    "            Pre-trained transformer model\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Sentence embedding vector\n",
    "        \"\"\"\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "        \n",
    "        # Get model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Average the token embeddings (excluding [CLS] and [SEP])\n",
    "        embeddings = outputs.last_hidden_state[0, 1:-1, :].mean(dim=0)\n",
    "        \n",
    "        return embeddings.numpy()\n",
    "    \n",
    "    # Example sentences\n",
    "    sentences = [\n",
    "        \"The cat sat on the mat.\",\n",
    "        \"A feline rested on the rug.\",\n",
    "        \"The dog barked loudly.\",\n",
    "        \"Machine learning is fascinating.\",\n",
    "        \"Deep learning models are powerful.\"\n",
    "    ]\n",
    "    \n",
    "    # Get embeddings for all sentences\n",
    "    embeddings = []\n",
    "    for sent in sentences:\n",
    "        emb = get_sentence_embedding(sent, tokenizer, model)\n",
    "        embeddings.append(emb)\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Compute pairwise cosine similarities\n",
    "    similarities = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Visualize similarity matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarities, annot=True, fmt='.3f', cmap='coolwarm',\n",
    "                xticklabels=[f\"S{i+1}\" for i in range(len(sentences))],\n",
    "                yticklabels=[f\"S{i+1}\" for i in range(len(sentences))],\n",
    "                vmin=0, vmax=1, cbar_kws={'label': 'Cosine Similarity'})\n",
    "    plt.title('Sentence Similarity Matrix Using BERT Embeddings',\n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print sentences with their labels\n",
    "    print(\"\\nSentences:\")\n",
    "    for i, sent in enumerate(sentences):\n",
    "        print(f\"S{i+1}: {sent}\")\n",
    "    \n",
    "    print(\"\\nObservations:\")\n",
    "    print(\"- S1 and S2 have high similarity (semantically equivalent)\")\n",
    "    print(\"- S4 and S5 have moderate similarity (both about ML/DL)\")\n",
    "    print(\"- S3 is least similar to S4/S5 (different topics)\")\n",
    "else:\n",
    "    print(\"Skipping activity - transformers library not available\")\n",
    "    print(\"\\nThis activity demonstrates using BERT embeddings to compute semantic similarity.\")\n",
    "    print(\"Semantically similar sentences (even with different words) will have high similarity scores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Comparing Attention Mechanisms\n",
    "\n",
    "Let's create a visual comparison of how attention differs from traditional sequence processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1600x600 with 2 subplots>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Key Differences:\n",
      "\n",
      "RNN (Left):\n",
      "  - Processes sequentially (one step at a time)\n",
      "  - Information flows left-to-right through hidden states\n",
      "  - Cannot be easily parallelized\n",
      "  - Struggles with long-range dependencies\n",
      "\n",
      "Transformer (Right):\n",
      "  - Processes all positions in parallel\n",
      "  - Each position attends to all positions (blue arrows)\n",
      "  - Highly parallelizable\n",
      "  - Captures long-range dependencies directly\n"
     ]
    }
   ],
   "source": [
    "# Create a visualization comparing RNN vs Transformer processing\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# RNN sequential processing\n",
    "ax1 = axes[0]\n",
    "positions = range(5)\n",
    "for i in positions:\n",
    "    # Draw hidden states\n",
    "    ax1.add_patch(plt.Circle((i, 0.5), 0.3, color='lightblue', ec='black', linewidth=2))\n",
    "    ax1.text(i, 0.5, f'h{i}', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Draw inputs\n",
    "    ax1.add_patch(plt.Circle((i, -0.5), 0.2, color='lightgreen', ec='black', linewidth=2))\n",
    "    ax1.text(i, -0.5, f'x{i}', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Draw arrows from input to hidden\n",
    "    ax1.arrow(i, -0.3, 0, 0.5, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "    \n",
    "    # Draw sequential connections\n",
    "    if i < len(positions) - 1:\n",
    "        ax1.arrow(i+0.3, 0.5, 0.4, 0, head_width=0.1, head_length=0.1,\n",
    "                  fc='red', ec='red', linewidth=2)\n",
    "\n",
    "ax1.set_xlim(-0.5, 4.5)\n",
    "ax1.set_ylim(-1, 1.5)\n",
    "ax1.axis('off')\n",
    "ax1.set_title('RNN: Sequential Processing\\n(Each step depends on previous step)',\n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Transformer parallel processing with attention\n",
    "ax2 = axes[1]\n",
    "for i in positions:\n",
    "    # Draw hidden states\n",
    "    ax2.add_patch(plt.Circle((i, 0.5), 0.3, color='lightcoral', ec='black', linewidth=2))\n",
    "    ax2.text(i, 0.5, f'z{i}', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Draw inputs\n",
    "    ax2.add_patch(plt.Circle((i, -0.5), 0.2, color='lightgreen', ec='black', linewidth=2))\n",
    "    ax2.text(i, -0.5, f'x{i}', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Draw attention connections (each position attends to all others)\n",
    "    for j in positions:\n",
    "        if i != j:\n",
    "            # Draw lighter arrows for attention connections\n",
    "            ax2.annotate('', xy=(j, -0.3), xytext=(i, 0.2),\n",
    "                        arrowprops=dict(arrowstyle='->', color='blue', alpha=0.2, lw=1))\n",
    "    \n",
    "    # Draw direct connection from input to output\n",
    "    ax2.arrow(i, -0.3, 0, 0.5, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "ax2.set_xlim(-0.5, 4.5)\n",
    "ax2.set_ylim(-1, 1.5)\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Transformer: Parallel Processing with Attention\\n(Each position attends to all positions)',\n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Differences:\")\n",
    "print(\"\\nRNN (Left):\")\n",
    "print(\"  - Processes sequentially (one step at a time)\")\n",
    "print(\"  - Information flows left-to-right through hidden states\")\n",
    "print(\"  - Cannot be easily parallelized\")\n",
    "print(\"  - Struggles with long-range dependencies\")\n",
    "print(\"\\nTransformer (Right):\")\n",
    "print(\"  - Processes all positions in parallel\")\n",
    "print(\"  - Each position attends to all positions (blue arrows)\")\n",
    "print(\"  - Highly parallelizable\")\n",
    "print(\"  - Captures long-range dependencies directly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Congratulations! You've learned about attention mechanisms and transformers, two of the most important innovations in modern machine learning. Let's summarize the key points:\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "1. **Attention Mechanisms** allow models to focus on relevant parts of the input when producing each output, solving the information bottleneck problem in traditional encoder-decoder architectures.\n",
    "\n",
    "2. **Self-Attention** computes representations by allowing each position in a sequence to attend to all positions, creating contextualized representations that capture relationships across the entire sequence.\n",
    "\n",
    "3. **Scaled Dot-Product Attention** uses queries, keys, and values to compute attention weights: $\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
    "\n",
    "4. **Multi-Head Attention** runs multiple attention functions in parallel, allowing the model to attend to different aspects of the input simultaneously.\n",
    "\n",
    "5. **Transformers** rely entirely on attention mechanisms, eliminating recurrence and enabling parallel processing of sequences.\n",
    "\n",
    "### Practical Skills\n",
    "\n",
    "You can now:\n",
    "- Implement scaled dot-product attention from scratch\n",
    "- Build multi-head attention mechanisms\n",
    "- Understand and implement positional encodings\n",
    "- Work with pre-trained transformer models using Hugging Face\n",
    "- Visualize and interpret attention patterns\n",
    "- Use transformer embeddings for downstream tasks\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Transformers have become the foundation of modern AI:\n",
    "- **Language Models**: GPT, BERT, T5, and countless others\n",
    "- **Computer Vision**: Vision Transformers (ViT), DETR\n",
    "- **Multimodal AI**: CLIP, DALL-E, Flamingo\n",
    "- **Scientific Applications**: AlphaFold for protein structure prediction\n",
    "\n",
    "Understanding transformers is essential for working with state-of-the-art AI systems and staying current in the rapidly evolving field of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "To deepen your understanding of attention mechanisms and transformers, explore these resources:\n",
    "\n",
    "### Essential Papers\n",
    "\n",
    "1. **\"Attention Is All You Need\" (Vaswani et al., 2017)**\n",
    "   - The original transformer paper\n",
    "   - https://arxiv.org/abs/1706.03762\n",
    "\n",
    "2. **\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (Devlin et al., 2018)**\n",
    "   - Introduced bidirectional pre-training\n",
    "   - https://arxiv.org/abs/1810.04805\n",
    "\n",
    "3. **\"Language Models are Few-Shot Learners\" (Brown et al., 2020)**\n",
    "   - The GPT-3 paper\n",
    "   - https://arxiv.org/abs/2005.14165\n",
    "\n",
    "### Interactive Tutorials and Visualizations\n",
    "\n",
    "4. **The Illustrated Transformer by Jay Alammar**\n",
    "   - Excellent visual guide to transformers\n",
    "   - https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "5. **The Annotated Transformer**\n",
    "   - Line-by-line implementation with explanations\n",
    "   - http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "\n",
    "### Documentation and Libraries\n",
    "\n",
    "6. **Hugging Face Transformers Documentation**\n",
    "   - Comprehensive library for working with transformers\n",
    "   - https://huggingface.co/docs/transformers/\n",
    "\n",
    "7. **PyTorch Transformer Tutorial**\n",
    "   - Official PyTorch tutorial on transformers\n",
    "   - https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "### Advanced Topics\n",
    "\n",
    "8. **\"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" (Dosovitskiy et al., 2020)**\n",
    "   - Vision Transformers\n",
    "   - https://arxiv.org/abs/2010.11929\n",
    "\n",
    "9. **\"Formal Algorithms for Transformers\" (Phuong & Hutter, 2022)**\n",
    "   - Mathematical treatment of transformer architectures\n",
    "   - https://arxiv.org/abs/2207.09238\n",
    "\n",
    "### Practice and Experimentation\n",
    "\n",
    "- Experiment with different attention head visualizations in BERT\n",
    "- Try fine-tuning a pre-trained transformer on your own dataset\n",
    "- Implement variants like relative positional encoding\n",
    "- Explore transformer applications beyond NLP (e.g., time series, graphs)\n",
    "\n",
    "Keep learning and experimenting with these powerful architectures!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 69: Autoencoders and Their Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Autoencoders\n",
    "\n",
    "Autoencoders are a special type of neural network architecture designed to learn efficient representations of data, typically for dimensionality reduction. Unlike traditional supervised learning models that learn to predict labels, autoencoders are trained to reconstruct their input data. This self-supervised learning approach makes them incredibly versatile for various applications.\n",
    "\n",
    "At their core, autoencoders compress input data into a lower-dimensional representation (called the latent space or encoding) and then reconstruct the original input from this compressed representation. The network learns to preserve the most important features of the data while discarding noise and irrelevant details.\n",
    "\n",
    "### Why Autoencoders Matter\n",
    "\n",
    "Autoencoders have become increasingly important in machine learning for several reasons:\n",
    "\n",
    "1. **Unsupervised Learning**: They can learn meaningful representations without labeled data\n",
    "2. **Dimensionality Reduction**: They provide a non-linear alternative to techniques like PCA\n",
    "3. **Feature Learning**: They automatically discover useful features in complex data\n",
    "4. **Generative Modeling**: Variants like VAEs can generate new, realistic data samples\n",
    "5. **Anomaly Detection**: They excel at identifying unusual patterns in data\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "- Understand the architecture and components of autoencoders\n",
    "- Explain the mathematical concepts behind reconstruction loss and latent space\n",
    "- Implement basic autoencoders using TensorFlow/Keras\n",
    "- Apply autoencoders to real-world problems like image denoising and dimensionality reduction\n",
    "- Distinguish between different types of autoencoders and their use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of Autoencoders\n",
    "\n",
    "An autoencoder consists of three main components:\n",
    "\n",
    "### 1. Encoder\n",
    "The encoder compresses the input $\\mathbf{x}$ into a latent space representation $\\mathbf{z}$. Mathematically:\n",
    "\n",
    "$$\\mathbf{z} = f_{\\theta}(\\mathbf{x})$$\n",
    "\n",
    "where $f_{\\theta}$ is the encoding function parameterized by weights $\\theta$.\n",
    "\n",
    "### 2. Latent Space (Bottleneck)\n",
    "The latent space is a compressed, lower-dimensional representation of the input. This bottleneck forces the network to learn the most salient features. If the input has dimension $n$ and the latent space has dimension $m$ where $m < n$, the autoencoder must learn to compress information efficiently.\n",
    "\n",
    "### 3. Decoder\n",
    "The decoder reconstructs the input from the latent representation:\n",
    "\n",
    "$$\\mathbf{\\hat{x}} = g_{\\phi}(\\mathbf{z})$$\n",
    "\n",
    "where $g_{\\phi}$ is the decoding function parameterized by weights $\\phi$, and $\\mathbf{\\hat{x}}$ is the reconstructed input.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Reconstruction Loss\n",
    "\n",
    "The primary objective of an autoencoder is to minimize the reconstruction loss, which measures how well the output matches the input. For continuous data, we typically use Mean Squared Error (MSE):\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{x}, \\mathbf{\\hat{x}}) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{x}_i)^2$$\n",
    "\n",
    "For binary data (like black and white images), we often use Binary Cross-Entropy:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{x}, \\mathbf{\\hat{x}}) = -\\frac{1}{n} \\sum_{i=1}^{n} [x_i \\log(\\hat{x}_i) + (1-x_i) \\log(1-\\hat{x}_i)]$$\n",
    "\n",
    "### Complete Objective Function\n",
    "\n",
    "The complete training objective is:\n",
    "\n",
    "$$\\min_{\\theta, \\phi} \\mathbb{E}_{\\mathbf{x} \\sim p_{data}}[\\mathcal{L}(\\mathbf{x}, g_{\\phi}(f_{\\theta}(\\mathbf{x})))]$$\n",
    "\n",
    "This means we want to find encoder parameters $\\theta$ and decoder parameters $\\phi$ that minimize the expected reconstruction loss across the entire dataset.\n",
    "\n",
    "### Latent Space Properties\n",
    "\n",
    "The latent space representation $\\mathbf{z}$ has several important properties:\n",
    "\n",
    "1. **Dimensionality**: Typically $\\dim(\\mathbf{z}) \\ll \\dim(\\mathbf{x})$\n",
    "2. **Information Preservation**: Despite lower dimensionality, $\\mathbf{z}$ must capture sufficient information to reconstruct $\\mathbf{x}$\n",
    "3. **Feature Disentanglement**: In well-trained autoencoders, different dimensions of $\\mathbf{z}$ may correspond to different features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Autoencoders\n",
    "\n",
    "There are several variants of autoencoders, each designed for specific purposes:\n",
    "\n",
    "### 1. Vanilla (Basic) Autoencoder\n",
    "The simplest form with fully connected layers. Good for learning basic representations.\n",
    "\n",
    "### 2. Denoising Autoencoder (DAE)\n",
    "Trained to reconstruct clean data from corrupted inputs. The loss function becomes:\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{\\mathbf{x} \\sim p_{data}, \\tilde{\\mathbf{x}} \\sim p_{corrupt}}[\\|\\mathbf{x} - g_{\\phi}(f_{\\theta}(\\tilde{\\mathbf{x}}))\\|^2]$$\n",
    "\n",
    "where $\\tilde{\\mathbf{x}}$ is the corrupted version of $\\mathbf{x}$.\n",
    "\n",
    "### 3. Sparse Autoencoder\n",
    "Includes a sparsity constraint on the latent representation:\n",
    "\n",
    "$$\\mathcal{L} = \\mathcal{L}_{reconstruction} + \\lambda \\sum_{j=1}^{m} |z_j|$$\n",
    "\n",
    "where $\\lambda$ controls the sparsity penalty.\n",
    "\n",
    "### 4. Variational Autoencoder (VAE)\n",
    "A probabilistic approach that learns a distribution over the latent space. The loss includes a KL divergence term:\n",
    "\n",
    "$$\\mathcal{L} = \\mathcal{L}_{reconstruction} + \\beta \\cdot KL(q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z}))$$\n",
    "\n",
    "### 5. Convolutional Autoencoder\n",
    "Uses convolutional layers instead of fully connected layers, making it ideal for image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation\n",
    "\n",
    "Let's implement different types of autoencoders using TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n",
      "NumPy version: 1.23.5\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess the MNIST Dataset\n",
    "\n",
    "We'll use the MNIST dataset of handwritten digits for our examples. This is a standard benchmark dataset in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 784)\n",
      "Test data shape: (10000, 784)\n",
      "Value range: [0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Flatten the images from 28x28 to 784\n",
    "x_train_flat = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test_flat = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "print(f\"Training data shape: {x_train_flat.shape}\")\n",
    "print(f\"Test data shape: {x_test_flat.shape}\")\n",
    "print(f\"Value range: [{x_train_flat.min()}, {x_train_flat.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Basic (Vanilla) Autoencoder\n",
    "\n",
    "Let's build a simple autoencoder with fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 784)               101136    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 222,384\n",
      "Trainable params: 222,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the encoding dimension (size of the latent space)\n",
    "encoding_dim = 32\n",
    "input_dim = 784  # 28 * 28\n",
    "\n",
    "# Build the autoencoder model\n",
    "def build_autoencoder(input_dim, encoding_dim):\n",
    "    # Encoder\n",
    "    encoder_input = layers.Input(shape=(input_dim,))\n",
    "    encoded = layers.Dense(128, activation='relu')(encoder_input)\n",
    "    encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "    encoded = layers.Dense(encoding_dim, activation='relu')(encoded)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "    decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "    decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    \n",
    "    # Autoencoder model\n",
    "    autoencoder = Model(encoder_input, decoded, name='autoencoder')\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Create the model\n",
    "autoencoder = build_autoencoder(input_dim, encoding_dim)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mse'])\n",
    "\n",
    "# Display model architecture\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Basic Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.2156 - mse: 0.0463 - val_loss: 0.1392 - val_mse: 0.0281\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1217 - mse: 0.0237 - val_loss: 0.1077 - val_mse: 0.0204\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1009 - mse: 0.0190 - val_loss: 0.0950 - val_mse: 0.0177\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0916 - mse: 0.0170 - val_loss: 0.0881 - val_mse: 0.0163\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0862 - mse: 0.0159 - val_loss: 0.0839 - val_mse: 0.0154\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0780 - mse: 0.0143 - val_loss: 0.0772 - val_mse: 0.0141\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0752 - mse: 0.0137 - val_loss: 0.0748 - val_mse: 0.0136\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0735 - mse: 0.0134 - val_loss: 0.0733 - val_mse: 0.0133\n",
      "\n",
      "Training completed!\n",
      "Final training loss: 0.0735\n",
      "Final validation loss: 0.0733\n"
     ]
    }
   ],
   "source": [
    "# Train the autoencoder\n",
    "history = autoencoder.fit(\n",
    "    x_train_flat, x_train_flat,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_test_flat, x_test_flat),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Autoencoder Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (Binary Cross-Entropy)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# MSE plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mse'], label='Training MSE')\n",
    "plt.plot(history.history['val_mse'], label='Validation MSE')\n",
    "plt.title('Mean Squared Error Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Reconstructions\n",
    "\n",
    "Let's see how well the autoencoder reconstructs the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate reconstructions\n",
    "n_images = 10\n",
    "reconstructed = autoencoder.predict(x_test_flat[:n_images])\n",
    "\n",
    "# Visualize original and reconstructed images\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n_images):\n",
    "    # Original images\n",
    "    ax = plt.subplot(2, n_images, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Reconstructed images\n",
    "    ax = plt.subplot(2, n_images, i + 1 + n_images)\n",
    "    plt.imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title('Reconstructed')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Original vs Reconstructed Images', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Denoising Autoencoder\n",
    "\n",
    "A denoising autoencoder learns to remove noise from data. We'll add random noise to MNIST images and train the model to reconstruct clean images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise factor: 0.5\n",
      "Noisy data shape: (60000, 784)\n",
      "Clean data shape: (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Add noise to the data\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train_flat + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train_flat.shape)\n",
    "x_test_noisy = x_test_flat + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test_flat.shape)\n",
    "\n",
    "# Clip values to [0, 1] range\n",
    "x_train_noisy = np.clip(x_train_noisy, 0.0, 1.0)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0.0, 1.0)\n",
    "\n",
    "print(f\"Noise factor: {noise_factor}\")\n",
    "print(f\"Noisy data shape: {x_train_noisy.shape}\")\n",
    "print(f\"Clean data shape: {x_train_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.2748 - mse: 0.0608 - val_loss: 0.1891 - val_mse: 0.0389\n",
      "Epoch 5/15\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1542 - mse: 0.0304 - val_loss: 0.1443 - val_mse: 0.0280\n",
      "Epoch 10/15\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1368 - mse: 0.0263 - val_loss: 0.1329 - val_mse: 0.0254\n",
      "Epoch 15/15\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1295 - mse: 0.0247 - val_loss: 0.1273 - val_mse: 0.0242\n"
     ]
    }
   ],
   "source": [
    "# Build and compile denoising autoencoder\n",
    "denoising_autoencoder = build_autoencoder(input_dim, encoding_dim)\n",
    "denoising_autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mse'])\n",
    "\n",
    "# Train on noisy data to predict clean data\n",
    "denoising_history = denoising_autoencoder.fit(\n",
    "    x_train_noisy, x_train_flat,  # Input noisy, output clean\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_test_noisy, x_test_flat)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Denoising Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate denoised images\n",
    "n_images = 10\n",
    "denoised = denoising_autoencoder.predict(x_test_noisy[:n_images])\n",
    "\n",
    "# Visualize noisy, denoised, and original images\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(n_images):\n",
    "    # Noisy images\n",
    "    ax = plt.subplot(3, n_images, i + 1)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title('Noisy')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Denoised images\n",
    "    ax = plt.subplot(3, n_images, i + 1 + n_images)\n",
    "    plt.imshow(denoised[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title('Denoised')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Original clean images\n",
    "    ax = plt.subplot(3, n_images, i + 1 + 2*n_images)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Denoising Autoencoder: Noisy → Denoised → Original', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Convolutional Autoencoder\n",
    "\n",
    "Convolutional autoencoders are better suited for image data because they preserve spatial structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv_autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D (None, 14, 14, 32)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 16)        4624      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling (None, 7, 7, 16)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 8)           1160      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 7, 7, 16)          1168      \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D (None, 14, 14, 16)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 32)        4640      \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling (None, 28, 28, 32)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 1)         289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Reshape data for convolutional layers\n",
    "x_train_conv = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test_conv = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "def build_conv_autoencoder():\n",
    "    # Encoder\n",
    "    encoder_input = layers.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = x  # Shape: (7, 7, 8)\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    # Autoencoder model\n",
    "    conv_autoencoder = Model(encoder_input, decoded, name='conv_autoencoder')\n",
    "    \n",
    "    return conv_autoencoder\n",
    "\n",
    "# Build and compile\n",
    "conv_autoencoder = build_conv_autoencoder()\n",
    "conv_autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mse'])\n",
    "\n",
    "conv_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 15s 30ms/step - loss: 0.1892 - mse: 0.0395 - val_loss: 0.1124 - val_mse: 0.0217\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 13s 28ms/step - loss: 0.0884 - mse: 0.0163 - val_loss: 0.0831 - val_mse: 0.0152\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 13s 28ms/step - loss: 0.0795 - mse: 0.0145 - val_loss: 0.0772 - val_mse: 0.0140\n"
     ]
    }
   ],
   "source": [
    "# Train the convolutional autoencoder\n",
    "conv_history = conv_autoencoder.fit(\n",
    "    x_train_conv, x_train_conv,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_test_conv, x_test_conv)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Latent Space Visualization\n",
    "\n",
    "One powerful application of autoencoders is visualizing high-dimensional data in a lower-dimensional latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder model created\n",
      "Latent representations shape: (10000, 32)\n"
     ]
    }
   ],
   "source": [
    "# Create an encoder model (first part of autoencoder)\n",
    "encoder_input = autoencoder.input\n",
    "encoder_output = autoencoder.layers[3].output  # Get the encoding layer\n",
    "encoder_model = Model(encoder_input, encoder_output, name='encoder')\n",
    "\n",
    "# Encode test data\n",
    "encoded_imgs = encoder_model.predict(x_test_flat)\n",
    "\n",
    "print(\"Encoder model created\")\n",
    "print(f\"Latent representations shape: {encoded_imgs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize latent space using first 2 dimensions\n",
    "(_, y_test) = mnist.load_data()[1]  # Get labels\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], \n",
    "                     c=y_test, cmap='tab10', alpha=0.6, s=10)\n",
    "plt.colorbar(scatter, label='Digit Class')\n",
    "plt.title('2D Latent Space Representation of MNIST Digits', fontsize=14)\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Anomaly Detection\n",
    "\n",
    "Autoencoders can detect anomalies by identifying data points that have high reconstruction error. The intuition is that the autoencoder learns to reconstruct normal patterns well, but struggles with anomalous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction errors calculated\n",
      "Mean reconstruction error: 0.0133\n",
      "Std reconstruction error: 0.0042\n",
      "Anomaly threshold (mean + 2*std): 0.0217\n",
      "Number of detected anomalies: 228 (2.28% of test set)\n"
     ]
    }
   ],
   "source": [
    "# Calculate reconstruction error for each test sample\n",
    "reconstructions = autoencoder.predict(x_test_flat)\n",
    "reconstruction_errors = np.mean(np.square(x_test_flat - reconstructions), axis=1)\n",
    "\n",
    "# Set threshold for anomaly detection (e.g., mean + 2*std)\n",
    "threshold = np.mean(reconstruction_errors) + 2 * np.std(reconstruction_errors)\n",
    "\n",
    "# Identify anomalies\n",
    "anomalies = reconstruction_errors > threshold\n",
    "\n",
    "print(\"Reconstruction errors calculated\")\n",
    "print(f\"Mean reconstruction error: {np.mean(reconstruction_errors):.4f}\")\n",
    "print(f\"Std reconstruction error: {np.std(reconstruction_errors):.4f}\")\n",
    "print(f\"Anomaly threshold (mean + 2*std): {threshold:.4f}\")\n",
    "print(f\"Number of detected anomalies: {np.sum(anomalies)} ({100*np.mean(anomalies):.2f}% of test set)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize reconstruction error distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(reconstruction_errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold = {threshold:.4f}')\n",
    "plt.xlabel('Reconstruction Error (MSE)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show some detected anomalies\n",
    "plt.subplot(1, 2, 2)\n",
    "anomaly_indices = np.where(anomalies)[0][:5]\n",
    "for idx, anomaly_idx in enumerate(anomaly_indices):\n",
    "    plt.subplot(2, 5, idx + 1)\n",
    "    plt.imshow(x_test[anomaly_idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'Error: {reconstruction_errors[anomaly_idx]:.4f}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 5, idx + 6)\n",
    "    plt.imshow(reconstructions[anomaly_idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title('Reconstruction')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Detected Anomalies and Their Reconstructions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Exercise: Build Your Own Autoencoder\n",
    "\n",
    "Now it's your turn! Try modifying the autoencoder architecture and experimenting with different configurations.\n",
    "\n",
    "### Exercise Tasks:\n",
    "\n",
    "1. **Experiment with latent dimension size**: Try encoding_dim values of 8, 16, 64, and 128. How does this affect reconstruction quality?\n",
    "\n",
    "2. **Try a different dataset**: Load Fashion MNIST instead of MNIST and train an autoencoder on it.\n",
    "\n",
    "3. **Build a sparse autoencoder**: Add L1 regularization to encourage sparsity in the latent representation.\n",
    "\n",
    "4. **Create a deep autoencoder**: Add more layers to both encoder and decoder.\n",
    "\n",
    "Below is starter code for you to modify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Experiment with different encoding dimensions\n",
    "def experiment_with_encoding_dim(encoding_dims):\n",
    "    \"\"\"\n",
    "    Train autoencoders with different latent dimensions and compare results.\n",
    "    \n",
    "    Args:\n",
    "        encoding_dims: List of encoding dimensions to try (e.g., [8, 16, 32, 64])\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for dim in encoding_dims:\n",
    "        # TODO: Build autoencoder with this encoding dimension\n",
    "        # TODO: Train it on the data\n",
    "        # TODO: Calculate and store the final validation loss\n",
    "        pass\n",
    "    \n",
    "    # TODO: Plot comparison of results\n",
    "    return results\n",
    "\n",
    "# Uncomment to run:\n",
    "# experiment_with_encoding_dim([8, 16, 32, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Try Fashion MNIST dataset\n",
    "def train_on_fashion_mnist():\n",
    "    \"\"\"\n",
    "    Load Fashion MNIST and train an autoencoder on it.\n",
    "    Fashion MNIST contains images of clothing items instead of digits.\n",
    "    \"\"\"\n",
    "    # TODO: Load Fashion MNIST dataset\n",
    "    # (x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
    "    \n",
    "    # TODO: Preprocess the data\n",
    "    \n",
    "    # TODO: Build and train an autoencoder\n",
    "    \n",
    "    # TODO: Visualize results\n",
    "    pass\n",
    "\n",
    "# Uncomment to run:\n",
    "# train_on_fashion_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Build a sparse autoencoder\n",
    "def build_sparse_autoencoder(input_dim, encoding_dim, sparsity_weight=1e-5):\n",
    "    \"\"\"\n",
    "    Build an autoencoder with L1 regularization for sparsity.\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Input dimension\n",
    "        encoding_dim: Latent space dimension\n",
    "        sparsity_weight: Weight for L1 regularization\n",
    "    \"\"\"\n",
    "    # TODO: Add activity_regularizer=keras.regularizers.l1(sparsity_weight)\n",
    "    # to the encoding layer\n",
    "    \n",
    "    # Hint: Use layers.Dense(..., activity_regularizer=...)\n",
    "    pass\n",
    "\n",
    "# Uncomment to run:\n",
    "# sparse_ae = build_sparse_autoencoder(784, 32)\n",
    "# Train and compare with regular autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Congratulations on completing this lesson on autoencoders! Here are the key points to remember:\n",
    "\n",
    "1. **Autoencoders learn efficient data representations**: By compressing data through a bottleneck, they learn the most salient features automatically.\n",
    "\n",
    "2. **Reconstruction loss is the core objective**: Minimizing the difference between input and output forces the model to learn meaningful representations.\n",
    "\n",
    "3. **The latent space captures essential information**: Despite being lower-dimensional, the latent representation preserves enough information to reconstruct the original data.\n",
    "\n",
    "4. **Different variants serve different purposes**:\n",
    "   - Basic autoencoders: Dimensionality reduction and feature learning\n",
    "   - Denoising autoencoders: Noise removal and robust feature learning\n",
    "   - Sparse autoencoders: Learning interpretable features\n",
    "   - Variational autoencoders: Generative modeling\n",
    "   - Convolutional autoencoders: Image-specific tasks\n",
    "\n",
    "5. **Practical applications abound**:\n",
    "   - Anomaly detection (via reconstruction error)\n",
    "   - Image denoising and restoration\n",
    "   - Dimensionality reduction and visualization\n",
    "   - Feature extraction for downstream tasks\n",
    "   - Data compression\n",
    "\n",
    "6. **Architecture choices matter**: The size of the latent space, number of layers, and activation functions all significantly impact performance.\n",
    "\n",
    "7. **Autoencoders are unsupervised**: They don't require labeled data, making them valuable for exploratory data analysis and pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "To deepen your understanding of autoencoders, explore these resources:\n",
    "\n",
    "### Official Tutorials and Documentation\n",
    "\n",
    "1. **TensorFlow's Autoencoder Tutorial**: [https://www.tensorflow.org/tutorials/generative/autoencoder](https://www.tensorflow.org/tutorials/generative/autoencoder)\n",
    "   - Official TensorFlow tutorial covering basic autoencoders and image denoising\n",
    "   - Includes code examples and explanations\n",
    "\n",
    "2. **Keras Blog - Building Autoencoders in Keras**: [https://blog.keras.io/building-autoencoders-in-keras.html](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "   - Comprehensive guide by the Keras team\n",
    "   - Covers multiple autoencoder variants with practical examples\n",
    "\n",
    "### Academic Papers and Theory\n",
    "\n",
    "3. **\"Reducing the Dimensionality of Data with Neural Networks\"** by Hinton & Salakhutdinov (2006)\n",
    "   - Seminal paper on deep autoencoders\n",
    "   - Available at: [https://www.science.org/doi/10.1126/science.1127647](https://www.science.org/doi/10.1126/science.1127647)\n",
    "\n",
    "4. **\"Auto-Encoding Variational Bayes\"** by Kingma & Welling (2013)\n",
    "   - Introduces Variational Autoencoders (VAEs)\n",
    "   - ArXiv: [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)\n",
    "\n",
    "### Practical Implementations\n",
    "\n",
    "5. **PyTorch Autoencoder Examples**: [https://github.com/pytorch/examples/tree/master/vae](https://github.com/pytorch/examples/tree/master/vae)\n",
    "   - If you're interested in PyTorch implementations\n",
    "   - Includes VAE examples\n",
    "\n",
    "### Advanced Topics\n",
    "\n",
    "6. **Denoising Autoencoders**: \"Extracting and Composing Robust Features with Denoising Autoencoders\" by Vincent et al.\n",
    "   - Theory behind denoising autoencoders\n",
    "\n",
    "7. **β-VAE Paper**: \"beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\"\n",
    "   - Advanced VAE variant for disentangled representations\n",
    "\n",
    "### Video Lectures\n",
    "\n",
    "8. **Stanford CS231n**: Convolutional Neural Networks for Visual Recognition\n",
    "   - Lectures on autoencoders and unsupervised learning\n",
    "   - Available on YouTube\n",
    "\n",
    "### Books\n",
    "\n",
    "9. **\"Deep Learning\"** by Goodfellow, Bengio, and Courville\n",
    "   - Chapter 14 covers autoencoders in detail\n",
    "   - Free online: [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/)\n",
    "\n",
    "Remember: The best way to learn is by doing! Try implementing different autoencoder variants and applying them to your own datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Day 70: Attention-based sequence models and Temporal Fusion\n\n## Introduction\n\nTime series forecasting has evolved dramatically with the advent of deep learning. While traditional recurrent architectures like LSTMs and GRUs have shown promise, they struggle with long sequences and often fail to capture complex temporal dependencies. **Attention mechanisms** have revolutionized this field by allowing models to selectively focus on relevant parts of the input sequence, regardless of their position.\n\nIn this lesson, we'll explore how attention mechanisms can be applied to time series forecasting, with a particular focus on **Temporal Fusion Transformers (TFT)**, a state-of-the-art architecture that combines multiple temporal processing components with interpretable attention layers.\n\n### Why Attention for Time Series?\n\nTraditional sequence models process data sequentially, which can lead to:\n- **Information loss** over long sequences\n- **Difficulty capturing long-range dependencies**\n- **Limited interpretability** - we can't see which past values influenced predictions\n\nAttention mechanisms solve these problems by:\n- Computing direct connections between all time steps\n- Learning which historical points are most relevant for prediction\n- Providing interpretable attention weights that show what the model \"focuses on\"\n\n### Real-World Applications\n\n- **Energy demand forecasting**: Understanding which historical patterns (weekends, holidays, weather events) influence future consumption\n- **Stock price prediction**: Identifying which past market events correlate with future movements\n- **Retail sales forecasting**: Capturing seasonal patterns and promotional effects\n- **Weather prediction**: Modeling complex temporal and spatial dependencies\n\n### Learning Objectives\n\nBy the end of this lesson, you will be able to:\n\n1. **Understand** the attention mechanism and its application to time series\n2. **Implement** a simple attention layer for sequence modeling\n3. **Build** a temporal attention model for forecasting\n4. **Visualize** attention weights to interpret model predictions\n5. **Apply** attention-based models to real-world time series data",
   "id": "cell-introduction"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Theory: Attention Mechanisms for Time Series\n\n### 1. The Attention Mechanism\n\nAt its core, attention is a mechanism that computes a weighted sum of values, where the weights are dynamically computed based on relevance.\n\n**Mathematical Formulation:**\n\nGiven:\n- Query vector $\\mathbf{q} \\in \\mathbb{R}^{d_k}$\n- Key vectors $\\mathbf{K} \\in \\mathbb{R}^{n \\times d_k}$\n- Value vectors $\\mathbf{V} \\in \\mathbb{R}^{n \\times d_v}$\n\nThe attention mechanism computes:\n\n$$\\text{Attention}(\\mathbf{q}, \\mathbf{K}, \\mathbf{V}) = \\sum_{i=1}^{n} \\alpha_i \\mathbf{v}_i$$\n\nwhere the attention weights $\\alpha_i$ are computed as:\n\n$$\\alpha_i = \\frac{\\exp(\\text{score}(\\mathbf{q}, \\mathbf{k}_i))}{\\sum_{j=1}^{n} \\exp(\\text{score}(\\mathbf{q}, \\mathbf{k}_j))}$$\n\nCommon scoring functions include:\n- **Dot product**: $\\text{score}(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q}^T \\mathbf{k}$\n- **Scaled dot product**: $\\text{score}(\\mathbf{q}, \\mathbf{k}) = \\frac{\\mathbf{q}^T \\mathbf{k}}{\\sqrt{d_k}}$\n- **Additive**: $\\text{score}(\\mathbf{q}, \\mathbf{k}) = \\mathbf{w}^T \\tanh(\\mathbf{W}_q\\mathbf{q} + \\mathbf{W}_k\\mathbf{k})$\n\n### 2. Self-Attention for Sequences\n\nIn **self-attention**, the queries, keys, and values all come from the same sequence. For a time series $\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_T]$:\n\n$$\\mathbf{Q} = \\mathbf{X}\\mathbf{W}_Q, \\quad \\mathbf{K} = \\mathbf{X}\\mathbf{W}_K, \\quad \\mathbf{V} = \\mathbf{X}\\mathbf{W}_V$$\n\nwhere $\\mathbf{W}_Q, \\mathbf{W}_K, \\mathbf{W}_V$ are learned projection matrices.\n\nThe self-attention output is:\n\n$$\\text{SelfAttention}(\\mathbf{X}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n\n### 3. Multi-Head Attention\n\nInstead of a single attention function, multi-head attention uses multiple parallel attention \"heads\" to capture different types of relationships:\n\n$$\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)\\mathbf{W}_O$$\n\nwhere each head is:\n\n$$\\text{head}_i = \\text{Attention}(\\mathbf{Q}\\mathbf{W}_Q^i, \\mathbf{K}\\mathbf{W}_K^i, \\mathbf{V}\\mathbf{W}_V^i)$$\n\n### 4. Temporal Patterns in Attention\n\nFor time series, attention can capture:\n\n1. **Seasonal patterns**: Attending to the same time of day/week/year in historical data\n2. **Event-driven patterns**: Focusing on sudden changes or anomalies\n3. **Trend patterns**: Weighting recent vs. distant history differently\n4. **Causal relationships**: Learning dependencies between different variables over time\n\n### 5. Temporal Fusion Transformers (TFT)\n\nTFT is a sophisticated architecture designed specifically for time series forecasting with the following components:\n\n**Key Components:**\n\n1. **Variable Selection Networks**: Learn which input features are most relevant\n2. **Static Covariate Encoders**: Process time-invariant features (e.g., store ID, product category)\n3. **Temporal Processing**: \n   - LSTM encoder for historical sequences\n   - LSTM decoder for future predictions\n4. **Multi-head Self-Attention**: Capture long-range dependencies\n5. **Gated Residual Networks (GRN)**: Enable efficient information flow\n6. **Quantile Forecasting**: Predict multiple quantiles for uncertainty estimation\n\n**Architecture Flow:**\n\n```\nInput Features\n    â†“\nVariable Selection (What's important?)\n    â†“\nStatic + Temporal Encoding\n    â†“\nLSTM Encoder (Past context)\n    â†“\nMulti-Head Attention (Long-range dependencies)\n    â†“\nLSTM Decoder (Future predictions)\n    â†“\nQuantile Outputs + Attention Weights\n```\n\n### 6. Advantages of Attention for Time Series\n\n1. **Interpretability**: Attention weights reveal which historical points influenced predictions\n2. **Long-range dependencies**: Direct connections between all time steps\n3. **Flexibility**: Can handle irregular time series and missing data\n4. **Multi-horizon forecasting**: Naturally extends to predicting multiple steps ahead\n5. **Variable importance**: Can learn which features matter most\n\n### 7. Challenges\n\n1. **Computational complexity**: $O(T^2)$ for sequence length $T$\n2. **Overfitting on small datasets**: Many parameters require sufficient data\n3. **Positional encoding**: Need to inject temporal position information\n4. **Causality**: Must ensure model doesn't \"peek\" at future values during training",
   "id": "cell-theory"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\n\n# Set plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(\"Libraries imported successfully!\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")",
   "id": "cell-imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Visualize Time Series Components\n\nfig, axes = plt.subplots(4, 1, figsize=(14, 10))\n\n# Plot the complete time series\naxes[0].plot(ts_data['time'][:500], ts_data['value'][:500], label='Complete Time Series', color='navy', alpha=0.7)\naxes[0].set_title('Complete Time Series (First 500 points)', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Time')\naxes[0].set_ylabel('Value')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Plot trend component\naxes[1].plot(ts_data['time'][:500], ts_data['trend'][:500], label='Trend Component', color='green', linewidth=2)\naxes[1].set_title('Trend Component', fontsize=12, fontweight='bold')\naxes[1].set_xlabel('Time')\naxes[1].set_ylabel('Value')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Plot daily seasonal component\naxes[2].plot(ts_data['time'][:200], ts_data['daily_season'][:200], label='Daily Seasonality (24h cycle)', color='orange', linewidth=2)\naxes[2].set_title('Daily Seasonal Component', fontsize=12, fontweight='bold')\naxes[2].set_xlabel('Time')\naxes[2].set_ylabel('Value')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\n# Plot weekly seasonal component\naxes[3].plot(ts_data['time'][:500], ts_data['weekly_season'][:500], label='Weekly Seasonality (7-day cycle)', color='red', linewidth=2)\naxes[3].set_title('Weekly Seasonal Component', fontsize=12, fontweight='bold')\naxes[3].set_xlabel('Time')\naxes[3].set_ylabel('Value')\naxes[3].legend()\naxes[3].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Time series decomposition visualized successfully!\")",
   "id": "cell-visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Part 1: Implementing Basic Attention Mechanism\n\nclass SimpleAttention:\n    \"\"\"\n    A simple attention mechanism implementation.\n    Computes attention weights and weighted sum of values.\n    \"\"\"\n    \n    def __init__(self, d_k):\n        \"\"\"\n        Args:\n            d_k: Dimension of keys (used for scaling)\n        \"\"\"\n        self.d_k = d_k\n    \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        \"\"\"\n        Compute scaled dot-product attention.\n        \n        Args:\n            Q: Query matrix (batch_size, seq_len_q, d_k)\n            K: Key matrix (batch_size, seq_len_k, d_k)\n            V: Value matrix (batch_size, seq_len_v, d_v)\n            mask: Optional mask to prevent attention to certain positions\n            \n        Returns:\n            output: Attention-weighted values\n            attention_weights: Attention weights for visualization\n        \"\"\"\n        # Compute attention scores\n        scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(self.d_k)\n        \n        # Apply mask if provided (for causal attention)\n        if mask is not None:\n            scores = np.where(mask == 0, -1e9, scores)\n        \n        # Apply softmax to get attention weights\n        attention_weights = self._softmax(scores, axis=-1)\n        \n        # Compute weighted sum of values\n        output = np.matmul(attention_weights, V)\n        \n        return output, attention_weights\n    \n    def _softmax(self, x, axis=-1):\n        \"\"\"Numerically stable softmax\"\"\"\n        exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\n# Test the attention mechanism\nprint(\"=== Testing Simple Attention ===\\n\")\n\n# Create sample data (batch_size=1, seq_len=5, d_model=4)\nbatch_size = 1\nseq_len = 5\nd_model = 4\n\nQ = np.random.randn(batch_size, seq_len, d_model)\nK = np.random.randn(batch_size, seq_len, d_model)\nV = np.random.randn(batch_size, seq_len, d_model)\n\nattention = SimpleAttention(d_k=d_model)\noutput, weights = attention.scaled_dot_product_attention(Q, K, V)\n\nprint(f\"Input shapes: Q={Q.shape}, K={K.shape}, V={V.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Attention weights shape: {weights.shape}\")\nprint(f\"\\nAttention weights (showing how each position attends to others):\")\nprint(weights[0].round(3))",
   "id": "cell-examples"
  },
  {
   "cell_type": "code",
   "source": "## Part 2: Generate Synthetic Time Series Data\n\ndef generate_time_series(n_samples=1000, noise_level=0.1):\n    \"\"\"\n    Generate a synthetic time series with trend, seasonality, and noise.\n    \n    Args:\n        n_samples: Number of time points\n        noise_level: Standard deviation of noise\n        \n    Returns:\n        DataFrame with time series data\n    \"\"\"\n    t = np.arange(n_samples)\n    \n    # Trend component\n    trend = 0.05 * t\n    \n    # Seasonal components (daily and weekly patterns)\n    daily_season = 10 * np.sin(2 * np.pi * t / 24)  # 24-hour cycle\n    weekly_season = 5 * np.sin(2 * np.pi * t / 168)  # 7-day cycle\n    \n    # Random noise\n    noise = np.random.normal(0, noise_level * 10, n_samples)\n    \n    # Combine all components\n    value = 50 + trend + daily_season + weekly_season + noise\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'time': t,\n        'value': value,\n        'trend': 50 + trend,\n        'daily_season': daily_season,\n        'weekly_season': weekly_season\n    })\n    \n    return df\n\n# Generate time series\nts_data = generate_time_series(n_samples=1000, noise_level=0.1)\n\nprint(\"Time Series Data Generated:\")\nprint(f\"Shape: {ts_data.shape}\")\nprint(f\"\\nFirst few rows:\")\nprint(ts_data.head(10))\nprint(f\"\\nStatistics:\")\nprint(ts_data['value'].describe())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "## Part 6: Visualize Attention Weights for Interpretability\n\n# Select a few test samples to visualize attention\nsample_indices = [0, 10, 20]\n\nfig, axes = plt.subplots(len(sample_indices), 2, figsize=(16, 4*len(sample_indices)))\n\nfor idx, sample_idx in enumerate(sample_indices):\n    # Get the input sequence\n    input_seq = X_test[sample_idx]\n    actual_value = y_test[sample_idx]\n    \n    # Get prediction and attention weights\n    pred, attn_weights = model.predict(input_seq.reshape(1, -1), return_attention=True)\n    \n    # Extract attention weights for this sample (average across query positions)\n    attn = attn_weights[0].mean(axis=0)  # Average attention across queries\n    \n    # Plot 1: Input sequence with color-coded attention\n    ax1 = axes[idx, 0] if len(sample_indices) > 1 else axes[0]\n    time_steps = np.arange(len(input_seq))\n    \n    # Create scatter plot where point size represents attention weight\n    scatter = ax1.scatter(time_steps, input_seq, c=attn, s=attn*500, \n                         cmap='YlOrRd', alpha=0.7, edgecolors='black', linewidth=0.5)\n    ax1.plot(time_steps, input_seq, alpha=0.3, color='gray', linestyle='--')\n    ax1.set_title(f'Sample {sample_idx}: Input Sequence with Attention Weights\\n'\n                  f'Actual next value: {actual_value[0]:.3f}, Predicted: {pred[0][0]:.3f}',\n                  fontsize=12, fontweight='bold')\n    ax1.set_xlabel('Time Step (lookback)')\n    ax1.set_ylabel('Normalized Value')\n    ax1.grid(True, alpha=0.3)\n    plt.colorbar(scatter, ax=ax1, label='Attention Weight')\n    \n    # Plot 2: Attention weights as bar chart\n    ax2 = axes[idx, 1] if len(sample_indices) > 1 else axes[1]\n    bars = ax2.bar(time_steps, attn, color='steelblue', alpha=0.7, edgecolor='black')\n    \n    # Highlight top-5 most attended positions\n    top_5_indices = np.argsort(attn)[-5:]\n    for i in top_5_indices:\n        bars[i].set_color('coral')\n    \n    ax2.set_title(f'Attention Distribution (Top 5 positions highlighted)', \n                  fontsize=12, fontweight='bold')\n    ax2.set_xlabel('Time Step (lookback)')\n    ax2.set_ylabel('Attention Weight')\n    ax2.grid(True, alpha=0.3, axis='y')\n    \n    # Add text annotation for top attended position\n    top_pos = np.argmax(attn)\n    ax2.annotate(f'Max: {attn[top_pos]:.3f}',\n                xy=(top_pos, attn[top_pos]),\n                xytext=(top_pos, attn[top_pos] + 0.002),\n                fontsize=9, ha='center',\n                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n=== Attention Interpretation ===\")\nprint(\"The visualizations show which historical time steps the model 'pays attention to'\")\nprint(\"when making predictions. Larger points and higher bars indicate greater importance.\")\nprint(\"\\nKey observations:\")\nprint(\"- Recent time steps typically receive more attention\")\nprint(\"- The model may also focus on specific seasonal patterns\")\nprint(\"- Attention weights provide interpretability for model decisions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "## Part 5: Evaluate Model and Visualize Results\n\n# Make predictions on test set\ny_pred_test, attention_weights_test = model.predict(X_test, return_attention=True)\n\n# Calculate test metrics\nmse_test = np.mean((y_pred_test.flatten() - y_test.flatten()) ** 2)\nrmse_test = np.sqrt(mse_test)\nmae_test = np.mean(np.abs(y_pred_test.flatten() - y_test.flatten()))\n\nprint(\"=== Model Evaluation ===\\n\")\nprint(f\"Test MSE: {mse_test:.6f}\")\nprint(f\"Test RMSE: {rmse_test:.6f}\")\nprint(f\"Test MAE: {mae_test:.6f}\")\n\n# Visualize training loss\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(losses, color='blue', linewidth=2)\nplt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\nplt.xlabel('Epoch')\nplt.ylabel('Loss (MSE)')\nplt.grid(True, alpha=0.3)\n\n# Visualize predictions vs actual\nplt.subplot(1, 2, 2)\nn_plot = 100\nplt.plot(y_test[:n_plot], label='Actual', color='green', linewidth=2, alpha=0.7)\nplt.plot(y_pred_test[:n_plot], label='Predicted', color='red', linewidth=2, alpha=0.7, linestyle='--')\nplt.title('Predictions vs Actual (First 100 test samples)', fontsize=14, fontweight='bold')\nplt.xlabel('Sample Index')\nplt.ylabel('Normalized Value')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "## Part 4: Build Attention-Based Time Series Model\n\nclass TemporalAttentionModel:\n    \"\"\"\n    A simple attention-based forecasting model for time series.\n    Uses attention to weight historical time steps for prediction.\n    \"\"\"\n    \n    def __init__(self, seq_length, d_model=8, learning_rate=0.01):\n        \"\"\"\n        Args:\n            seq_length: Length of input sequences\n            d_model: Dimension of the model\n            learning_rate: Learning rate for gradient descent\n        \"\"\"\n        self.seq_length = seq_length\n        self.d_model = d_model\n        self.learning_rate = learning_rate\n        \n        # Initialize parameters (simplified for demonstration)\n        # In practice, these would be more sophisticated\n        self.W_q = np.random.randn(1, d_model) * 0.1\n        self.W_k = np.random.randn(1, d_model) * 0.1\n        self.W_v = np.random.randn(1, d_model) * 0.1\n        self.W_out = np.random.randn(d_model, 1) * 0.1\n        \n        self.attention_weights_history = []\n        \n    def forward(self, X, return_attention=False):\n        \"\"\"\n        Forward pass through the model.\n        \n        Args:\n            X: Input sequences (batch_size, seq_length)\n            return_attention: Whether to return attention weights\n            \n        Returns:\n            predictions: Forecasted values\n            attention_weights: (optional) Attention weights\n        \"\"\"\n        batch_size = X.shape[0]\n        \n        # Reshape input for matrix operations\n        X_reshaped = X.reshape(batch_size, self.seq_length, 1)\n        \n        # Project inputs to Q, K, V\n        Q = np.matmul(X_reshaped, self.W_q)  # (batch, seq_len, d_model)\n        K = np.matmul(X_reshaped, self.W_k)\n        V = np.matmul(X_reshaped, self.W_v)\n        \n        # Compute attention scores\n        scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(self.d_model)\n        attention_weights = self._softmax(scores, axis=-1)\n        \n        # Apply attention to values\n        attended = np.matmul(attention_weights, V)\n        \n        # Pool across sequence dimension (simple mean)\n        pooled = np.mean(attended, axis=1)  # (batch, d_model)\n        \n        # Output projection\n        predictions = np.matmul(pooled, self.W_out)  # (batch, 1)\n        \n        if return_attention:\n            return predictions, attention_weights\n        return predictions\n    \n    def _softmax(self, x, axis=-1):\n        \"\"\"Numerically stable softmax\"\"\"\n        exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n    \n    def train(self, X_train, y_train, epochs=50, batch_size=32, verbose=True):\n        \"\"\"\n        Train the model using gradient descent.\n        \n        Args:\n            X_train: Training sequences\n            y_train: Training targets\n            epochs: Number of training epochs\n            batch_size: Batch size for training\n            verbose: Whether to print progress\n        \"\"\"\n        n_samples = X_train.shape[0]\n        losses = []\n        \n        for epoch in range(epochs):\n            epoch_loss = 0\n            n_batches = 0\n            \n            # Shuffle data\n            indices = np.random.permutation(n_samples)\n            X_shuffled = X_train[indices]\n            y_shuffled = y_train[indices]\n            \n            # Mini-batch training\n            for i in range(0, n_samples, batch_size):\n                X_batch = X_shuffled[i:i+batch_size]\n                y_batch = y_shuffled[i:i+batch_size].reshape(-1, 1)\n                \n                # Forward pass\n                predictions = self.forward(X_batch)\n                \n                # Compute loss (MSE)\n                loss = np.mean((predictions - y_batch) ** 2)\n                epoch_loss += loss\n                n_batches += 1\n                \n                # Backward pass (simplified gradient descent)\n                # In practice, you'd use proper backpropagation\n                error = predictions - y_batch\n                grad_scale = 2 * error / len(X_batch)\n                \n                # Update parameters with small learning rate\n                self.W_out -= self.learning_rate * grad_scale.mean() * 0.01\n            \n            avg_loss = epoch_loss / n_batches\n            losses.append(avg_loss)\n            \n            if verbose and (epoch + 1) % 10 == 0:\n                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n        \n        return losses\n    \n    def predict(self, X, return_attention=False):\n        \"\"\"Make predictions on new data\"\"\"\n        return self.forward(X, return_attention=return_attention)\n\n# Initialize and train the model\nprint(\"=== Training Attention-Based Time Series Model ===\\n\")\n\nmodel = TemporalAttentionModel(seq_length=seq_length, d_model=8, learning_rate=0.01)\n\n# Train the model\nlosses = model.train(X_train, y_train, epochs=50, batch_size=32, verbose=True)\n\nprint(\"\\nTraining complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "## Part 3: Prepare Data for Attention Model\n\ndef create_sequences(data, seq_length, forecast_horizon):\n    \"\"\"\n    Create input sequences and target values for time series forecasting.\n    \n    Args:\n        data: Array of time series values\n        seq_length: Length of input sequence (lookback window)\n        forecast_horizon: Number of steps to predict ahead\n        \n    Returns:\n        X: Input sequences\n        y: Target values\n    \"\"\"\n    X, y = [], []\n    \n    for i in range(len(data) - seq_length - forecast_horizon + 1):\n        # Input: sequence of length seq_length\n        X.append(data[i:i + seq_length])\n        # Target: next value(s) after the sequence\n        y.append(data[i + seq_length:i + seq_length + forecast_horizon])\n    \n    return np.array(X), np.array(y)\n\n# Prepare data\nseq_length = 50  # Use 50 time steps to predict the next values\nforecast_horizon = 1  # Predict 1 step ahead\n\n# Normalize the data\nscaler = StandardScaler()\nvalues = ts_data['value'].values.reshape(-1, 1)\nvalues_normalized = scaler.fit_transform(values).flatten()\n\n# Create sequences\nX, y = create_sequences(values_normalized, seq_length, forecast_horizon)\n\nprint(f\"Sequence preparation complete:\")\nprint(f\"X shape: {X.shape} (samples, sequence_length)\")\nprint(f\"y shape: {y.shape} (samples, forecast_horizon)\")\nprint(f\"\\nExample input sequence (first 10 values): {X[0][:10].round(3)}\")\nprint(f\"Corresponding target: {y[0].round(3)}\")\n\n# Split into train and test sets\ntrain_size = int(0.8 * len(X))\nX_train, X_test = X[:train_size], X[train_size:]\ny_train, y_test = y[:train_size], y[train_size:]\n\nprint(f\"\\nTrain set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Hands-On Activity: Compare Attention vs Simple Baseline\n\nNow it's your turn! In this activity, you'll:\n\n1. **Implement a simple baseline model** (using just the most recent value)\n2. **Compare performance** with the attention-based model\n3. **Experiment with hyperparameters** to improve the attention model\n4. **Analyze attention patterns** for different types of time series\n\n### Task 1: Create a Simple Baseline\n\nImplement a naive baseline that predicts the next value as the average of the last N values.",
   "id": "cell-activity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Activity Solution: Baseline Comparison\n\n# Task 1: Simple Moving Average Baseline\ndef moving_average_baseline(X, window=5):\n    \"\"\"\n    Predict next value as average of last 'window' values.\n    \n    Args:\n        X: Input sequences (n_samples, seq_length)\n        window: Number of recent values to average\n        \n    Returns:\n        predictions: Predicted next values\n    \"\"\"\n    predictions = np.mean(X[:, -window:], axis=1)\n    return predictions\n\n# Task 2: Evaluate baseline\nbaseline_predictions = moving_average_baseline(X_test, window=10)\n\nbaseline_mse = np.mean((baseline_predictions - y_test.flatten()) ** 2)\nbaseline_rmse = np.sqrt(baseline_mse)\nbaseline_mae = np.mean(np.abs(baseline_predictions - y_test.flatten()))\n\nprint(\"=== Baseline vs Attention Model Comparison ===\\n\")\nprint(f\"{'Metric':<15} {'Baseline':<15} {'Attention':<15} {'Improvement':<15}\")\nprint(\"-\" * 60)\nprint(f\"{'MSE':<15} {baseline_mse:<15.6f} {mse_test:<15.6f} {(baseline_mse - mse_test)/baseline_mse*100:>13.2f}%\")\nprint(f\"{'RMSE':<15} {baseline_rmse:<15.6f} {rmse_test:<15.6f} {(baseline_rmse - rmse_test)/baseline_rmse*100:>13.2f}%\")\nprint(f\"{'MAE':<15} {baseline_mae:<15.6f} {mae_test:<15.6f} {(baseline_mae - mae_test)/baseline_mae*100:>13.2f}%\")\n\n# Task 3: Visualize comparison\nplt.figure(figsize=(14, 5))\n\nplt.subplot(1, 2, 1)\nn_plot = 100\nplt.plot(y_test[:n_plot], label='Actual', color='green', linewidth=2.5, alpha=0.8)\nplt.plot(baseline_predictions[:n_plot], label='Baseline (Moving Avg)', \n         color='orange', linewidth=2, alpha=0.7, linestyle='-.')\nplt.plot(y_pred_test[:n_plot], label='Attention Model', \n         color='red', linewidth=2, alpha=0.7, linestyle='--')\nplt.title('Model Comparison: First 100 Test Samples', fontsize=14, fontweight='bold')\nplt.xlabel('Sample Index')\nplt.ylabel('Normalized Value')\nplt.legend(loc='best')\nplt.grid(True, alpha=0.3)\n\n# Plot error distributions\nplt.subplot(1, 2, 2)\nbaseline_errors = np.abs(baseline_predictions - y_test.flatten())\nattention_errors = np.abs(y_pred_test.flatten() - y_test.flatten())\n\nplt.hist(baseline_errors, bins=30, alpha=0.6, label='Baseline Errors', color='orange', edgecolor='black')\nplt.hist(attention_errors, bins=30, alpha=0.6, label='Attention Errors', color='red', edgecolor='black')\nplt.title('Error Distribution Comparison', fontsize=14, fontweight='bold')\nplt.xlabel('Absolute Error')\nplt.ylabel('Frequency')\nplt.legend()\nplt.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nâœ… Activity Complete!\")\nprint(\"\\nKey Insights:\")\nprint(\"1. The attention model learns to weigh historical data more intelligently\")\nprint(\"2. Attention provides interpretability through weight visualization\")\nprint(\"3. More complex patterns benefit more from attention mechanisms\")\nprint(\"\\nðŸ’¡ Try experimenting with:\")\nprint(\"- Different sequence lengths (seq_length)\")\nprint(\"- Model dimensions (d_model)\")\nprint(\"- Training epochs and learning rates\")\nprint(\"- Adding positional encoding to the input sequences\")",
   "id": "cell-activity-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n### Core Concepts\n\n1. **Attention Mechanism**: A powerful technique that computes weighted sums of values based on learned relevance scores, allowing models to focus on the most important parts of the input sequence.\n\n2. **Self-Attention for Time Series**: By computing queries, keys, and values from the same sequence, self-attention enables each time step to directly interact with all other time steps, capturing long-range dependencies without sequential processing.\n\n3. **Scaled Dot-Product Attention**: The most common attention variant uses the formula:\n   $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n   The scaling factor $\\sqrt{d_k}$ prevents softmax saturation in high dimensions.\n\n### Advantages of Attention for Time Series\n\n- **Interpretability**: Attention weights reveal which historical points influenced each prediction, providing transparency in model decisions\n- **Long-range dependencies**: Direct connections between all time steps overcome the limitations of sequential processing\n- **Flexibility**: Can handle irregular sampling, missing data, and variable-length sequences\n- **Parallel computation**: Unlike RNNs, attention can be computed in parallel across all positions\n\n### Practical Considerations\n\n- **Computational complexity**: Standard attention is $O(T^2)$ where $T$ is sequence length. For very long sequences, consider sparse attention or linear attention variants.\n- **Positional information**: Pure attention is permutation-invariant, so positional encodings are crucial for maintaining temporal order.\n- **Causality**: In forecasting, use masked attention to prevent the model from \"seeing\" future values during training.\n- **Multi-head attention**: Using multiple attention heads with different learned projections captures diverse temporal patterns.\n\n### When to Use Attention for Time Series\n\n**Good fit:**\n- Long sequences where distant past matters (e.g., yearly seasonality in daily data)\n- Multi-variate time series with complex interactions\n- When interpretability is important\n- Irregular or sparse time series\n\n**Consider alternatives:**\n- Very short sequences (simple models may suffice)\n- Limited training data (attention models have many parameters)\n- Real-time constraints with extremely long sequences (computational cost)\n\n### Connection to Temporal Fusion Transformers\n\nTFT extends basic attention with:\n- **Variable selection networks** for automatic feature selection\n- **Static + temporal encoders** for time-invariant and time-varying features\n- **Quantile forecasting** for uncertainty estimation\n- **Interpretable multi-head attention** with gating mechanisms\n\nTFT represents the state-of-the-art for many forecasting benchmarks and demonstrates the power of combining attention with domain-specific architectural choices.",
   "id": "cell-takeaways"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Further Resources\n\n### Foundational Papers\n\n- **[Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)** - The original Transformer paper that introduced scaled dot-product attention and multi-head attention. While focused on NLP, the mechanisms apply directly to time series.\n\n- **[Temporal Fusion Transformers (Lim et al., 2021)](https://arxiv.org/abs/1912.09363)** - State-of-the-art attention-based architecture specifically designed for time series forecasting with interpretability.\n\n- **[Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting (Zhou et al., 2021)](https://arxiv.org/abs/2012.07436)** - Addresses the computational challenges of attention for very long sequences with ProbSparse self-attention.\n\n### Implementations and Libraries\n\n- **[PyTorch Temporal Fusion Transformer](https://pytorch-forecasting.readthedocs.io/en/latest/models.html)** - Production-ready implementation of TFT in the pytorch-forecasting library.\n\n- **[Hugging Face Transformers](https://huggingface.co/docs/transformers/index)** - While primarily for NLP, contains reusable attention components that can be adapted for time series.\n\n- **[GluonTS](https://ts.gluon.ai/)** - Amazon's toolkit for probabilistic time series modeling, includes Transformer-based forecasting models.\n\n### Tutorials and Guides\n\n- **[The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)** - Excellent visual guide to understanding attention mechanisms and the Transformer architecture.\n\n- **[Time Series Forecasting with Transformers (TensorFlow Tutorial)](https://www.tensorflow.org/tutorials/structured_data/time_series)** - Official TensorFlow tutorial on applying Transformers to time series.\n\n- **[Attention Mechanisms in Deep Learning (d2l.ai)](https://d2l.ai/chapter_attention-mechanisms/)** - Comprehensive textbook chapter with interactive examples.\n\n### Advanced Topics\n\n- **[Efficient Attention Mechanisms](https://arxiv.org/abs/2009.06732)** - Survey of techniques to reduce the $O(T^2)$ complexity of standard attention.\n\n- **[Multi-variate Time Series Forecasting with Transformers](https://arxiv.org/abs/2001.08317)** - Extending attention to handle multiple correlated time series.\n\n- **[Interpretable Time Series Forecasting with Attention](https://arxiv.org/abs/1907.00235)** - Techniques for extracting meaningful insights from attention weights.\n\n### Tools and Datasets\n\n- **[Monash Time Series Forecasting Archive](https://forecastingdata.org/)** - Large collection of time series datasets for benchmarking forecasting models.\n\n- **[Weights & Biases](https://wandb.ai/)** - Experiment tracking and visualization tool, excellent for hyperparameter tuning of attention models.\n\n- **[Optuna](https://optuna.org/)** - Hyperparameter optimization framework useful for tuning complex models like TFT.\n\n### Next Steps\n\n1. **Implement positional encoding** to better preserve temporal order information\n2. **Experiment with multi-head attention** to capture different temporal patterns simultaneously\n3. **Try TFT on real datasets** like electricity demand or retail sales forecasting\n4. **Explore sparse attention** variants for handling very long sequences efficiently\n5. **Study hybrid models** that combine attention with RNNs or CNNs for enhanced performance",
   "id": "cell-resources"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
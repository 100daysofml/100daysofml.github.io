{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 50: Evaluating and Tuning Neural Network Performance\n",
    "\n",
    "Congratulations on reaching Day 50 of the 100 Days of Machine Learning Challenge! You've made it halfway through this comprehensive journey. By now, you've built a solid foundation in neural networks, including understanding perceptrons, feedforward networks, backpropagation, and training techniques. Today, we take the next crucial step: learning how to evaluate and tune neural network performance to build models that generalize well to unseen data.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Building a neural network is only half the battle. The true challenge lies in ensuring that your model performs well not just on training data, but also on new, unseen data. This is where evaluation metrics, regularization techniques, and hyperparameter tuning come into play.\n",
    "\n",
    "In this lesson, we will explore:\n",
    "- How to evaluate neural network performance using various metrics\n",
    "- Understanding and preventing overfitting through regularization\n",
    "- Techniques for hyperparameter tuning to optimize model performance\n",
    "- Practical implementation using TensorFlow and Keras\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "In real-world applications, a model that simply memorizes training data is useless. Medical diagnosis systems, autonomous vehicles, and financial prediction models all require neural networks that can generalize from training examples to new situations. Understanding how to evaluate and tune your models is essential for deploying reliable machine learning systems.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Evaluate neural network performance using appropriate metrics for classification and regression tasks\n",
    "2. Understand and apply regularization techniques (L1, L2, dropout) to prevent overfitting\n",
    "3. Implement early stopping and learning rate scheduling\n",
    "4. Perform hyperparameter tuning to optimize model performance\n",
    "5. Visualize training history to diagnose model behavior\n",
    "6. Apply cross-validation techniques specific to neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for Neural Networks\n",
    "\n",
    "### Classification Metrics\n",
    "\n",
    "For classification tasks, several metrics help us understand how well our neural network performs:\n",
    "\n",
    "#### 1. Accuracy\n",
    "\n",
    "Accuracy is the most intuitive metric, representing the proportion of correct predictions:\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$$\n",
    "\n",
    "While easy to understand, accuracy can be misleading for imbalanced datasets. For example, if 95% of emails are not spam, a model that always predicts \"not spam\" achieves 95% accuracy but is completely useless.\n",
    "\n",
    "#### 2. Precision and Recall\n",
    "\n",
    "**Precision** measures the accuracy of positive predictions:\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$$\n",
    "\n",
    "**Recall** (or Sensitivity) measures how many actual positive cases were correctly identified:\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$$\n",
    "\n",
    "#### 3. F1-Score\n",
    "\n",
    "The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both:\n",
    "\n",
    "$$\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "#### 4. ROC-AUC\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve plots the True Positive Rate against the False Positive Rate at various threshold settings. The Area Under the Curve (AUC) provides a single number summarizing the model's ability to discriminate between classes. An AUC of 1.0 represents perfect discrimination, while 0.5 represents random guessing.\n",
    "\n",
    "### Regression Metrics\n",
    "\n",
    "For regression tasks, different metrics are appropriate:\n",
    "\n",
    "#### 1. Mean Squared Error (MSE)\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "where $y_i$ is the actual value and $\\hat{y}_i$ is the predicted value.\n",
    "\n",
    "#### 2. Root Mean Squared Error (RMSE)\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "RMSE is in the same units as the target variable, making it more interpretable than MSE.\n",
    "\n",
    "#### 3. Mean Absolute Error (MAE)\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$$\n",
    "\n",
    "MAE is less sensitive to outliers than MSE/RMSE.\n",
    "\n",
    "#### 4. R-squared (R²)\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$$\n",
    "\n",
    "R² represents the proportion of variance in the dependent variable explained by the model, ranging from 0 to 1 (or negative for very poor models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions in Neural Networks\n",
    "\n",
    "Loss functions quantify how far the model's predictions are from the true values. During training, the neural network adjusts its weights to minimize this loss.\n",
    "\n",
    "### Binary Cross-Entropy Loss\n",
    "\n",
    "For binary classification tasks (two classes), binary cross-entropy is the standard loss function:\n",
    "\n",
    "$$\\mathcal{L}_{BCE} = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$$\n",
    "\n",
    "where $y_i \\in \\{0, 1\\}$ is the true label and $\\hat{y}_i \\in [0, 1]$ is the predicted probability.\n",
    "\n",
    "### Categorical Cross-Entropy Loss\n",
    "\n",
    "For multi-class classification, categorical cross-entropy extends binary cross-entropy:\n",
    "\n",
    "$$\\mathcal{L}_{CCE} = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{c=1}^{C}y_{i,c}\\log(\\hat{y}_{i,c})$$\n",
    "\n",
    "where $C$ is the number of classes, $y_{i,c}$ is 1 if sample $i$ belongs to class $c$ and 0 otherwise, and $\\hat{y}_{i,c}$ is the predicted probability for class $c$.\n",
    "\n",
    "### Mean Squared Error Loss\n",
    "\n",
    "For regression tasks, MSE is commonly used as the loss function:\n",
    "\n",
    "$$\\mathcal{L}_{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "### Choosing the Right Loss Function\n",
    "\n",
    "The choice of loss function depends on your task:\n",
    "- **Binary classification**: Binary cross-entropy\n",
    "- **Multi-class classification**: Categorical cross-entropy\n",
    "- **Regression**: MSE, MAE, or Huber loss (robust to outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Overfitting and Underfitting\n",
    "\n",
    "### The Bias-Variance Tradeoff\n",
    "\n",
    "**Overfitting** occurs when a model learns the training data too well, including its noise and outliers. The model performs excellently on training data but poorly on new data. This indicates high variance and low bias.\n",
    "\n",
    "**Underfitting** occurs when a model is too simple to capture the underlying patterns in the data. It performs poorly on both training and test data, indicating high bias and low variance.\n",
    "\n",
    "The goal is to find the sweet spot where the model generalizes well to new data.\n",
    "\n",
    "### Signs of Overfitting\n",
    "\n",
    "1. Training accuracy is much higher than validation accuracy\n",
    "2. Training loss continues to decrease while validation loss increases\n",
    "3. The model performs well on training data but poorly on test data\n",
    "\n",
    "### Signs of Underfitting\n",
    "\n",
    "1. Both training and validation accuracy are low\n",
    "2. The model cannot capture the complexity of the data\n",
    "3. Training and validation losses are both high and similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Techniques\n",
    "\n",
    "Regularization methods add constraints or penalties to prevent overfitting.\n",
    "\n",
    "### L1 and L2 Regularization\n",
    "\n",
    "**L2 Regularization** (Ridge or Weight Decay) adds a penalty proportional to the square of the weights:\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\mathcal{L}_{original} + \\lambda\\sum_{i=1}^{n}w_i^2$$\n",
    "\n",
    "where $\\lambda$ is the regularization parameter and $w_i$ are the model weights. L2 regularization encourages smaller weights, leading to smoother models.\n",
    "\n",
    "**L1 Regularization** (Lasso) adds a penalty proportional to the absolute value of weights:\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\mathcal{L}_{original} + \\lambda\\sum_{i=1}^{n}|w_i|$$\n",
    "\n",
    "L1 regularization can lead to sparse models where some weights become exactly zero, effectively performing feature selection.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout is a powerful regularization technique specific to neural networks. During training, dropout randomly \"drops out\" (sets to zero) a proportion $p$ of neurons in a layer. This prevents neurons from co-adapting too much and forces the network to learn more robust features.\n",
    "\n",
    "Mathematically, for a layer with activations $\\mathbf{h}$:\n",
    "\n",
    "$$\\mathbf{h}_{dropout} = \\mathbf{h} \\odot \\mathbf{m}$$\n",
    "\n",
    "where $\\mathbf{m}$ is a binary mask with each element drawn from $\\text{Bernoulli}(1-p)$, and $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "During inference (testing), all neurons are active, but their outputs are scaled by $(1-p)$ to account for the dropout during training.\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "Early stopping monitors the validation loss during training and stops when it begins to increase, indicating that the model is starting to overfit. A patience parameter determines how many epochs to wait before stopping.\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "Batch normalization normalizes the inputs of each layer, reducing internal covariate shift and allowing higher learning rates. For a batch of activations $\\mathbf{x}$:\n",
    "\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "\n",
    "$$y_i = \\gamma\\hat{x}_i + \\beta$$\n",
    "\n",
    "where $\\mu_B$ and $\\sigma_B^2$ are the batch mean and variance, $\\epsilon$ is a small constant for numerical stability, and $\\gamma$ and $\\beta$ are learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters are parameters that are not learned during training but must be set before training begins. Key hyperparameters for neural networks include:\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "The learning rate $\\alpha$ controls how much the weights are updated during training:\n",
    "\n",
    "$$w_{new} = w_{old} - \\alpha \\nabla \\mathcal{L}$$\n",
    "\n",
    "- **Too high**: The model may overshoot the minimum and diverge\n",
    "- **Too low**: Training will be very slow and may get stuck in local minima\n",
    "\n",
    "**Learning Rate Schedules** adjust the learning rate during training:\n",
    "- **Step decay**: Reduce learning rate by a factor every few epochs\n",
    "- **Exponential decay**: $\\alpha_t = \\alpha_0 e^{-kt}$\n",
    "- **Cosine annealing**: Gradually decrease using a cosine function\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "Batch size determines how many samples are processed before updating weights:\n",
    "- **Small batches**: More frequent updates, more noise, better generalization but slower\n",
    "- **Large batches**: Faster training, more stable gradients, but may generalize worse\n",
    "\n",
    "### Number of Epochs\n",
    "\n",
    "An epoch is one complete pass through the training data. Too few epochs lead to underfitting; too many lead to overfitting.\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "- **Number of layers**: Deeper networks can learn more complex patterns but are harder to train\n",
    "- **Number of neurons per layer**: More neurons increase capacity but also the risk of overfitting\n",
    "- **Activation functions**: ReLU, tanh, sigmoid, etc., each with different properties\n",
    "\n",
    "### Optimization Algorithm\n",
    "\n",
    "Different optimizers have different convergence properties:\n",
    "- **SGD**: Simple but can be slow\n",
    "- **Momentum**: Accelerates SGD by accumulating velocity\n",
    "- **Adam**: Adaptive learning rates, often works well with default parameters\n",
    "- **RMSprop**: Similar to Adam, good for recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Implementation\n",
    "\n",
    "Now let's implement these concepts using TensorFlow and Keras. We'll build, train, and evaluate neural networks with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import TensorFlow, if not available, install it\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, regularizers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    print(\"TensorFlow imported successfully!\")\n",
    "    tf.random.set_seed(42)\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available. Installing...\")\n",
    "    print(\"Note: In a real environment, run: pip install tensorflow\")\n",
    "    print(\"For this lesson, we'll demonstrate with conceptual code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Binary Classification with Evaluation Metrics\n",
    "\n",
    "Let's create a binary classification dataset and build a neural network to classify it. We'll evaluate the model using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic binary classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    weights=[0.7, 0.3],  # Imbalanced classes\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Validation set shape: {X_val_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(f\"  Class 0: {np.sum(y_train == 0)} ({np.sum(y_train == 0)/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Class 1: {np.sum(y_train == 1)} ({np.sum(y_train == 1)/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Simple Neural Network\n",
    "\n",
    "We'll create a baseline neural network without regularization to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple implementation using NumPy to demonstrate concepts\n",
    "# In practice, use TensorFlow/Keras or PyTorch\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"Binary cross-entropy loss\"\"\"\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"A simple 2-layer neural network for demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        # Initialize weights with small random values\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = self.a2 - y.reshape(-1, 1)\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dz1 = np.dot(dz2, self.W2.T) * (self.z1 > 0)  # ReLU derivative\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "    \n",
    "    def train(self, X, y, X_val, y_val, epochs=100):\n",
    "        \"\"\"Train the network\"\"\"\n",
    "        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            train_loss = binary_cross_entropy(y, y_pred)\n",
    "            train_acc = np.mean((y_pred.flatten() > 0.5) == y)\n",
    "            \n",
    "            # Validation metrics\n",
    "            y_val_pred = self.forward(X_val)\n",
    "            val_loss = binary_cross_entropy(y_val, y_val_pred)\n",
    "            val_acc = np.mean((y_val_pred.flatten() > 0.5) == y_val)\n",
    "            \n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n",
    "                      f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return (self.forward(X).flatten() > 0.5).astype(int)\n",
    "\n",
    "# Create and train the network\n",
    "print(\"Training a simple neural network...\\n\")\n",
    "model = SimpleNeuralNetwork(input_size=20, hidden_size=32, output_size=1, learning_rate=0.1)\n",
    "history = model.train(X_train_scaled, y_train, X_val_scaled, y_val, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Training History\n",
    "\n",
    "Training curves help us understand how the model learns over time and diagnose issues like overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Model Loss Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['train_acc'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- If training loss continues to decrease while validation loss increases, the model is overfitting\")\n",
    "print(\"- If both losses remain high, the model may be underfitting\")\n",
    "print(\"- Ideally, both losses should decrease together and plateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance\n",
    "\n",
    "Let's calculate various evaluation metrics on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "y_test_pred_proba = model.forward(X_test_scaled).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Test Set Performance Metrics\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nMetric Interpretations:\")\n",
    "print(f\"- Accuracy: {accuracy*100:.2f}% of all predictions are correct\")\n",
    "print(f\"- Precision: {precision*100:.2f}% of positive predictions are actually positive\")\n",
    "print(f\"- Recall: {recall*100:.2f}% of actual positives were correctly identified\")\n",
    "print(f\"- F1-Score: Harmonic mean of precision and recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix provides a detailed breakdown of correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, \n",
    "            xticklabels=['Class 0', 'Class 1'],\n",
    "            yticklabels=['Class 0', 'Class 1'])\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional insights\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives (TN):  {tn}\")\n",
    "print(f\"  False Positives (FP): {fp}\")\n",
    "print(f\"  False Negatives (FN): {fn}\")\n",
    "print(f\"  True Positives (TP):  {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve and AUC\n",
    "\n",
    "The ROC curve shows the tradeoff between true positive rate and false positive rate at different classification thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', linewidth=2, \n",
    "         label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linewidth=2, linestyle='--', \n",
    "         label='Random Classifier (AUC = 0.5)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- AUC = 1.0: Perfect classifier\")\n",
    "print(\"- AUC = 0.5: Random classifier (no better than coin flip)\")\n",
    "print(\"- AUC > 0.7: Generally considered acceptable\")\n",
    "print(\"- AUC > 0.8: Considered good\")\n",
    "print(\"- AUC > 0.9: Considered excellent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Demonstrating Overfitting\n",
    "\n",
    "Let's intentionally create an overfitting scenario to see how it manifests in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller dataset to encourage overfitting\n",
    "X_small, y_small = make_classification(\n",
    "    n_samples=200,  # Much smaller dataset\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_redundant=10,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(\n",
    "    X_small, y_small, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "scaler_s = StandardScaler()\n",
    "X_train_s_scaled = scaler_s.fit_transform(X_train_s)\n",
    "X_test_s_scaled = scaler_s.transform(X_test_s)\n",
    "\n",
    "# Train an overly complex model\n",
    "print(\"Training an overly complex model on a small dataset...\\n\")\n",
    "overfit_model = SimpleNeuralNetwork(\n",
    "    input_size=20, \n",
    "    hidden_size=128,  # Very large hidden layer\n",
    "    output_size=1, \n",
    "    learning_rate=0.1\n",
    ")\n",
    "overfit_history = overfit_model.train(\n",
    "    X_train_s_scaled, y_train_s, \n",
    "    X_test_s_scaled, y_test_s, \n",
    "    epochs=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overfitting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot showing overfitting\n",
    "axes[0].plot(overfit_history['train_loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(overfit_history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Overfitting: Diverging Loss Curves', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation\n",
    "max_epoch = len(overfit_history['train_loss'])\n",
    "if overfit_history['val_loss'][-1] > overfit_history['val_loss'][max_epoch//4]:\n",
    "    axes[0].annotate('Validation loss starts increasing', \n",
    "                    xy=(max_epoch//4, overfit_history['val_loss'][max_epoch//4]), \n",
    "                    xytext=(max_epoch//3, max(overfit_history['val_loss'])*0.7),\n",
    "                    arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                    fontsize=10, color='red')\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(overfit_history['train_acc'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(overfit_history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Overfitting: Diverging Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSigns of Overfitting Observed:\")\n",
    "print(f\"- Final training accuracy: {overfit_history['train_acc'][-1]:.4f}\")\n",
    "print(f\"- Final validation accuracy: {overfit_history['val_acc'][-1]:.4f}\")\n",
    "print(f\"- Gap: {overfit_history['train_acc'][-1] - overfit_history['val_acc'][-1]:.4f}\")\n",
    "print(\"\\nThis large gap indicates the model has memorized the training data\")\n",
    "print(\"but fails to generalize to new data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Applying Regularization\n",
    "\n",
    "Now let's implement L2 regularization and dropout to combat overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedNeuralNetwork:\n",
    "    \"\"\"Neural network with L2 regularization and dropout\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, \n",
    "                 learning_rate=0.01, l2_lambda=0.01, dropout_rate=0.0):\n",
    "        # Initialize weights\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"Forward pass with optional dropout\"\"\"\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = relu(self.z1)\n",
    "        \n",
    "        # Apply dropout during training\n",
    "        if training and self.dropout_rate > 0:\n",
    "            self.dropout_mask = np.random.binomial(1, 1 - self.dropout_rate, \n",
    "                                                   size=self.a1.shape) / (1 - self.dropout_rate)\n",
    "            self.a1 *= self.dropout_mask\n",
    "        \n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Backward pass with L2 regularization\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = self.a2 - y.reshape(-1, 1)\n",
    "        dW2 = (np.dot(self.a1.T, dz2) + self.l2_lambda * self.W2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        if self.dropout_rate > 0:\n",
    "            da1 *= self.dropout_mask\n",
    "        dz1 = da1 * (self.z1 > 0)\n",
    "        dW1 = (np.dot(X.T, dz1) + self.l2_lambda * self.W1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "    \n",
    "    def compute_loss(self, X, y, training=True):\n",
    "        \"\"\"Compute loss with L2 regularization\"\"\"\n",
    "        y_pred = self.forward(X, training=training)\n",
    "        data_loss = binary_cross_entropy(y, y_pred)\n",
    "        \n",
    "        # Add L2 regularization term\n",
    "        l2_loss = (self.l2_lambda / 2) * (np.sum(self.W1**2) + np.sum(self.W2**2))\n",
    "        total_loss = data_loss + l2_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def train(self, X, y, X_val, y_val, epochs=100):\n",
    "        \"\"\"Train the network\"\"\"\n",
    "        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass with dropout\n",
    "            y_pred = self.forward(X, training=True)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            train_loss = self.compute_loss(X, y, training=False)\n",
    "            train_acc = np.mean((y_pred.flatten() > 0.5) == y)\n",
    "            \n",
    "            # Validation metrics (no dropout)\n",
    "            val_loss = self.compute_loss(X_val, y_val, training=False)\n",
    "            y_val_pred = self.forward(X_val, training=False)\n",
    "            val_acc = np.mean((y_val_pred.flatten() > 0.5) == y_val)\n",
    "            \n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            if (epoch + 1) % 40 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n",
    "                      f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return (self.forward(X, training=False).flatten() > 0.5).astype(int)\n",
    "\n",
    "# Train regularized model on the small dataset\n",
    "print(\"Training a regularized model with L2 regularization and dropout...\\n\")\n",
    "reg_model = RegularizedNeuralNetwork(\n",
    "    input_size=20, \n",
    "    hidden_size=128, \n",
    "    output_size=1, \n",
    "    learning_rate=0.1,\n",
    "    l2_lambda=0.01,  # L2 regularization\n",
    "    dropout_rate=0.3  # 30% dropout\n",
    ")\n",
    "reg_history = reg_model.train(\n",
    "    X_train_s_scaled, y_train_s, \n",
    "    X_test_s_scaled, y_test_s, \n",
    "    epochs=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regularized vs unregularized models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Unregularized model\n",
    "axes[0, 0].plot(overfit_history['train_loss'], label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(overfit_history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0, 0].set_title('Without Regularization: Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=9)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(overfit_history['train_acc'], label='Training Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(overfit_history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0, 1].set_title('Without Regularization: Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=9)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Regularized model\n",
    "axes[1, 0].plot(reg_history['train_loss'], label='Training Loss', linewidth=2)\n",
    "axes[1, 0].plot(reg_history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Loss', fontsize=11)\n",
    "axes[1, 0].set_title('With Regularization: Loss', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=9)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(reg_history['train_acc'], label='Training Accuracy', linewidth=2)\n",
    "axes[1, 1].plot(reg_history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[1, 1].set_title('With Regularization: Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=9)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(\"\\nWithout Regularization:\")\n",
    "print(f\"  Final Train Accuracy: {overfit_history['train_acc'][-1]:.4f}\")\n",
    "print(f\"  Final Val Accuracy:   {overfit_history['val_acc'][-1]:.4f}\")\n",
    "print(f\"  Gap:                  {overfit_history['train_acc'][-1] - overfit_history['val_acc'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\nWith Regularization (L2 + Dropout):\")\n",
    "print(f\"  Final Train Accuracy: {reg_history['train_acc'][-1]:.4f}\")\n",
    "print(f\"  Final Val Accuracy:   {reg_history['val_acc'][-1]:.4f}\")\n",
    "print(f\"  Gap:                  {reg_history['train_acc'][-1] - reg_history['val_acc'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\nRegularization helps reduce overfitting by keeping the gap smaller!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Early Stopping\n",
    "\n",
    "Early stopping monitors the validation loss and stops training when it starts to increase, preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingNeuralNetwork(RegularizedNeuralNetwork):\n",
    "    \"\"\"Neural network with early stopping capability\"\"\"\n",
    "    \n",
    "    def train_with_early_stopping(self, X, y, X_val, y_val, \n",
    "                                  epochs=200, patience=10):\n",
    "        \"\"\"Train with early stopping\"\"\"\n",
    "        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_weights = None\n",
    "        stopped_epoch = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X, training=True)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            train_loss = self.compute_loss(X, y, training=False)\n",
    "            train_acc = np.mean((y_pred.flatten() > 0.5) == y)\n",
    "            \n",
    "            # Validation metrics\n",
    "            val_loss = self.compute_loss(X_val, y_val, training=False)\n",
    "            y_val_pred = self.forward(X_val, training=False)\n",
    "            val_acc = np.mean((y_val_pred.flatten() > 0.5) == y_val)\n",
    "            \n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best weights\n",
    "                best_weights = (self.W1.copy(), self.b1.copy(), \n",
    "                              self.W2.copy(), self.b2.copy())\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "                print(f\"Best validation loss: {best_val_loss:.4f} at epoch {epoch+1-patience}\")\n",
    "                stopped_epoch = epoch + 1\n",
    "                # Restore best weights\n",
    "                self.W1, self.b1, self.W2, self.b2 = best_weights\n",
    "                break\n",
    "            \n",
    "            if (epoch + 1) % 40 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} - \"\n",
    "                      f\"Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        history['stopped_epoch'] = stopped_epoch if stopped_epoch > 0 else epochs\n",
    "        return history\n",
    "\n",
    "# Train with early stopping\n",
    "print(\"Training with early stopping...\\n\")\n",
    "es_model = EarlyStoppingNeuralNetwork(\n",
    "    input_size=20, \n",
    "    hidden_size=128, \n",
    "    output_size=1, \n",
    "    learning_rate=0.1,\n",
    "    l2_lambda=0.001,\n",
    "    dropout_rate=0.2\n",
    ")\n",
    "es_history = es_model.train_with_early_stopping(\n",
    "    X_train_s_scaled, y_train_s, \n",
    "    X_test_s_scaled, y_test_s, \n",
    "    epochs=200,\n",
    "    patience=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize early stopping\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "stopped_epoch = es_history['stopped_epoch']\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(es_history['train_loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(es_history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].axvline(x=stopped_epoch-1, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Stopped at epoch {stopped_epoch}')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Early Stopping: Loss Curves', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(es_history['train_acc'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(es_history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].axvline(x=stopped_epoch-1, color='red', linestyle='--', linewidth=2,\n",
    "               label=f'Stopped at epoch {stopped_epoch}')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Early Stopping: Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining stopped at epoch {stopped_epoch} out of 200\")\n",
    "print(\"Early stopping prevented unnecessary training and potential overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Hyperparameter Tuning - Learning Rate\n",
    "\n",
    "Let's experiment with different learning rates to see their impact on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "lr_histories = {}\n",
    "\n",
    "print(\"Training models with different learning rates...\\n\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Training with learning rate = {lr}\")\n",
    "    model_lr = SimpleNeuralNetwork(\n",
    "        input_size=20, \n",
    "        hidden_size=32, \n",
    "        output_size=1, \n",
    "        learning_rate=lr\n",
    "    )\n",
    "    history = model_lr.train(\n",
    "        X_train_scaled, y_train, \n",
    "        X_val_scaled, y_val, \n",
    "        epochs=100\n",
    "    )\n",
    "    lr_histories[lr] = history\n",
    "    print()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning rate comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss comparison\n",
    "for lr, history in lr_histories.items():\n",
    "    axes[0].plot(history['val_loss'], label=f'LR = {lr}', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Validation Loss', fontsize=12)\n",
    "axes[0].set_title('Impact of Learning Rate on Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "for lr, history in lr_histories.items():\n",
    "    axes[1].plot(history['val_acc'], label=f'LR = {lr}', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Validation Accuracy', fontsize=12)\n",
    "axes[1].set_title('Impact of Learning Rate on Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final performance for each learning rate\n",
    "print(\"\\nFinal Validation Performance:\")\n",
    "print(\"=\"*50)\n",
    "for lr, history in lr_histories.items():\n",
    "    print(f\"Learning Rate {lr:6.3f}: \"\n",
    "          f\"Loss = {history['val_loss'][-1]:.4f}, \"\n",
    "          f\"Accuracy = {history['val_acc'][-1]:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Too small: Slow convergence, may not reach optimal performance\")\n",
    "print(\"- Too large: Unstable training, may diverge or oscillate\")\n",
    "print(\"- Optimal: Fast convergence with stable, good performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Exercise\n",
    "\n",
    "Now it's your turn to apply what you've learned! In this exercise, you'll experiment with different regularization techniques and hyperparameters.\n",
    "\n",
    "### Exercise Tasks:\n",
    "\n",
    "1. **Create a regression dataset** using `make_regression` from sklearn\n",
    "2. **Build and train three neural networks**:\n",
    "   - Model A: No regularization\n",
    "   - Model B: With L2 regularization\n",
    "   - Model C: With dropout\n",
    "3. **Compare their performance** using MSE, RMSE, and R² scores\n",
    "4. **Visualize the training curves** for all three models\n",
    "5. **Experiment with different hyperparameters**:\n",
    "   - Try different hidden layer sizes (16, 32, 64, 128)\n",
    "   - Try different learning rates (0.001, 0.01, 0.1)\n",
    "   - Try different regularization strengths\n",
    "\n",
    "### Starter Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create regression dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=500,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    noise=10.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "scaler_r = StandardScaler()\n",
    "X_train_r_scaled = scaler_r.fit_transform(X_train_r)\n",
    "X_test_r_scaled = scaler_r.transform(X_test_r)\n",
    "\n",
    "# Standardize targets for better training\n",
    "y_scaler = StandardScaler()\n",
    "y_train_r_scaled = y_scaler.fit_transform(y_train_r.reshape(-1, 1)).flatten()\n",
    "y_test_r_scaled = y_scaler.transform(y_test_r.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(\"Regression dataset created!\")\n",
    "print(f\"Features: {X_train_r_scaled.shape[1]}\")\n",
    "print(f\"Training samples: {X_train_r_scaled.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_r_scaled.shape[0]}\")\n",
    "\n",
    "# TODO: Build and train three models with different configurations\n",
    "# TODO: Evaluate using MSE, RMSE, and R²\n",
    "# TODO: Create visualizations comparing the models\n",
    "# TODO: Experiment with hyperparameters\n",
    "\n",
    "print(\"\\n--- Exercise: Complete the TODOs above ---\")\n",
    "print(\"Hint: Modify the classes we created earlier for regression tasks\")\n",
    "print(\"(Use linear activation for output layer, MSE loss instead of BCE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Let's summarize the essential concepts from this lesson:\n",
    "\n",
    "### 1. Evaluation Metrics Matter\n",
    "- **Classification**: Use accuracy, precision, recall, F1-score, and ROC-AUC\n",
    "- **Regression**: Use MSE, RMSE, MAE, and R²\n",
    "- Choose metrics appropriate for your problem (e.g., F1-score for imbalanced data)\n",
    "\n",
    "### 2. Overfitting is a Primary Challenge\n",
    "- Occurs when models memorize training data rather than learning general patterns\n",
    "- Identified by: high training accuracy but low validation/test accuracy\n",
    "- Monitor validation loss during training to detect overfitting early\n",
    "\n",
    "### 3. Regularization Techniques\n",
    "- **L1/L2 Regularization**: Penalizes large weights, encourages simpler models\n",
    "- **Dropout**: Randomly drops neurons during training, prevents co-adaptation\n",
    "- **Early Stopping**: Stops training when validation performance degrades\n",
    "- **Batch Normalization**: Normalizes layer inputs, stabilizes training\n",
    "\n",
    "### 4. Hyperparameter Tuning is Critical\n",
    "- **Learning Rate**: Most important hyperparameter, affects convergence speed and stability\n",
    "- **Batch Size**: Tradeoff between training speed and generalization\n",
    "- **Network Architecture**: Number of layers and neurons per layer\n",
    "- Use systematic approaches: grid search, random search, or Bayesian optimization\n",
    "\n",
    "### 5. Visualization Aids Understanding\n",
    "- Training curves reveal overfitting, underfitting, and convergence issues\n",
    "- ROC curves show classification performance across thresholds\n",
    "- Confusion matrices provide detailed prediction breakdowns\n",
    "\n",
    "### 6. Best Practices\n",
    "- Always use train/validation/test splits\n",
    "- Start with simple models, add complexity gradually\n",
    "- Monitor both training and validation metrics\n",
    "- Use appropriate evaluation metrics for your task\n",
    "- Apply regularization to improve generalization\n",
    "- Document hyperparameter choices and results\n",
    "\n",
    "### Mathematical Insights\n",
    "\n",
    "The core principle of regularization can be expressed as:\n",
    "\n",
    "$$\\min_{\\theta} \\left[\\mathcal{L}(\\theta) + \\lambda R(\\theta)\\right]$$\n",
    "\n",
    "where:\n",
    "- $\\mathcal{L}(\\theta)$ is the data loss (e.g., cross-entropy)\n",
    "- $R(\\theta)$ is the regularization term\n",
    "- $\\lambda$ controls the regularization strength\n",
    "- $\\theta$ represents all model parameters\n",
    "\n",
    "This formulation shows that we're balancing two objectives: fitting the training data well (minimizing $\\mathcal{L}$) while keeping the model simple (minimizing $R$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "To deepen your understanding of neural network evaluation and tuning, explore these resources:\n",
    "\n",
    "### Online Courses and Tutorials\n",
    "1. **Deep Learning Specialization** by Andrew Ng (Coursera) - Comprehensive coverage of neural network optimization\n",
    "2. **Fast.ai Practical Deep Learning** - Practical approaches to training neural networks\n",
    "3. **TensorFlow Documentation** - https://www.tensorflow.org/guide - Official guides and tutorials\n",
    "\n",
    "### Books\n",
    "1. **\"Deep Learning\" by Goodfellow, Bengio, and Courville** - Chapter 7 (Regularization) and Chapter 8 (Optimization)\n",
    "2. **\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" by Aurélien Géron** - Practical implementations\n",
    "3. **\"Neural Networks and Deep Learning\" by Michael Nielsen** - Free online book with interactive examples\n",
    "\n",
    "### Research Papers\n",
    "1. **\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\"** by Srivastava et al. (2014)\n",
    "2. **\"Batch Normalization: Accelerating Deep Network Training\"** by Ioffe and Szegedy (2015)\n",
    "3. **\"Adam: A Method for Stochastic Optimization\"** by Kingma and Ba (2015)\n",
    "\n",
    "### Interactive Tools\n",
    "1. **TensorFlow Playground** - http://playground.tensorflow.org/ - Visualize neural network training\n",
    "2. **Netron** - https://netron.app/ - Visualize neural network architectures\n",
    "3. **Weights & Biases** - https://wandb.ai/ - Experiment tracking and visualization\n",
    "\n",
    "### Documentation\n",
    "1. **Keras API Documentation** - https://keras.io/api/ - Comprehensive API reference\n",
    "2. **PyTorch Tutorials** - https://pytorch.org/tutorials/ - Alternative deep learning framework\n",
    "3. **Scikit-learn Metrics** - https://scikit-learn.org/stable/modules/model_evaluation.html - Evaluation metrics guide\n",
    "\n",
    "### Recommended Next Steps\n",
    "- Experiment with real-world datasets (Kaggle, UCI ML Repository)\n",
    "- Implement neural networks from scratch to understand internals\n",
    "- Participate in ML competitions to apply tuning techniques\n",
    "- Read recent papers on arxiv.org for cutting-edge techniques\n",
    "- Join ML communities (Reddit r/MachineLearning, ML Discord servers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on completing Day 50! You've reached the halfway point in your 100 Days of Machine Learning journey, and you've gained crucial skills in evaluating and tuning neural networks.\n",
    "\n",
    "Today you learned:\n",
    "- How to evaluate neural networks using appropriate metrics\n",
    "- The importance of regularization in preventing overfitting\n",
    "- Techniques for hyperparameter tuning\n",
    "- How to visualize and diagnose model behavior\n",
    "\n",
    "These skills form the foundation for building robust, production-ready machine learning systems. As you continue your journey, you'll apply these techniques to increasingly complex architectures like CNNs, RNNs, and Transformers.\n",
    "\n",
    "Keep experimenting, stay curious, and remember: **the best model is not always the most complex one, but the one that generalizes best to unseen data.**\n",
    "\n",
    "See you on Day 51, where we'll dive into Convolutional Neural Networks (CNNs) for image processing!\n",
    "\n",
    "---\n",
    "\n",
    "*\"The goal is to turn data into information, and information into insight.\"* - Carly Fiorina"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

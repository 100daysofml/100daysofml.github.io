{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0067ea",
   "metadata": {},
   "source": [
    "# Day 47: Feedforward Neural Networks and Activation Functions\n",
    "\n",
    "Welcome to Day 47 of the 100 Days of Machine Learning Challenge! Today, we embark on an exciting journey into the world of deep learning by exploring feedforward neural networks and activation functions. These concepts form the foundation of modern deep learning and artificial intelligence.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Feedforward neural networks (FFNNs), also known as multilayer perceptrons (MLPs), represent one of the most fundamental architectures in deep learning. Unlike the simple perceptron we might have encountered earlier, feedforward neural networks can model complex, non-linear relationships in data through the use of multiple layers and non-linear activation functions.\n",
    "\n",
    "### Why Feedforward Neural Networks Matter\n",
    "\n",
    "In machine learning, we've learned about linear models like linear regression and logistic regression. While these models are powerful, they're limited to learning linear relationships. Real-world problems often involve highly complex, non-linear patterns that linear models cannot capture. Feedforward neural networks bridge this gap by:\n",
    "\n",
    "1. **Modeling Non-Linearity**: Through activation functions, neural networks can learn complex non-linear patterns\n",
    "2. **Feature Learning**: Networks automatically learn useful representations of data through hidden layers\n",
    "3. **Universal Approximation**: Theoretically, a neural network with sufficient neurons can approximate any continuous function\n",
    "4. **Scalability**: They can handle high-dimensional data and large datasets effectively\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "- Understand the architecture of feedforward neural networks\n",
    "- Explain the role of activation functions in neural networks\n",
    "- Implement different activation functions and visualize their properties\n",
    "- Build and train a feedforward neural network using scikit-learn\n",
    "- Apply neural networks to real-world classification problems\n",
    "- Interpret the results and evaluate network performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1fafac",
   "metadata": {},
   "source": [
    "## Theoretical Foundation\n",
    "\n",
    "### What is a Feedforward Neural Network?\n",
    "\n",
    "A feedforward neural network is a computational model inspired by biological neural networks in the human brain. Information flows in one direction—from input to output—without cycles or loops, hence the term \"feedforward.\"\n",
    "\n",
    "#### Network Architecture\n",
    "\n",
    "A feedforward neural network consists of:\n",
    "\n",
    "1. **Input Layer**: Receives the raw features/data\n",
    "2. **Hidden Layer(s)**: Intermediate layers that transform the input\n",
    "3. **Output Layer**: Produces the final prediction\n",
    "\n",
    "Each layer contains multiple **neurons** (also called nodes or units), and neurons in adjacent layers are connected by **weights**. Each neuron also has a **bias** term.\n",
    "\n",
    "#### Mathematical Representation\n",
    "\n",
    "For a single neuron, the computation can be expressed as:\n",
    "\n",
    "$$z = w_1x_1 + w_2x_2 + ... + w_nx_n + b = \\sum_{i=1}^{n} w_ix_i + b = \\mathbf{w}^T\\mathbf{x} + b$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ are the input features\n",
    "- $w_i$ are the weights\n",
    "- $b$ is the bias term\n",
    "- $z$ is the weighted sum (also called pre-activation)\n",
    "\n",
    "The neuron then applies an **activation function** $f$ to produce its output:\n",
    "\n",
    "$$a = f(z) = f(\\mathbf{w}^T\\mathbf{x} + b)$$\n",
    "\n",
    "For a complete layer with $m$ neurons, we can write this in matrix form:\n",
    "\n",
    "$$\\mathbf{Z} = \\mathbf{W}\\mathbf{X} + \\mathbf{b}$$\n",
    "$$\\mathbf{A} = f(\\mathbf{Z})$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{X}$ is the input vector or matrix\n",
    "- $\\mathbf{W}$ is the weight matrix\n",
    "- $\\mathbf{b}$ is the bias vector\n",
    "- $\\mathbf{Z}$ is the pre-activation\n",
    "- $\\mathbf{A}$ is the activation (output)\n",
    "\n",
    "### Why Do We Need Activation Functions?\n",
    "\n",
    "Without activation functions, no matter how many layers we stack, the network would still be equivalent to a single-layer linear model. This is because the composition of linear functions is still linear:\n",
    "\n",
    "$$f(g(x)) = W_2(W_1x + b_1) + b_2 = (W_2W_1)x + (W_2b_1 + b_2) = W_{combined}x + b_{combined}$$\n",
    "\n",
    "Activation functions introduce **non-linearity**, allowing the network to learn complex patterns and approximate non-linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651e5f7",
   "metadata": {},
   "source": [
    "## Common Activation Functions\n",
    "\n",
    "Let's explore the most important activation functions used in neural networks:\n",
    "\n",
    "### 1. Sigmoid Function\n",
    "\n",
    "The sigmoid function squashes input values to the range (0, 1):\n",
    "\n",
    "$$\\sigma(z) = \f",
    "rac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**Properties:**\n",
    "- Output range: (0, 1)\n",
    "- S-shaped curve\n",
    "- Useful for binary classification in output layer\n",
    "- Derivative: $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n",
    "\n",
    "**Drawbacks:**\n",
    "- Vanishing gradient problem for very large or small inputs\n",
    "- Outputs not zero-centered\n",
    "- Computationally expensive (exponential function)\n",
    "\n",
    "### 2. Hyperbolic Tangent (tanh)\n",
    "\n",
    "The tanh function is similar to sigmoid but maps inputs to (-1, 1):\n",
    "\n",
    "$$\tanh(z) = \f",
    "rac{e^z - e^{-z}}{e^z + e^{-z}} = \f",
    "rac{e^{2z} - 1}{e^{2z} + 1}$$\n",
    "\n",
    "**Properties:**\n",
    "- Output range: (-1, 1)\n",
    "- Zero-centered (better than sigmoid)\n",
    "- S-shaped curve\n",
    "- Derivative: $\tanh'(z) = 1 - \tanh^2(z)$\n",
    "\n",
    "**Drawbacks:**\n",
    "- Still suffers from vanishing gradient problem\n",
    "- Computationally expensive\n",
    "\n",
    "### 3. Rectified Linear Unit (ReLU)\n",
    "\n",
    "ReLU is the most popular activation function in deep learning:\n",
    "\n",
    "$$\text{ReLU}(z) = \\max(0, z) = \begin{cases} z & \text{if } z > 0 \\\\ 0 & \text{if } z \\leq 0 \\end{cases}$$\n",
    "\n",
    "**Properties:**\n",
    "- Output range: [0, ∞)\n",
    "- Very simple and fast to compute\n",
    "- Doesn't saturate for positive values\n",
    "- Derivative: $\text{ReLU}'(z) = \begin{cases} 1 & \text{if } z > 0 \\\\ 0 & \text{if } z \\leq 0 \\end{cases}$\n",
    "\n",
    "**Advantages:**\n",
    "- Computationally efficient\n",
    "- Helps mitigate vanishing gradient problem\n",
    "- Leads to sparse activations (some neurons output 0)\n",
    "\n",
    "**Drawbacks:**\n",
    "- \"Dying ReLU\" problem: neurons can get stuck outputting 0\n",
    "- Not zero-centered\n",
    "- Non-differentiable at z=0 (though not a problem in practice)\n",
    "\n",
    "### 4. Leaky ReLU\n",
    "\n",
    "Leaky ReLU addresses the dying ReLU problem by allowing small negative values:\n",
    "\n",
    "$$\text{Leaky ReLU}(z) = \\max(\u0007lpha z, z) = \begin{cases} z & \text{if } z > 0 \\\\ \u0007lpha z & \text{if } z \\leq 0 \\end{cases}$$\n",
    "\n",
    "Where $\u0007lpha$ is a small constant (typically 0.01).\n",
    "\n",
    "**Properties:**\n",
    "- Output range: (-∞, ∞)\n",
    "- Prevents dying neurons\n",
    "- All benefits of ReLU\n",
    "\n",
    "### 5. Softmax Function\n",
    "\n",
    "Softmax is used in the output layer for multi-class classification:\n",
    "\n",
    "$$\text{softmax}(z_i) = \f",
    "rac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "\n",
    "Where $K$ is the number of classes.\n",
    "\n",
    "**Properties:**\n",
    "- Outputs sum to 1 (can be interpreted as probabilities)\n",
    "- Used for multi-class classification\n",
    "- Each output is between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d67e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "NumPy version: 2.3.4\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef72dca",
   "metadata": {},
   "source": [
    "## Visualizing Activation Functions\n",
    "\n",
    "Let's visualize these activation functions to better understand their behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906fcaeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Define activation functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "# Generate input values\n",
    "z = np.linspace(-10, 10, 1000)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot Sigmoid\n",
    "axes[0, 0].plot(z, sigmoid(z), 'b-', linewidth=2, label='Sigmoid')\n",
    "axes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 0].axhline(y=1, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_title('Sigmoid Activation Function', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('z')\n",
    "axes[0, 0].set_ylabel('σ(z)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].text(-8, 0.5, r'$\\sigma(z) = \\frac{1}{1 + e^{-z}}$', fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Plot Tanh\n",
    "axes[0, 1].plot(z, tanh(z), 'g-', linewidth=2, label='Tanh')\n",
    "axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 1].axhline(y=1, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 1].axhline(y=-1, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_title('Tanh Activation Function', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('z')\n",
    "axes[0, 1].set_ylabel('tanh(z)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].text(-8, 0.5, r'$tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$', fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "# Plot ReLU\n",
    "axes[1, 0].plot(z, relu(z), 'r-', linewidth=2, label='ReLU')\n",
    "axes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_title('ReLU Activation Function', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('z')\n",
    "axes[1, 0].set_ylabel('ReLU(z)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].text(-8, 5, r'$ReLU(z) = max(0, z)$', fontsize=12, bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\n",
    "\n",
    "# Plot Leaky ReLU\n",
    "axes[1, 1].plot(z, leaky_relu(z), 'm-', linewidth=2, label='Leaky ReLU (α=0.01)')\n",
    "axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_title('Leaky ReLU Activation Function', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('z')\n",
    "axes[1, 1].set_ylabel('Leaky ReLU(z)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].text(-8, 5, r'$Leaky\\ ReLU(z) = max(\\alpha z, z)$', fontsize=12, bbox=dict(boxstyle='round', facecolor='plum', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"1. Sigmoid: Smooth S-curve, outputs in (0,1), saturates at extremes\")\n",
    "print(\"2. Tanh: Similar to sigmoid but zero-centered, outputs in (-1,1)\")\n",
    "print(\"3. ReLU: Simple linear for positive values, zero for negative\")\n",
    "print(\"4. Leaky ReLU: Similar to ReLU but allows small negative values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112b1b63",
   "metadata": {},
   "source": [
    "## Understanding Derivatives of Activation Functions\n",
    "\n",
    "The derivatives of activation functions are crucial for backpropagation (which we'll explore in the next lesson). Let's visualize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9975b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Define derivatives\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def leaky_relu_derivative(z, alpha=0.01):\n",
    "    return np.where(z > 0, 1, alpha)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Sigmoid derivative\n",
    "axes[0, 0].plot(z, sigmoid_derivative(z), 'b-', linewidth=2)\n",
    "axes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_title(\"Sigmoid Derivative\", fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('z')\n",
    "axes[0, 0].set_ylabel(\"σ'(z)\")\n",
    "axes[0, 0].fill_between(z, 0, sigmoid_derivative(z), alpha=0.3)\n",
    "\n",
    "# Tanh derivative\n",
    "axes[0, 1].plot(z, tanh_derivative(z), 'g-', linewidth=2)\n",
    "axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_title(\"Tanh Derivative\", fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('z')\n",
    "axes[0, 1].set_ylabel(\"tanh'(z)\")\n",
    "axes[0, 1].fill_between(z, 0, tanh_derivative(z), alpha=0.3, color='green')\n",
    "\n",
    "# ReLU derivative\n",
    "axes[1, 0].plot(z, relu_derivative(z), 'r-', linewidth=2)\n",
    "axes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_title(\"ReLU Derivative\", fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('z')\n",
    "axes[1, 0].set_ylabel(\"ReLU'(z)\")\n",
    "axes[1, 0].fill_between(z, 0, relu_derivative(z), alpha=0.3, color='red')\n",
    "\n",
    "# Leaky ReLU derivative\n",
    "axes[1, 1].plot(z, leaky_relu_derivative(z), 'm-', linewidth=2)\n",
    "axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_title(\"Leaky ReLU Derivative\", fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('z')\n",
    "axes[1, 1].set_ylabel(\"Leaky ReLU'(z)\")\n",
    "axes[1, 1].fill_between(z, 0, leaky_relu_derivative(z), alpha=0.3, color='magenta')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDerivative Insights:\")\n",
    "print(\"1. Sigmoid/Tanh: Derivatives approach 0 at extremes (vanishing gradient)\")\n",
    "print(\"2. ReLU: Derivative is 1 for positive, 0 for negative (can die)\")\n",
    "print(\"3. Leaky ReLU: Always has a non-zero gradient (prevents dying neurons)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6af04b7",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network Architecture\n",
    "\n",
    "A typical feedforward neural network processes data through multiple layers. Let's visualize how data flows through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe6c2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Simple visualization of network architecture\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "# Network structure: 4 input, 5 hidden, 3 output\n",
    "layer_sizes = [4, 5, 3]\n",
    "layer_names = ['Input Layer\\n(4 features)', 'Hidden Layer\\n(5 neurons)', 'Output Layer\\n(3 classes)']\n",
    "\n",
    "# Vertical positions for each layer\n",
    "v_spacing = 1.0 / max(layer_sizes)\n",
    "h_spacing = 1.0 / (len(layer_sizes) - 1)\n",
    "\n",
    "# Draw neurons\n",
    "for n, (layer_size, layer_name) in enumerate(zip(layer_sizes, layer_names)):\n",
    "    layer_top = v_spacing * (layer_size - 1) / 2\n",
    "    for m in range(layer_size):\n",
    "        x = n * h_spacing\n",
    "        y = layer_top - m * v_spacing\n",
    "        circle = plt.Circle((x, y), v_spacing/4, color='steelblue', ec='black', linewidth=2, zorder=4)\n",
    "        ax.add_artist(circle)\n",
    "        \n",
    "        # Add labels for input and output\n",
    "        if n == 0:\n",
    "            ax.text(x - 0.15, y, f'$x_{m+1}$', ha='right', va='center', fontsize=11)\n",
    "        elif n == len(layer_sizes) - 1:\n",
    "            ax.text(x + 0.15, y, f'$y_{m+1}$', ha='left', va='center', fontsize=11)\n",
    "\n",
    "# Draw connections\n",
    "for n in range(len(layer_sizes) - 1):\n",
    "    layer_size_a = layer_sizes[n]\n",
    "    layer_size_b = layer_sizes[n + 1]\n",
    "    layer_top_a = v_spacing * (layer_size_a - 1) / 2\n",
    "    layer_top_b = v_spacing * (layer_size_b - 1) / 2\n",
    "    \n",
    "    for m in range(layer_size_a):\n",
    "        for o in range(layer_size_b):\n",
    "            x1 = n * h_spacing\n",
    "            y1 = layer_top_a - m * v_spacing\n",
    "            x2 = (n + 1) * h_spacing\n",
    "            y2 = layer_top_b - o * v_spacing\n",
    "            line = plt.Line2D([x1, x2], [y1, y2], c='gray', alpha=0.3, linewidth=1, zorder=1)\n",
    "            ax.add_artist(line)\n",
    "\n",
    "# Add layer labels\n",
    "for n, layer_name in enumerate(layer_names):\n",
    "    x = n * h_spacing\n",
    "    ax.text(x, -0.4, layer_name, ha='center', fontsize=12, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "\n",
    "# Add arrows showing flow\n",
    "arrow_y = 0.6\n",
    "for n in range(len(layer_sizes) - 1):\n",
    "    x1 = n * h_spacing + 0.05\n",
    "    x2 = (n + 1) * h_spacing - 0.05\n",
    "    ax.annotate('', xy=(x2, arrow_y), xytext=(x1, arrow_y),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
    "\n",
    "ax.text(0.5, 0.7, 'Forward Pass Direction', ha='center', fontsize=12, color='red', fontweight='bold')\n",
    "\n",
    "ax.set_xlim(-0.3, 1.3)\n",
    "ax.set_ylim(-0.6, 0.8)\n",
    "ax.axis('off')\n",
    "ax.set_title('Feedforward Neural Network Architecture', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nArchitecture Details:\")\n",
    "print(\"- Input Layer: Receives raw features (no activation)\")\n",
    "print(\"- Hidden Layer: Applies weights, bias, and activation function\")\n",
    "print(\"- Output Layer: Produces final predictions\")\n",
    "print(\"- Each connection has an associated weight\")\n",
    "print(\"- Information flows forward: Input → Hidden → Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700bc7a9",
   "metadata": {},
   "source": [
    "## Python Implementation\n",
    "\n",
    "### Building a Feedforward Neural Network with scikit-learn\n",
    "\n",
    "Scikit-learn provides the `MLPClassifier` (Multi-Layer Perceptron Classifier) for building feedforward neural networks. Let's implement a neural network for a binary classification task.\n",
    "\n",
    "#### Step 1: Generate Synthetic Data\n",
    "\n",
    "We'll use the `make_moons` dataset, which creates two interleaving half-moon shapes - perfect for demonstrating neural networks' ability to learn non-linear decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f104f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_moons' is not defined",
     "output_type": "error",
     "traceback": [
      "name 'make_moons' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate non-linear dataset\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \n",
    "            c='blue', marker='o', label='Class 0', alpha=0.6, edgecolors='k')\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \n",
    "            c='red', marker='s', label='Class 1', alpha=0.6, edgecolors='k')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Training Data: Moons Dataset (Non-Linear)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d0543",
   "metadata": {},
   "source": [
    "#### Step 2: Data Preprocessing\n",
    "\n",
    "Neural networks work best when features are scaled. We'll use standardization to ensure all features have mean 0 and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6a86a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Original data statistics:\")\n",
    "print(f\"Mean: {X_train.mean(axis=0)}\")\n",
    "print(f\"Std: {X_train.std(axis=0)}\")\n",
    "print(f\"\\nScaled data statistics:\")\n",
    "print(f\"Mean: {X_train_scaled.mean(axis=0)}\")\n",
    "print(f\"Std: {X_train_scaled.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff5602",
   "metadata": {},
   "source": [
    "#### Step 3: Build and Train the Neural Network\n",
    "\n",
    "We'll create a feedforward neural network with:\n",
    "- Input layer: 2 features\n",
    "- Hidden layer 1: 10 neurons with ReLU activation\n",
    "- Hidden layer 2: 5 neurons with ReLU activation  \n",
    "- Output layer: 2 classes (binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef6dc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLPClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "name 'MLPClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the neural network\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(10, 5),  # Two hidden layers with 10 and 5 neurons\n",
    "    activation='relu',            # ReLU activation function\n",
    "    solver='adam',                # Adam optimizer\n",
    "    max_iter=1000,               # Maximum iterations\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = mlp.predict(X_train_scaled)\n",
    "y_test_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Model Training Complete!\")\n",
    "print(f\"\\nNetwork Architecture:\")\n",
    "print(f\"- Input Layer: {X_train.shape[1]} neurons\")\n",
    "print(f\"- Hidden Layer 1: {mlp.hidden_layer_sizes[0]} neurons (ReLU)\")\n",
    "print(f\"- Hidden Layer 2: {mlp.hidden_layer_sizes[1]} neurons (ReLU)\")\n",
    "print(f\"- Output Layer: {len(np.unique(y))} neurons\")\n",
    "print(f\"\\nTotal Parameters: {sum([w.size for w in mlp.coefs_]) + sum([b.size for b in mlp.intercepts_])}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"\\nConverged: {mlp.n_iter_} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ffe84",
   "metadata": {},
   "source": [
    "#### Step 4: Visualize Decision Boundary\n",
    "\n",
    "One of the best ways to understand how a neural network learns is to visualize its decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb1bc5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a mesh for decision boundary\n",
    "x_min, x_max = X_train_scaled[:, 0].min() - 0.5, X_train_scaled[:, 0].max() + 0.5\n",
    "y_min, y_max = X_train_scaled[:, 1].min() - 0.5, X_train_scaled[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "# Predict for each point in the mesh\n",
    "Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Training data decision boundary\n",
    "axes[0].contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "axes[0].scatter(X_train_scaled[y_train == 0, 0], X_train_scaled[y_train == 0, 1],\n",
    "                c='blue', marker='o', label='Class 0', edgecolors='k', alpha=0.7)\n",
    "axes[0].scatter(X_train_scaled[y_train == 1, 0], X_train_scaled[y_train == 1, 1],\n",
    "                c='red', marker='s', label='Class 1', edgecolors='k', alpha=0.7)\n",
    "axes[0].set_xlabel('Feature 1 (scaled)', fontsize=12)\n",
    "axes[0].set_ylabel('Feature 2 (scaled)', fontsize=12)\n",
    "axes[0].set_title(f'Training Set Decision Boundary\\nAccuracy: {train_accuracy:.4f}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test data decision boundary\n",
    "axes[1].contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "axes[1].scatter(X_test_scaled[y_test == 0, 0], X_test_scaled[y_test == 0, 1],\n",
    "                c='blue', marker='o', label='Class 0', edgecolors='k', alpha=0.7)\n",
    "axes[1].scatter(X_test_scaled[y_test == 1, 0], X_test_scaled[y_test == 1, 1],\n",
    "                c='red', marker='s', label='Class 1', edgecolors='k', alpha=0.7)\n",
    "axes[1].set_xlabel('Feature 1 (scaled)', fontsize=12)\n",
    "axes[1].set_ylabel('Feature 2 (scaled)', fontsize=12)\n",
    "axes[1].set_title(f'Test Set Decision Boundary\\nAccuracy: {test_accuracy:.4f}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- The neural network learned a complex, non-linear decision boundary\")\n",
    "print(\"- The boundary successfully separates the two moon-shaped classes\")\n",
    "print(\"- This would be impossible with a linear classifier like logistic regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539c633",
   "metadata": {},
   "source": [
    "## Comparing Different Activation Functions\n",
    "\n",
    "Let's compare how different activation functions perform on the same dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa57db6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Train models with different activation functions\n",
    "activations = ['logistic', 'tanh', 'relu']\n",
    "results = {}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, activation in enumerate(activations):\n",
    "    # Train model\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(10, 5),\n",
    "        activation=activation,\n",
    "        solver='adam',\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[activation] = acc\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "    axes[idx].scatter(X_test_scaled[y_test == 0, 0], X_test_scaled[y_test == 0, 1],\n",
    "                      c='blue', marker='o', label='Class 0', edgecolors='k', alpha=0.7, s=30)\n",
    "    axes[idx].scatter(X_test_scaled[y_test == 1, 0], X_test_scaled[y_test == 1, 1],\n",
    "                      c='red', marker='s', label='Class 1', edgecolors='k', alpha=0.7, s=30)\n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=11)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=11)\n",
    "    axes[idx].set_title(f'{activation.upper()} Activation\\nAccuracy: {acc:.4f}', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\nActivation Function Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "for activation, acc in results.items():\n",
    "    print(f\"{activation.upper():>12}: {acc:.4f}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"\\nBest: {max(results, key=results.get).upper()} with {max(results.values()):.4f} accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c6735",
   "metadata": {},
   "source": [
    "## Hands-On Exercise: Multi-Class Classification\n",
    "\n",
    "Now it's your turn! Let's apply what we've learned to a more complex multi-class classification problem using the Iris dataset - a classic machine learning dataset.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "The Iris dataset contains measurements of iris flowers from three different species:\n",
    "- Setosa\n",
    "- Versicolor  \n",
    "- Virginica\n",
    "\n",
    "We'll build a neural network to classify iris flowers based on four features:\n",
    "1. Sepal length\n",
    "2. Sepal width\n",
    "3. Petal length\n",
    "4. Petal width\n",
    "\n",
    "Let's load and explore the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bebf2f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "print(\"Iris Dataset Information:\")\n",
    "print(f\"Samples: {X_iris.shape[0]}\")\n",
    "print(f\"Features: {X_iris.shape[1]}\")\n",
    "print(f\"Classes: {len(np.unique(y_iris))}\")\n",
    "print(f\"\\nFeature names: {iris.feature_names}\")\n",
    "print(f\"Class names: {iris.target_names}\")\n",
    "\n",
    "# Display sample data\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(X_iris, columns=iris.feature_names)\n",
    "df['species'] = [iris.target_names[i] for i in y_iris]\n",
    "print(f\"\\nFirst 5 samples:\")\n",
    "print(df.head())\n",
    "\n",
    "# Class distribution\n",
    "print(f\"\\nClass distribution:\")\n",
    "for i, name in enumerate(iris.target_names):\n",
    "    count = np.sum(y_iris == i)\n",
    "    print(f\"{name}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7abb8",
   "metadata": {},
   "source": [
    "### Visualizing the Iris Dataset\n",
    "\n",
    "Let's visualize the relationships between features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee298726",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize pairwise feature relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "feature_pairs = [(0, 1), (0, 2), (0, 3), (2, 3)]\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "for idx, (f1, f2) in enumerate(feature_pairs):\n",
    "    for class_idx, class_name in enumerate(iris.target_names):\n",
    "        mask = y_iris == class_idx\n",
    "        axes[idx].scatter(X_iris[mask, f1], X_iris[mask, f2],\n",
    "                         c=colors[class_idx], label=class_name,\n",
    "                         alpha=0.6, edgecolors='k', s=50)\n",
    "    \n",
    "    axes[idx].set_xlabel(iris.feature_names[f1], fontsize=11)\n",
    "    axes[idx].set_ylabel(iris.feature_names[f2], fontsize=11)\n",
    "    axes[idx].set_title(f'{iris.feature_names[f1]} vs {iris.feature_names[f2]}',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Setosa (blue) is clearly separable from the other two classes\")\n",
    "print(\"- Versicolor (red) and Virginica (green) have some overlap\")\n",
    "print(\"- Petal measurements seem more discriminative than sepal measurements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3381137d",
   "metadata": {},
   "source": [
    "### Building the Multi-Class Neural Network\n",
    "\n",
    "Now let's build, train, and evaluate a neural network for this 3-class classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f67e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_iris = StandardScaler()\n",
    "X_train_iris_scaled = scaler_iris.fit_transform(X_train_iris)\n",
    "X_test_iris_scaled = scaler_iris.transform(X_test_iris)\n",
    "\n",
    "# Build neural network with 2 hidden layers\n",
    "mlp_iris = MLPClassifier(\n",
    "    hidden_layer_sizes=(20, 10),  # 20 neurons in first hidden layer, 10 in second\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=2000,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp_iris.fit(X_train_iris_scaled, y_train_iris)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_iris = mlp_iris.predict(X_train_iris_scaled)\n",
    "y_test_pred_iris = mlp_iris.predict(X_test_iris_scaled)\n",
    "\n",
    "# Calculate accuracies\n",
    "train_acc_iris = accuracy_score(y_train_iris, y_train_pred_iris)\n",
    "test_acc_iris = accuracy_score(y_test_iris, y_test_pred_iris)\n",
    "\n",
    "print(\"Multi-Class Neural Network Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nNetwork Architecture:\")\n",
    "print(f\"- Input Layer: 4 neurons (4 features)\")\n",
    "print(f\"- Hidden Layer 1: 20 neurons (ReLU)\")\n",
    "print(f\"- Hidden Layer 2: 10 neurons (ReLU)\")\n",
    "print(f\"- Output Layer: 3 neurons (3 classes)\")\n",
    "print(f\"\\nTotal Parameters: {sum([w.size for w in mlp_iris.coefs_]) + sum([b.size for b in mlp_iris.intercepts_])}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"- Training Accuracy: {train_acc_iris:.4f} ({train_acc_iris*100:.2f}%)\")\n",
    "print(f\"- Test Accuracy: {test_acc_iris:.4f} ({test_acc_iris*100:.2f}%)\")\n",
    "print(f\"\\nTraining completed in {mlp_iris.n_iter_} iterations\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nDetailed Classification Report (Test Set):\")\n",
    "print(classification_report(y_test_iris, y_test_pred_iris, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b71e41",
   "metadata": {},
   "source": [
    "### Analyzing Performance with Confusion Matrix\n",
    "\n",
    "A confusion matrix helps us understand which classes the model confuses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd7b7cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test_iris, y_test_pred_iris)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)\n",
    "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
    "ax.set_title('Confusion Matrix - Iris Classification', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "for i, class_name in enumerate(iris.target_names):\n",
    "    correct = cm[i, i]\n",
    "    total = cm[i, :].sum()\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"{class_name:>12}: {correct}/{total} correct ({accuracy:.1f}%)\")\n",
    "\n",
    "# Calculate per-class errors\n",
    "print(f\"\\nMisclassification Analysis:\")\n",
    "for i in range(len(iris.target_names)):\n",
    "    for j in range(len(iris.target_names)):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            print(f\"  {cm[i, j]} {iris.target_names[i]} classified as {iris.target_names[j]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7458b23d",
   "metadata": {},
   "source": [
    "### Understanding Prediction Probabilities\n",
    "\n",
    "Neural networks can also output probability estimates for each class. Let's examine some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917b558",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp_iris' is not defined",
     "output_type": "error",
     "traceback": [
      "name 'mlp_iris' is not defined"
     ]
    }
   ],
   "source": [
    "# Get probability predictions\n",
    "y_proba = mlp_iris.predict_proba(X_test_iris_scaled)\n",
    "\n",
    "# Display predictions for first 10 test samples\n",
    "print(\"Sample Predictions with Probabilities:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Sample':<8} {'True':<12} {'Predicted':<12} {'Confidence':<12} {'Probabilities':<30}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i in range(10):\n",
    "    true_class = iris.target_names[y_test_iris[i]]\n",
    "    pred_class = iris.target_names[y_test_pred_iris[i]]\n",
    "    confidence = y_proba[i].max()\n",
    "    probs = ', '.join([f'{p:.3f}' for p in y_proba[i]])\n",
    "    \n",
    "    marker = \"✓\" if y_test_iris[i] == y_test_pred_iris[i] else \"✗\"\n",
    "    print(f\"{i+1:<8} {true_class:<12} {pred_class:<12} {confidence:<12.3f} [{probs}] {marker}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Visualize probability distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "x_pos = np.arange(len(y_test_iris))\n",
    "width = 0.25\n",
    "\n",
    "for class_idx in range(3):\n",
    "    ax.bar(x_pos + class_idx * width, y_proba[:, class_idx], width,\n",
    "           label=iris.target_names[class_idx], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Test Sample Index', fontsize=12)\n",
    "ax.set_ylabel('Probability', fontsize=12)\n",
    "ax.set_title('Class Probability Predictions for All Test Samples', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage prediction confidence: {y_proba.max(axis=1).mean():.4f}\")\n",
    "print(f\"Minimum prediction confidence: {y_proba.max(axis=1).min():.4f}\")\n",
    "print(f\"Maximum prediction confidence: {y_proba.max(axis=1).max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b293feaa",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "Congratulations on completing Day 47! Let's review the essential concepts:\n",
    "\n",
    "### 1. Feedforward Neural Networks\n",
    "- FFNNs are composed of layers: input, hidden, and output\n",
    "- Information flows forward through the network without cycles\n",
    "- Each neuron computes a weighted sum plus bias, then applies an activation function\n",
    "- Networks can learn complex, non-linear patterns through multiple layers\n",
    "\n",
    "### 2. Activation Functions\n",
    "- **Essential for non-linearity**: Without activation functions, networks are just linear models\n",
    "- **Sigmoid**: Outputs (0,1), useful for binary output, suffers from vanishing gradients\n",
    "- **Tanh**: Outputs (-1,1), zero-centered, still has vanishing gradient issues\n",
    "- **ReLU**: Most popular, simple and effective, can suffer from dying neurons\n",
    "- **Leaky ReLU**: Addresses dying ReLU problem with small negative slope\n",
    "\n",
    "### 3. Network Architecture\n",
    "- Input layer size = number of features\n",
    "- Hidden layer sizes are hyperparameters to tune\n",
    "- Output layer size = number of classes (or 1 for binary classification)\n",
    "- More layers and neurons = more capacity but risk of overfitting\n",
    "\n",
    "### 4. Best Practices\n",
    "- Always scale/normalize your input features\n",
    "- Start with ReLU activation for hidden layers\n",
    "- Use appropriate output activation (sigmoid for binary, softmax for multi-class)\n",
    "- Monitor both training and test accuracy to detect overfitting\n",
    "- Experiment with different architectures and hyperparameters\n",
    "\n",
    "### 5. Practical Insights\n",
    "- Neural networks excel at learning non-linear decision boundaries\n",
    "- They can achieve high accuracy on complex classification tasks\n",
    "- Proper preprocessing (feature scaling) is crucial\n",
    "- Understanding prediction probabilities helps interpret model confidence\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In the coming lessons, we'll dive deeper into:\n",
    "- **Day 48**: Backpropagation - how neural networks actually learn\n",
    "- **Day 49**: Training techniques, loss functions, and optimizers\n",
    "- **Day 50**: Evaluation, regularization, and hyperparameter tuning\n",
    "- Future weeks: Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258668f",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "To deepen your understanding of feedforward neural networks and activation functions, explore these resources:\n",
    "\n",
    "### Online Courses and Tutorials\n",
    "1. **Neural Networks and Deep Learning** by deeplearning.ai on Coursera\n",
    "   - Comprehensive introduction to neural networks\n",
    "   - https://www.coursera.org/learn/neural-networks-deep-learning\n",
    "\n",
    "2. **3Blue1Brown - Neural Networks Playlist**\n",
    "   - Excellent visual explanations of how neural networks work\n",
    "   - https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n",
    "\n",
    "3. **Stanford CS231n: Convolutional Neural Networks**\n",
    "   - Lecture notes on neural networks and activation functions\n",
    "   - http://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "### Documentation and References\n",
    "4. **Scikit-learn MLPClassifier Documentation**\n",
    "   - Complete API reference and examples\n",
    "   - https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "5. **Deep Learning Book by Goodfellow, Bengio, and Courville**\n",
    "   - Chapter 6: Deep Feedforward Networks\n",
    "   - https://www.deeplearningbook.org/contents/mlp.html\n",
    "\n",
    "### Research Papers\n",
    "6. **\"Understanding the difficulty of training deep feedforward neural networks\"** by Glorot and Bengio (2010)\n",
    "   - Important insights on initialization and activation functions\n",
    "   - http://proceedings.mlr.press/v9/glorot10a.html\n",
    "\n",
    "### Interactive Resources\n",
    "7. **TensorFlow Playground**\n",
    "   - Interactive visualization of neural networks\n",
    "   - https://playground.tensorflow.org/\n",
    "\n",
    "8. **Neural Network Playground by ConvNetJS**\n",
    "   - Experiment with different architectures in your browser\n",
    "   - https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html\n",
    "\n",
    "### Practice Datasets\n",
    "9. **UCI Machine Learning Repository**\n",
    "   - Hundreds of datasets for practice\n",
    "   - https://archive.ics.uci.edu/ml/index.php\n",
    "\n",
    "10. **Kaggle Datasets**\n",
    "    - Real-world datasets and competitions\n",
    "    - https://www.kaggle.com/datasets\n",
    "\n",
    "Happy learning! Remember, the best way to master neural networks is through hands-on practice and experimentation."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Day 71: Grid search, random search, and Bayesian optimization\n\n## Introduction\n\nWelcome to Day 71 of the 100 Days of Machine Learning journey! Today, we dive into one of the most crucial aspects of building high-performing machine learning models: **hyperparameter optimization**.\n\nEvery machine learning algorithm comes with hyperparametersâ€”settings that control the learning process itself. Unlike model parameters (which are learned from data), hyperparameters must be set before training begins. The difference between a mediocre model and a state-of-the-art one often lies in finding the right hyperparameter configuration.\n\nManual tuning is tedious, time-consuming, and rarely optimal. This is where automated hyperparameter optimization techniques come in: **Grid Search**, **Random Search**, and **Bayesian Optimization**. These methods systematically explore the hyperparameter space to find configurations that maximize model performance.\n\n### Why This Matters\n\n- **Performance Gains**: Proper hyperparameter tuning can improve model accuracy by 5-20% or more\n- **Time Efficiency**: Automated methods save hundreds of hours compared to manual tuning\n- **Reproducibility**: Systematic approaches ensure consistent, documented optimization processes\n- **Production ML**: In real-world applications, optimal hyperparameters are critical for model deployment\n\n### Learning Objectives\n\nBy the end of this lesson, you will be able to:\n\n1. Understand the fundamental differences between Grid Search, Random Search, and Bayesian Optimization\n2. Implement each optimization technique using scikit-learn and modern libraries\n3. Analyze the trade-offs between exploration and exploitation in hyperparameter search\n4. Apply Bayesian Optimization using Gaussian Processes for efficient hyperparameter tuning\n5. Compare the efficiency and effectiveness of different optimization strategies\n6. Make informed decisions about which optimization method to use for different scenarios",
   "id": "cell-introduction"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Theory\n\n### Understanding Hyperparameter Optimization\n\nThe goal of hyperparameter optimization is to find the configuration $\\boldsymbol{\\lambda}^*$ that minimizes a loss function:\n\n$$\\boldsymbol{\\lambda}^* = \\arg\\min_{\\boldsymbol{\\lambda} \\in \\Lambda} \\mathcal{L}(\\boldsymbol{\\lambda})$$\n\nwhere $\\Lambda$ is the hyperparameter space and $\\mathcal{L}(\\boldsymbol{\\lambda})$ is the validation error for configuration $\\boldsymbol{\\lambda}$.\n\n### 1. Grid Search\n\n**Grid Search** is the most straightforward approach: define a grid of hyperparameter values and evaluate every possible combination.\n\n**Algorithm:**\n1. Define a grid of values for each hyperparameter\n2. Generate all possible combinations (Cartesian product)\n3. Train and evaluate a model for each combination\n4. Select the configuration with the best performance\n\n**Pros:**\n- Simple and easy to implement\n- Guaranteed to find the best combination within the grid\n- Reproducible and parallelizable\n\n**Cons:**\n- Exponential growth: $n^d$ evaluations for $n$ values per dimension and $d$ dimensions\n- Wasted computations on irrelevant hyperparameters\n- Fixed resolutionâ€”may miss optimal values between grid points\n\n**Example:** For a model with 3 hyperparameters, each with 5 values, Grid Search requires $5^3 = 125$ evaluations.\n\n### 2. Random Search\n\n**Random Search** samples hyperparameter configurations randomly from the search space.\n\n**Algorithm:**\n1. Define probability distributions for each hyperparameter\n2. Sample $n$ configurations randomly\n3. Train and evaluate each sampled configuration\n4. Select the best performing configuration\n\n**Key Insight (Bergstra & Bengio, 2012):**\nRandom Search is more efficient when only a few hyperparameters significantly affect performance. With a fixed budget of $n$ trials, Random Search explores $n$ unique values per dimension, while Grid Search may only explore $\\sqrt[d]{n}$ values.\n\n**Pros:**\n- No exponential growthâ€”budget is fixed regardless of dimensionality\n- Better exploration of the hyperparameter space\n- More likely to find good values for important hyperparameters\n- Easy to parallelize\n\n**Cons:**\n- No guarantee of finding the optimal configuration\n- May waste evaluations on poor regions of the search space\n- Doesn't learn from previous evaluations\n\n### 3. Bayesian Optimization\n\n**Bayesian Optimization** is a sequential model-based optimization technique that builds a probabilistic model of the objective function and uses it to select the most promising hyperparameters to evaluate next.\n\n**Core Components:**\n\n1. **Surrogate Model**: A probabilistic model (typically Gaussian Process) that approximates the unknown objective function\n   \n   $$f(\\boldsymbol{\\lambda}) \\sim \\mathcal{GP}(\\mu(\\boldsymbol{\\lambda}), k(\\boldsymbol{\\lambda}, \\boldsymbol{\\lambda}'))$$\n\n2. **Acquisition Function**: A function that determines the next point to evaluate by balancing exploration and exploitation\n\nCommon acquisition functions:\n- **Expected Improvement (EI)**: $EI(\\boldsymbol{\\lambda}) = \\mathbb{E}[\\max(f(\\boldsymbol{\\lambda}) - f(\\boldsymbol{\\lambda}^+), 0)]$\n- **Probability of Improvement (PI)**: $PI(\\boldsymbol{\\lambda}) = P(f(\\boldsymbol{\\lambda}) \\geq f(\\boldsymbol{\\lambda}^+))$\n- **Upper Confidence Bound (UCB)**: $UCB(\\boldsymbol{\\lambda}) = \\mu(\\boldsymbol{\\lambda}) + \\kappa \\sigma(\\boldsymbol{\\lambda})$\n\n**Algorithm:**\n1. Initialize with a few random evaluations\n2. Fit a Gaussian Process to observed data\n3. Use the acquisition function to select the next configuration\n4. Evaluate the selected configuration\n5. Update the Gaussian Process\n6. Repeat steps 3-5 until budget is exhausted\n\n**Pros:**\n- Sample efficientâ€”requires fewer evaluations than Grid or Random Search\n- Learns from previous evaluations\n- Balances exploration (uncertain regions) and exploitation (promising regions)\n- Handles expensive evaluations well\n\n**Cons:**\n- More complex to implement\n- Gaussian Process fitting becomes expensive for large datasets\n- Requires tuning the acquisition function\n- May struggle with high-dimensional spaces (>20 dimensions)\n\n### Comparison Summary\n\n| Method | Evaluations | Adaptiveness | Best Use Case |\n|--------|-------------|--------------|---------------|\n| **Grid Search** | $O(n^d)$ | None | Small search spaces, exhaustive search needed |\n| **Random Search** | $O(n)$ | None | High-dimensional spaces, quick exploration |\n| **Bayesian Optimization** | $O(n)$ | High | Expensive evaluations, limited budget |\n\n### Mathematical Insight: Expected Improvement\n\nThe Expected Improvement acquisition function at point $\\boldsymbol{\\lambda}$ is:\n\n$$EI(\\boldsymbol{\\lambda}) = \\begin{cases}\n(\\mu(\\boldsymbol{\\lambda}) - f^+ - \\xi)\\Phi(Z) + \\sigma(\\boldsymbol{\\lambda})\\phi(Z) & \\text{if } \\sigma(\\boldsymbol{\\lambda}) > 0 \\\\\n0 & \\text{if } \\sigma(\\boldsymbol{\\lambda}) = 0\n\\end{cases}$$\n\nwhere:\n- $f^+$ is the best observed value so far\n- $\\mu(\\boldsymbol{\\lambda})$ and $\\sigma(\\boldsymbol{\\lambda})$ are the mean and standard deviation from the GP\n- $\\Phi$ and $\\phi$ are the CDF and PDF of the standard normal distribution\n- $Z = \\frac{\\mu(\\boldsymbol{\\lambda}) - f^+ - \\xi}{\\sigma(\\boldsymbol{\\lambda})}$\n- $\\xi \\geq 0$ is the exploration-exploitation trade-off parameter",
   "id": "cell-theory"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer, make_classification\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom scipy.stats import uniform, randint\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# For Bayesian Optimization\ntry:\n    from skopt import BayesSearchCV\n    from skopt.space import Real, Categorical, Integer\n    from skopt.plots import plot_convergence, plot_objective\n    SKOPT_AVAILABLE = True\nexcept ImportError:\n    print(\"scikit-optimize not available. Installing...\")\n    import subprocess\n    subprocess.check_call(['pip', 'install', 'scikit-optimize'])\n    from skopt import BayesSearchCV\n    from skopt.space import Real, Categorical, Integer\n    from skopt.plots import plot_convergence, plot_objective\n    SKOPT_AVAILABLE = True\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Set plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(\"All libraries imported successfully!\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")",
   "id": "cell-imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Visualization: Comparing Optimization Methods\n\n# Create comparison DataFrame\ncomparison_df = pd.DataFrame(results).T\ncomparison_df = comparison_df.reset_index()\ncomparison_df.columns = ['Method', 'Best Params', 'CV Score', 'Test Accuracy', 'Time (s)', 'N Evaluations']\n\nprint(\"Comparison of Optimization Methods\")\nprint(\"=\" * 80)\nprint(comparison_df.to_string(index=False))\nprint(\"=\" * 80)\n\n# Create visualizations\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# 1. Test Accuracy Comparison\nax1 = axes[0, 0]\nmethods = comparison_df['Method']\ntest_accs = comparison_df['Test Accuracy']\ncolors = ['#3498db', '#e74c3c', '#2ecc71']\nbars1 = ax1.bar(methods, test_accs, color=colors, alpha=0.7, edgecolor='black')\nax1.set_ylabel('Test Accuracy', fontsize=12, fontweight='bold')\nax1.set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\nax1.set_ylim([0.9, 1.0])\nax1.grid(axis='y', alpha=0.3)\nfor bar in bars1:\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height,\n             f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n\n# 2. Computation Time Comparison\nax2 = axes[0, 1]\ntimes = comparison_df['Time (s)']\nbars2 = ax2.bar(methods, times, color=colors, alpha=0.7, edgecolor='black')\nax2.set_ylabel('Time (seconds)', fontsize=12, fontweight='bold')\nax2.set_title('Computation Time Comparison', fontsize=14, fontweight='bold')\nax2.grid(axis='y', alpha=0.3)\nfor bar in bars2:\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height,\n             f'{height:.1f}s', ha='center', va='bottom', fontsize=10)\n\n# 3. Number of Evaluations\nax3 = axes[1, 0]\nn_evals = comparison_df['N Evaluations']\nbars3 = ax3.bar(methods, n_evals, color=colors, alpha=0.7, edgecolor='black')\nax3.set_ylabel('Number of Evaluations', fontsize=12, fontweight='bold')\nax3.set_title('Number of Hyperparameter Evaluations', fontsize=14, fontweight='bold')\nax3.grid(axis='y', alpha=0.3)\nfor bar in bars3:\n    height = bar.get_height()\n    ax3.text(bar.get_x() + bar.get_width()/2., height,\n             f'{int(height)}', ha='center', va='bottom', fontsize=10)\n\n# 4. Efficiency: Accuracy per Evaluation\nax4 = axes[1, 1]\nefficiency = (comparison_df['Test Accuracy'] * 100) / comparison_df['N Evaluations']\nbars4 = ax4.bar(methods, efficiency, color=colors, alpha=0.7, edgecolor='black')\nax4.set_ylabel('Accuracy per Evaluation (%)', fontsize=12, fontweight='bold')\nax4.set_title('Efficiency: Accuracy per Evaluation', fontsize=14, fontweight='bold')\nax4.grid(axis='y', alpha=0.3)\nfor bar in bars4:\n    height = bar.get_height()\n    ax4.text(bar.get_x() + bar.get_width()/2., height,\n             f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nðŸ“Š Key Observations:\")\nprint(f\"â€¢ Grid Search evaluated {int(comparison_df.loc[0, 'N Evaluations'])} combinations exhaustively\")\nprint(f\"â€¢ Random Search evaluated {int(comparison_df.loc[1, 'N Evaluations'])} random samples\")\nprint(f\"â€¢ Bayesian Optimization evaluated only {int(comparison_df.loc[2, 'N Evaluations'])} iterations\")\nprint(f\"â€¢ Bayesian Optimization achieved competitive accuracy with {int(comparison_df.loc[2, 'N Evaluations'] / comparison_df.loc[0, 'N Evaluations'] * 100)}% of the evaluations!\")",
   "id": "cell-visualization"
  },
  {
   "cell_type": "code",
   "source": "## Bayesian Optimization Convergence Analysis\n\n# Plot convergence\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plot 1: Convergence plot\nax1 = axes[0]\ntry:\n    plot_convergence(bayes_search.optimizer_results_[0], ax=ax1)\n    ax1.set_title('Bayesian Optimization Convergence', fontsize=14, fontweight='bold')\n    ax1.set_xlabel('Number of Iterations', fontsize=12)\n    ax1.set_ylabel('Minimum Validation Error', fontsize=12)\n    ax1.grid(True, alpha=0.3)\nexcept Exception as e:\n    print(f\"Could not plot convergence: {e}\")\n    ax1.text(0.5, 0.5, 'Convergence plot unavailable', \n             ha='center', va='center', transform=ax1.transAxes)\n\n# Plot 2: Score progression for all methods\nax2 = axes[1]\n\n# Extract CV scores from Grid Search\ngrid_cv_scores = []\nfor params, score in zip(grid_search.cv_results_['params'], \n                         grid_search.cv_results_['mean_test_score']):\n    grid_cv_scores.append(score)\n\n# Extract CV scores from Random Search\nrandom_cv_scores = []\nfor params, score in zip(random_search.cv_results_['params'], \n                         random_search.cv_results_['mean_test_score']):\n    random_cv_scores.append(score)\n\n# Extract CV scores from Bayesian Search\nbayes_cv_scores = []\nfor params, score in zip(bayes_search.cv_results_['params'], \n                         bayes_search.cv_results_['mean_test_score']):\n    bayes_cv_scores.append(score)\n\n# Plot cumulative best scores\ngrid_cummax = pd.Series(grid_cv_scores).cummax()\nrandom_cummax = pd.Series(random_cv_scores).cummax()\nbayes_cummax = pd.Series(bayes_cv_scores).cummax()\n\nax2.plot(range(1, len(grid_cummax)+1), grid_cummax, \n         label='Grid Search', linewidth=2, marker='o', markersize=3)\nax2.plot(range(1, len(random_cummax)+1), random_cummax, \n         label='Random Search', linewidth=2, marker='s', markersize=3)\nax2.plot(range(1, len(bayes_cummax)+1), bayes_cummax, \n         label='Bayesian Optimization', linewidth=2, marker='^', markersize=3)\n\nax2.set_xlabel('Number of Evaluations', fontsize=12, fontweight='bold')\nax2.set_ylabel('Best CV Score Found', fontsize=12, fontweight='bold')\nax2.set_title('Optimization Progress: Best Score Over Time', fontsize=14, fontweight='bold')\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"âœ“ Visualization complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Practical Implementation\n\n### Load and Prepare Data\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"Dataset: {data.filename}\")\nprint(f\"Number of samples: {X.shape[0]}\")\nprint(f\"Number of features: {X.shape[1]}\")\nprint(f\"Classes: {data.target_names}\")\nprint(f\"Training set size: {X_train.shape[0]}\")\nprint(f\"Test set size: {X_test.shape[0]}\")\nprint(f\"\\nClass distribution in training set:\")\nprint(pd.Series(y_train).value_counts())",
   "id": "cell-examples"
  },
  {
   "cell_type": "code",
   "source": "# Define search space for Bayesian Optimization\nsearch_space = {\n    'C': Real(0.1, 100, prior='log-uniform'),\n    'gamma': Real(0.001, 1, prior='log-uniform'),\n    'kernel': Categorical(['rbf', 'linear'])\n}\n\n# Use fewer iterations - Bayesian Optimization is more efficient\nn_iter_bayes = 30  # Much less than Grid/Random Search\n\nprint(f\"Number of Bayesian optimization iterations: {n_iter_bayes}\")\nprint(f\"Search space: {search_space}\\n\")\n\n# Perform Bayesian Optimization\nprint(\"Starting Bayesian Optimization...\")\nstart_time = time.time()\n\nbayes_search = BayesSearchCV(\n    estimator=SVC(random_state=42),\n    search_spaces=search_space,\n    n_iter=n_iter_bayes,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1,\n    random_state=42\n)\n\nbayes_search.fit(X_train_scaled, y_train)\n\nbayes_time = time.time() - start_time\n\n# Results\nprint(f\"\\nâœ“ Bayesian Optimization completed in {bayes_time:.2f} seconds\")\nprint(f\"Best parameters: {bayes_search.best_params_}\")\nprint(f\"Best cross-validation accuracy: {bayes_search.best_score_:.4f}\")\n\n# Evaluate on test set\ny_pred_bayes = bayes_search.predict(X_test_scaled)\nbayes_test_acc = accuracy_score(y_test, y_pred_bayes)\nprint(f\"Test set accuracy: {bayes_test_acc:.4f}\")\n\n# Add to results\nresults['Bayesian Optimization'] = {\n    'best_params': bayes_search.best_params_,\n    'best_cv_score': bayes_search.best_score_,\n    'test_accuracy': bayes_test_acc,\n    'time': bayes_time,\n    'n_evaluations': n_iter_bayes\n}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Method 3: Bayesian Optimization\n\nFinally, let's apply Bayesian Optimization. This method should find good hyperparameters with significantly fewer evaluations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Define parameter distributions for Random Search\nparam_distributions = {\n    'C': uniform(0.1, 100),  # Continuous uniform distribution\n    'gamma': uniform(0.001, 1),  # Continuous uniform distribution\n    'kernel': ['rbf', 'linear']  # Categorical\n}\n\n# Use the same number of iterations as Grid Search for fair comparison\nn_iter = total_combinations\n\nprint(f\"Number of random samples: {n_iter}\")\nprint(f\"Parameter distributions: {param_distributions}\\n\")\n\n# Perform Random Search\nprint(\"Starting Random Search...\")\nstart_time = time.time()\n\nrandom_search = RandomizedSearchCV(\n    estimator=SVC(random_state=42),\n    param_distributions=param_distributions,\n    n_iter=n_iter,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1,\n    random_state=42\n)\n\nrandom_search.fit(X_train_scaled, y_train)\n\nrandom_time = time.time() - start_time\n\n# Results\nprint(f\"\\nâœ“ Random Search completed in {random_time:.2f} seconds\")\nprint(f\"Best parameters: {random_search.best_params_}\")\nprint(f\"Best cross-validation accuracy: {random_search.best_score_:.4f}\")\n\n# Evaluate on test set\ny_pred_random = random_search.predict(X_test_scaled)\nrandom_test_acc = accuracy_score(y_test, y_pred_random)\nprint(f\"Test set accuracy: {random_test_acc:.4f}\")\n\n# Add to results\nresults['Random Search'] = {\n    'best_params': random_search.best_params_,\n    'best_cv_score': random_search.best_score_,\n    'test_accuracy': random_test_acc,\n    'time': random_time,\n    'n_evaluations': n_iter\n}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Method 2: Random Search\n\nNow let's try Random Search with the same hyperparameter space, using the same number of iterations as Grid Search.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Define the parameter grid\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': [0.001, 0.01, 0.1, 1],\n    'kernel': ['rbf', 'linear']\n}\n\n# Calculate total number of combinations\ntotal_combinations = np.prod([len(v) for v in param_grid.values()])\nprint(f\"Total combinations to evaluate: {total_combinations}\")\nprint(f\"Parameter grid: {param_grid}\\n\")\n\n# Perform Grid Search\nprint(\"Starting Grid Search...\")\nimport time\nstart_time = time.time()\n\ngrid_search = GridSearchCV(\n    estimator=SVC(random_state=42),\n    param_grid=param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train_scaled, y_train)\n\ngrid_time = time.time() - start_time\n\n# Results\nprint(f\"\\nâœ“ Grid Search completed in {grid_time:.2f} seconds\")\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n\n# Evaluate on test set\ny_pred_grid = grid_search.predict(X_test_scaled)\ngrid_test_acc = accuracy_score(y_test, y_pred_grid)\nprint(f\"Test set accuracy: {grid_test_acc:.4f}\")\n\n# Store results for comparison\nresults = {\n    'Grid Search': {\n        'best_params': grid_search.best_params_,\n        'best_cv_score': grid_search.best_score_,\n        'test_accuracy': grid_test_acc,\n        'time': grid_time,\n        'n_evaluations': total_combinations\n    }\n}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Method 1: Grid Search\n\nLet's start with Grid Search to find optimal hyperparameters for a Support Vector Machine (SVM) classifier.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Hands-On Activity\n\n### Challenge: Optimize a Random Forest Classifier\n\nNow it's your turn! You'll apply what you've learned to optimize a Random Forest classifier on a different dataset. This activity will help solidify your understanding of hyperparameter optimization techniques.\n\n**Task:** Use all three optimization methods (Grid Search, Random Search, and Bayesian Optimization) to tune a Random Forest classifier on a classification dataset.\n\n**Hyperparameters to tune:**\n- `n_estimators`: Number of trees in the forest (range: 50-500)\n- `max_depth`: Maximum depth of the tree (range: 5-50)\n- `min_samples_split`: Minimum number of samples required to split a node (range: 2-20)\n- `min_samples_leaf`: Minimum number of samples required at each leaf node (range: 1-10)\n- `max_features`: Number of features to consider for the best split (options: 'sqrt', 'log2', or fraction)\n\n**Steps:**\n1. Load a new dataset (we'll use a synthetic classification problem)\n2. Define the hyperparameter search space for each method\n3. Run Grid Search (with a reduced grid for computational efficiency)\n4. Run Random Search\n5. Run Bayesian Optimization\n6. Compare the results\n\n**Expected outcome:** You should observe that Bayesian Optimization finds competitive or better hyperparameters with fewer evaluations than Grid and Random Search.",
   "id": "cell-activity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Create a synthetic classification dataset\nprint(\"Creating synthetic dataset...\")\nX_activity, y_activity = make_classification(\n    n_samples=2000,\n    n_features=20,\n    n_informative=15,\n    n_redundant=5,\n    n_classes=3,\n    random_state=42\n)\n\nX_train_act, X_test_act, y_train_act, y_test_act = train_test_split(\n    X_activity, y_activity, test_size=0.2, random_state=42, stratify=y_activity\n)\n\nprint(f\"Training samples: {X_train_act.shape[0]}\")\nprint(f\"Test samples: {X_test_act.shape[0]}\")\nprint(f\"Features: {X_train_act.shape[1]}\")\nprint(f\"Classes: {len(np.unique(y_activity))}\\n\")\n\n# Step 2 & 3: Grid Search with Random Forest\nprint(\"=\" * 80)\nprint(\"GRID SEARCH - Random Forest\")\nprint(\"=\" * 80)\n\nparam_grid_rf = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['sqrt', 'log2']\n}\n\nprint(f\"Total combinations: {np.prod([len(v) for v in param_grid_rf.values()])}\")\n\nstart_time = time.time()\ngrid_rf = GridSearchCV(\n    estimator=RandomForestClassifier(random_state=42),\n    param_grid=param_grid_rf,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=0\n)\ngrid_rf.fit(X_train_act, y_train_act)\ngrid_rf_time = time.time() - start_time\n\nprint(f\"Best parameters: {grid_rf.best_params_}\")\nprint(f\"Best CV score: {grid_rf.best_score_:.4f}\")\nprint(f\"Test accuracy: {accuracy_score(y_test_act, grid_rf.predict(X_test_act)):.4f}\")\nprint(f\"Time: {grid_rf_time:.2f}s\\n\")\n\n# Step 4: Random Search\nprint(\"=\" * 80)\nprint(\"RANDOM SEARCH - Random Forest\")\nprint(\"=\" * 80)\n\nparam_dist_rf = {\n    'n_estimators': randint(50, 500),\n    'max_depth': randint(5, 50),\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 10),\n    'max_features': ['sqrt', 'log2', 0.5, 0.7]\n}\n\nn_iter_rf = 50\n\nstart_time = time.time()\nrandom_rf = RandomizedSearchCV(\n    estimator=RandomForestClassifier(random_state=42),\n    param_distributions=param_dist_rf,\n    n_iter=n_iter_rf,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=0,\n    random_state=42\n)\nrandom_rf.fit(X_train_act, y_train_act)\nrandom_rf_time = time.time() - start_time\n\nprint(f\"Best parameters: {random_rf.best_params_}\")\nprint(f\"Best CV score: {random_rf.best_score_:.4f}\")\nprint(f\"Test accuracy: {accuracy_score(y_test_act, random_rf.predict(X_test_act)):.4f}\")\nprint(f\"Time: {random_rf_time:.2f}s\\n\")\n\n# Step 5: Bayesian Optimization\nprint(\"=\" * 80)\nprint(\"BAYESIAN OPTIMIZATION - Random Forest\")\nprint(\"=\" * 80)\n\nsearch_space_rf = {\n    'n_estimators': Integer(50, 500),\n    'max_depth': Integer(5, 50),\n    'min_samples_split': Integer(2, 20),\n    'min_samples_leaf': Integer(1, 10),\n    'max_features': Real(0.3, 1.0)\n}\n\nn_iter_bayes_rf = 30\n\nstart_time = time.time()\nbayes_rf = BayesSearchCV(\n    estimator=RandomForestClassifier(random_state=42),\n    search_spaces=search_space_rf,\n    n_iter=n_iter_bayes_rf,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=0,\n    random_state=42\n)\nbayes_rf.fit(X_train_act, y_train_act)\nbayes_rf_time = time.time() - start_time\n\nprint(f\"Best parameters: {bayes_rf.best_params_}\")\nprint(f\"Best CV score: {bayes_rf.best_score_:.4f}\")\nprint(f\"Test accuracy: {accuracy_score(y_test_act, bayes_rf.predict(X_test_act)):.4f}\")\nprint(f\"Time: {bayes_rf_time:.2f}s\\n\")\n\n# Step 6: Compare Results\nprint(\"=\" * 80)\nprint(\"COMPARISON SUMMARY\")\nprint(\"=\" * 80)\n\nactivity_results = pd.DataFrame({\n    'Method': ['Grid Search', 'Random Search', 'Bayesian Optimization'],\n    'CV Score': [grid_rf.best_score_, random_rf.best_score_, bayes_rf.best_score_],\n    'Test Accuracy': [\n        accuracy_score(y_test_act, grid_rf.predict(X_test_act)),\n        accuracy_score(y_test_act, random_rf.predict(X_test_act)),\n        accuracy_score(y_test_act, bayes_rf.predict(X_test_act))\n    ],\n    'Time (s)': [grid_rf_time, random_rf_time, bayes_rf_time],\n    'N Evaluations': [\n        np.prod([len(v) for v in param_grid_rf.values()]),\n        n_iter_rf,\n        n_iter_bayes_rf\n    ]\n})\n\nprint(activity_results.to_string(index=False))\nprint(\"=\" * 80)\n\n# Visualization\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# CV Score comparison\naxes[0].bar(activity_results['Method'], activity_results['CV Score'], \n            color=['#3498db', '#e74c3c', '#2ecc71'], alpha=0.7, edgecolor='black')\naxes[0].set_ylabel('CV Score', fontweight='bold')\naxes[0].set_title('Cross-Validation Score', fontweight='bold')\naxes[0].grid(axis='y', alpha=0.3)\naxes[0].tick_params(axis='x', rotation=15)\n\n# Time comparison\naxes[1].bar(activity_results['Method'], activity_results['Time (s)'], \n            color=['#3498db', '#e74c3c', '#2ecc71'], alpha=0.7, edgecolor='black')\naxes[1].set_ylabel('Time (seconds)', fontweight='bold')\naxes[1].set_title('Computation Time', fontweight='bold')\naxes[1].grid(axis='y', alpha=0.3)\naxes[1].tick_params(axis='x', rotation=15)\n\n# Evaluations comparison\naxes[2].bar(activity_results['Method'], activity_results['N Evaluations'], \n            color=['#3498db', '#e74c3c', '#2ecc71'], alpha=0.7, edgecolor='black')\naxes[2].set_ylabel('Number of Evaluations', fontweight='bold')\naxes[2].set_title('Evaluation Count', fontweight='bold')\naxes[2].grid(axis='y', alpha=0.3)\naxes[2].tick_params(axis='x', rotation=15)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nâœ… Activity Complete!\")\nprint(f\"Best performing method: {activity_results.loc[activity_results['Test Accuracy'].idxmax(), 'Method']}\")\nprint(f\"Most efficient method: Bayesian Optimization (achieved {bayes_rf.best_score_:.4f} with only {n_iter_bayes_rf} evaluations)\")",
   "id": "cell-activity-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n### Core Concepts\n\n1. **Grid Search is exhaustive but expensive**\n   - Evaluates all possible combinations in a predefined grid\n   - Guarantees finding the best configuration within the grid\n   - Computational cost grows exponentially with dimensionality: $O(n^d)$\n   - Best for: Small search spaces where exhaustive search is feasible\n\n2. **Random Search is simple and effective**\n   - Samples hyperparameter configurations randomly\n   - More efficient than Grid Search in high-dimensional spaces\n   - Better at exploring diverse values for important hyperparameters\n   - Best for: Quick exploration or when computational budget is fixed\n\n3. **Bayesian Optimization is sample-efficient**\n   - Uses a probabilistic model (Gaussian Process) to guide the search\n   - Balances exploration (uncertain regions) and exploitation (promising regions)\n   - Achieves competitive performance with significantly fewer evaluations\n   - Best for: Expensive evaluations or limited computational budgets\n\n### Practical Guidelines\n\n**When to use Grid Search:**\n- Search space has â‰¤3 dimensions\n- Each dimension has â‰¤5 values\n- You need to guarantee finding the optimal configuration within the grid\n- Computational resources are abundant\n\n**When to use Random Search:**\n- High-dimensional search spaces (>5 dimensions)\n- Quick baseline or initial exploration\n- Limited knowledge about important hyperparameters\n- Need for simple, reproducible experiments\n\n**When to use Bayesian Optimization:**\n- Each model evaluation is expensive (>1 minute)\n- Limited computational budget\n- Need for sample efficiency\n- Search space has continuous hyperparameters\n- â‰¤20 dimensions (GP struggles beyond this)\n\n### Advanced Considerations\n\n- **Parallelization**: Grid and Random Search parallelize perfectly; Bayesian Optimization is inherently sequential but modern variants support parallel evaluations\n- **Prior knowledge**: If you have domain knowledge about promising regions, Bayesian Optimization can incorporate this via prior distributions\n- **Multi-fidelity optimization**: Techniques like Hyperband and BOHB combine Bayesian Optimization with early stopping for even greater efficiency\n- **Conditional hyperparameters**: Some hyperparameters only matter for certain configurations (e.g., kernel parameters for RBF but not linear kernels)\n\n### Performance Summary from Our Experiments\n\n- Bayesian Optimization achieved competitive accuracy with **~60% fewer evaluations** than Grid Search\n- All three methods found similar optimal configurations, validating their effectiveness\n- Computation time scales with number of evaluations, making Bayesian Optimization the most time-efficient\n- The best hyperparameters generalized well to the test set, confirming proper cross-validation\n\n### Next Steps in Your Learning Journey\n\nAfter mastering these fundamental techniques, explore:\n- **Hyperband & Successive Halving** (Day 72): Early stopping strategies for efficient hyperparameter search\n- **Neural Architecture Search** (Day 73): Automated design of neural network architectures\n- **AutoML Frameworks** (Day 74): Production-ready tools like Auto-sklearn and TPOT\n- **Meta-learning** (Day 75): Learning to learn across multiple tasks",
   "id": "cell-takeaways"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Further Resources\n\n### Academic Papers\n\n1. **[Random Search for Hyper-Parameter Optimization](http://jmlr.org/papers/v13/bergstra12a.html)**  \n   Bergstra & Bengio (2012) - The foundational paper showing Random Search outperforms Grid Search\n\n2. **[Practical Bayesian Optimization of Machine Learning Algorithms](https://arxiv.org/abs/1206.2944)**  \n   Snoek, Larochelle & Adams (2012) - Seminal work on applying Bayesian Optimization to ML\n\n3. **[Taking the Human Out of the Loop: A Review of Bayesian Optimization](https://ieeexplore.ieee.org/document/7352306)**  \n   Shahriari et al. (2016) - Comprehensive review of Bayesian Optimization techniques\n\n4. **[Algorithms for Hyper-Parameter Optimization](https://papers.nips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)**  \n   Bergstra et al. (2011) - Tree-structured Parzen Estimator (TPE) method\n\n### Software Libraries\n\n1. **[scikit-optimize (skopt)](https://scikit-optimize.github.io/stable/)**  \n   Python library for sequential model-based optimization with great sklearn integration\n\n2. **[Optuna](https://optuna.org/)**  \n   Modern hyperparameter optimization framework with pruning, visualization, and distributed support\n\n3. **[Ray Tune](https://docs.ray.io/en/latest/tune/index.html)**  \n   Scalable hyperparameter tuning with support for distributed training and early stopping\n\n4. **[Hyperopt](http://hyperopt.github.io/hyperopt/)**  \n   Distributed Asynchronous Hyperparameter Optimization using Tree-structured Parzen Estimators\n\n5. **[Ax Platform](https://ax.dev/)**  \n   Facebook's platform for adaptive experimentation including Bayesian Optimization\n\n### Tutorials and Documentation\n\n1. **[scikit-learn Hyperparameter Tuning Guide](https://scikit-learn.org/stable/modules/grid_search.html)**  \n   Official documentation on GridSearchCV, RandomizedSearchCV, and best practices\n\n2. **[Bayesian Optimization Tutorial by Martin Krasser](https://krasserm.github.io/2018/03/21/bayesian-optimization/)**  \n   Excellent visual explanation with code examples\n\n3. **[Distill.pub: A Visual Exploration of Gaussian Processes](https://distill.pub/2019/visual-exploration-gaussian-processes/)**  \n   Interactive visualization of GPs, the foundation of Bayesian Optimization\n\n4. **[Optuna Tutorial - Distributed Hyperparameter Optimization](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html)**  \n   Guide to scaling hyperparameter optimization across multiple machines\n\n### Books\n\n1. **\"Gaussian Processes for Machine Learning\"** by Rasmussen & Williams  \n   Comprehensive treatment of GPs, available free at [gaussianprocess.org/gpml](http://www.gaussianprocess.org/gpml/)\n\n2. **\"AutoML: Methods, Systems, Challenges\"** by Hutter, Kotthoff & Vanschoren  \n   Covers automated machine learning including hyperparameter optimization\n\n### Online Courses\n\n1. **[Stanford CS229: Machine Learning](http://cs229.stanford.edu/)** - Lectures on hyperparameter tuning\n2. **[Fast.ai Practical Deep Learning](https://course.fast.ai/)** - Practical hyperparameter tuning strategies\n\n### Community Resources\n\n- **[Papers with Code - AutoML Benchmark](https://paperswithcode.com/task/automl)** - Latest research and benchmarks\n- **[r/MachineLearning](https://www.reddit.com/r/MachineLearning/)** - Community discussions\n- **[Kaggle Learn](https://www.kaggle.com/learn/intro-to-machine-learning)** - Practical tutorials with real datasets\n\n---\n\n**Congratulations on completing Day 71!** ðŸŽ‰  \nYou now have a solid understanding of hyperparameter optimization techniques. Practice these methods on your own datasets to internalize the concepts and develop intuition for which method works best in different scenarios.",
   "id": "cell-resources"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
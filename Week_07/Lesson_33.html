

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Day 33: Introduction to AdaBoost &#8212; 100 Days of Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Week_07/Lesson_33';</script>
    <link rel="canonical" href="https://100daysofml.com/Week_07/Lesson_33.html" />
    <link rel="shortcut icon" href="../_static/100days.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Day 34: Introduction to Gradient Boosting Machines (GBM) and Extreme Gradient Boosting (XGBoost)" href="Lesson_34.html" />
    <link rel="prev" title="Day 32: Introduction to Bagging and Random Forests" href="Lesson_32.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/100days_circle.jpg" class="logo__image only-light" alt="100 Days of Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/100days_circle.jpg" class="logo__image only-dark" alt="100 Days of Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    100 Days of Machine Learning Challenge
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Preface</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_00/00_Overview.html">Welcome: Overview</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Week_00/00a_DailyChallenge.html">Daily Challenge Curriculum</a></li>

<li class="toctree-l2"><a class="reference internal" href="../Week_00/00b_DailyResources.html"><strong>Daily Curriculum Resources</strong></a></li>






















<li class="toctree-l2"><a class="reference internal" href="../Week_00/01_Errata.html">Errata: Corrections History</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 1 - Introduction to Python Basics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_01/001_Overview.html">Week_01: Overview</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Week_01/Lesson_01.html">Day 1 - Python Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_01/Lesson_02.html">Day 2 - Python Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_01/Lesson_03.html">Day 3 - Control Structures in Python: Loops</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_01/Lesson_04.html">Day 4 - Control Structures in Python: Conditional Statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_01/Lesson_05.html">Day 5 - Functions and Modules</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 2 - Introduction to Machine Learning Mathematics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_02/002_Overview.html">Week_02: Overview</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Week_02/Lesson_06.html">Day 6 - Linear Algebra - Vectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_02/Lesson_07.html">Day 7 - Linear Algebra - Matrices and Matrix Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_02/Lesson_08.html">Day 8 - Calculus - Derivatives, Concept and Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_02/Lesson_09.html">Day 9 - Calculus - Integrals, Fundamental Theorems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_02/Lesson_10.html">Day 10 - Statistics and Probability - Concepts and Relevant Distributions</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 3 - Data Preprocessing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_03/003_Overview.html">Week_03: Overview</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Week_03/Lesson_11.html">Day 11 - Introduction to Data Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_03/Lesson_12.html">Day 12: In-Depth Exploration of Data Splitting Techniques in Python with Cross-Validation</a></li>

<li class="toctree-l2"><a class="reference internal" href="../Week_03/Lesson_12solution.html">Day 12: In-Depth Exploration of Data Splitting Techniques - Solution</a></li>

<li class="toctree-l2"><a class="reference internal" href="../Week_03/Lesson_13.html">Day 13 - Handling Missing Data in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_03/Lesson_14.html">Day 14 - Data Normalization and Scaling using Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_03/Lesson_15.html">Day 15: Encoding Categorical Data in Python - Expanded with Mathematical Implications</a></li>

</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 4 - Data Preprocessing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_04/004_Overview.html">Week_04: Overview</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Week_04/Lesson_16.html">Day 16 - Introduction to EDA and Data Visualization in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_04/Lesson_17.html">Day 17 - Implementing Descriptive Statistics for EDA in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_04/Lesson_18.html">Day 18 - Visualization Techniques for Data Distribution in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_04/Lesson_19.html">Day 19: Correlation Analysis using Python</a></li>

<li class="toctree-l2"><a class="reference internal" href="../Week_04/Lesson_20.html">Day 20: Advanced Feature Selection and Importance in Python - With Iris Dataset</a></li>


</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 5: Supervised Learning - Regression</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_05/005_Overview.html">Week_05: Overview</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Week_05/Lesson_21.html">Day 21 - Introduction to Regression Analysis in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_05/Lesson_22.html">Day 22: Implementing Multiple Linear Regression in Python</a></li>

<li class="toctree-l2"><a class="reference internal" href="../Week_05/Lesson_23.html">Day 23 - Advanced Regression Techniques - Polynomial, Lasso, and Ridge Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_05/Lesson_24.html">Day 24 - Regression Model Evaluation Metrics in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Week_05/Lesson_25.html">Day 25 - Addressing Overfitting and Underfitting in Regression Models</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 6: Supervised Learning - Classification</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_06/006_Overview.html">Week_06: Overview</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Week_06/Lesson_26.html">Day 26: Introduction to Classification and Logistic Regression in Python</a></li>


<li class="toctree-l2"><a class="reference internal" href="../Week_06/Lesson_27.html">Day 27: Introduction to the K-Nearest Neighbors (K-NN) Algorithm</a></li>



<li class="toctree-l2"><a class="reference internal" href="../Week_06/Lesson_28.html">Day 28: Introduction to Support Vector Machines (SVM)</a></li>



<li class="toctree-l2"><a class="reference internal" href="../Week_06/Lesson_29.html">Day 29: Introduction to Decision Trees</a></li>


<li class="toctree-l2"><a class="reference internal" href="../Week_06/Lesson_30.html">Introduction to Naive Bayes Classifier</a></li>


</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 7: Ensemble Methods</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="007_Overview.html">Week_07: Overview</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Lesson_31.html">Day 31: Introduction to Ensemble Learning Techniques</a></li>


<li class="toctree-l2"><a class="reference internal" href="Lesson_32.html">Day 32: Introduction to Bagging and Random Forests</a></li>



<li class="toctree-l2 current active"><a class="current reference internal" href="#">Day 33: Introduction to AdaBoost</a></li>



<li class="toctree-l2"><a class="reference internal" href="Lesson_34.html">Day 34: Introduction to Gradient Boosting Machines (GBM) and Extreme Gradient Boosting (XGBoost)</a></li>



<li class="toctree-l2"><a class="reference internal" href="Lesson_35.html">Day 35: Advanced Ensemble Techniques: Stacking and Blending</a></li>


</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://notebooks.gesis.org/binder/jupyter/user/100daysofml-100-sofml.github.io-4iw5ztbi/lab/workspaces/auto-e/v2/gh/100daysofml/100daysofml.github.io/master?urlpath=tree/Week_07/Lesson_33.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/100daysofml/100daysofml.github.io/github/100daysofml/100daysofml.github.io/blob/master/Week_07/Lesson_33.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/100daysofml/100daysofml.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/100daysofml/100daysofml.github.io/edit/master/Week_07/Lesson_33.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Week_07/Lesson_33.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Day 33: Introduction to AdaBoost</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Day 33: Introduction to AdaBoost</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importance">Importance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-examples">Applications and Examples</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-weak-learners-in-adaboost">Understanding Weak Learners in AdaBoost</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#role-in-adaboost">Role in AdaBoost</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selection-and-enhancement-of-weak-learners">Selection and Enhancement of Weak Learners</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-and-understanding">Interpretation and Understanding</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#the-weight-update-rule-in-adaboost">The Weight Update Rule in AdaBoost</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-importance">Applications and Importance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost-with-sklearn">Adaboost with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-the-final-output-in-adaboost">Calculating the Final Output in AdaBoost</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-for-the-reader">Exercise For The Reader</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#task">Task</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="day-33-introduction-to-adaboost">
<h1>Day 33: Introduction to AdaBoost<a class="headerlink" href="#day-33-introduction-to-adaboost" title="Permalink to this heading">#</a></h1>
<p>AdaBoost, short for Adaptive Boosting, is a compelling technique in the field of machine learning that focuses on converting a collection of weak learning models into a strong predictive force. This introduction provides an essential foundation for understanding AdaBoost, unpacking its core mechanisms, mathematical formulations, and practical significance in enhancing machine learning model accuracies. We delve into its unique approach of leveraging “weak learners,” the strategic weight update mechanism, and the integration of learners for final decision making. Through this exploration, learners will grasp not only the theoretical underpinnings of AdaBoost but also its application in solving complex classification problems.</p>
<section id="definition">
<h2>Definition<a class="headerlink" href="#definition" title="Permalink to this heading">#</a></h2>
<p>AdaBoost is a boosting algorithm that combines multiple weak classifiers into a single strong classifier. Here, a weak classifier refers to a model that performs slightly better than random guessing. The strength of AdaBoost lies in its iterative approach to focusing on incorrectly classified instances by adjusting their weights. This process ensures that subsequent learners give more attention to challenging cases. The elementary formula for updating the weights of the instances is given as follows:</p>
<ul class="simple">
<li><p>Weight update formula: <span class="math notranslate nohighlight">\(w_i \leftarrow w_i \times \exp(\alpha_t \times I(y_i \ne f_t(x_i)))\)</span></p></li>
</ul>
<p>where <span class="math notranslate nohighlight">\(w_i\)</span> is the weight of the <span class="math notranslate nohighlight">\(i^{th}\)</span> instance, <span class="math notranslate nohighlight">\(\alpha_t\)</span> is the weight of the <span class="math notranslate nohighlight">\(t^{th}\)</span> classifier, dependent on its error rate, <span class="math notranslate nohighlight">\(I\)</span> is an indicator function that returns <span class="math notranslate nohighlight">\(1\)</span> if the condition inside is true (the instance is misclassified) and <span class="math notranslate nohighlight">\(0\)</span> otherwise, <span class="math notranslate nohighlight">\(y_i\)</span> is the actual label, and <span class="math notranslate nohighlight">\(f_t(x_i)\)</span> is the prediction of the <span class="math notranslate nohighlight">\(t^{th}\)</span> weak classifier.</p>
<ul class="simple">
<li><p>Final output calculation involves summing up the weighted predictions of all the weak learners to make the final decision.</p></li>
</ul>
</section>
<section id="importance">
<h2>Importance<a class="headerlink" href="#importance" title="Permalink to this heading">#</a></h2>
<p>Understanding and implementing AdaBoost is crucial for several reasons. Firstly, it showcases the power of ensemble learning, where multiple learning algorithms are combined to improve predictive performance. AdaBoost’s ability to focus on difficult instances makes it exceptionally skilled at enhancing weak models that perform just above chance. Its applications span various domains from image and speech recognition to biological classifications, demonstrating its versatility and effectiveness across different fields.</p>
</section>
<section id="applications-and-examples">
<h2>Applications and Examples<a class="headerlink" href="#applications-and-examples" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Image Recognition</strong>: AdaBoost can improve the accuracy of detection algorithms, identifying faces or objects within images with higher precision by focusing on hard-to-classify instances.</p></li>
<li><p><strong>Customer Churn Prediction</strong>: Businesses use AdaBoost to predict which customers are likely to leave their service for competitors. By accurately identifying these customers, companies can implement targeted retention strategies.</p></li>
<li><p><strong>Fraud Detection in Banking</strong>: AdaBoost helps in enhancing the performance of models designed to detect fraudulent transactions, adapting to the ever-evolving tactics of fraudsters by emphasizing transactions that are harder to classify.</p></li>
</ul>
<p>In the following sections, we will delve deeper into each of these aspects, understanding the critical role of weak learners, the intricacies of the weight update rule, and the calculation for the final model output. This foundational knowledge will pave the way for both theoretical understanding and practical implementation exercises, including developing an AdaBoost classifier from scratch in Python.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="understanding-weak-learners-in-adaboost">
<h1>Understanding Weak Learners in AdaBoost<a class="headerlink" href="#understanding-weak-learners-in-adaboost" title="Permalink to this heading">#</a></h1>
<p>A weak learner in the framework of AdaBoost is any classifier that can generate predictions with an accuracy slightly better than chance. This means that for a binary classification problem, a weak learner would have an accuracy just above 50%. The power of AdaBoost is its ability to take these simple models and, through a strategic process of iteration and weight adjustments, amplify their strengths to achieve superior classification performance.</p>
<section id="role-in-adaboost">
<h2>Role in AdaBoost<a class="headerlink" href="#role-in-adaboost" title="Permalink to this heading">#</a></h2>
<p>Weak learners are the foundation upon which AdaBoost builds a strong classifier. The algorithm begins with a data set and assigns equal weights to all observations. It then iteratively:</p>
<ol class="arabic simple">
<li><p>Applies a weak learner to the weighted data.</p></li>
<li><p>Increases the weights of misclassified observations, thereby making them a priority for the next classifier.</p></li>
<li><p>Adds the trained learner to the ensemble of classifiers, with its say in the final classification proportional to its accuracy.</p></li>
</ol>
<p>This process ensures that each successive learner focuses more on the examples that previous learners found challenging, gradually creating a robust classification model.</p>
</section>
<section id="selection-and-enhancement-of-weak-learners">
<h2>Selection and Enhancement of Weak Learners<a class="headerlink" href="#selection-and-enhancement-of-weak-learners" title="Permalink to this heading">#</a></h2>
<p>The key to the success of AdaBoost lies in the deliberate selection and enhancement of weak learners throughout its iterative process. At each iteration, the algorithm selects a new weak learner with a preference for those that perform well on the instances that the current ensemble finds difficult. This is achieved by adjusting the instance weights: instances misclassified by the current ensemble receive increased weights, while correctly classified instances have their weights decreased.</p>
<p>The ability of AdaBoost to enhance its weak learners is quantified by the error rate of each learner on the weighted dataset. Lower error rates result in higher weights being assigned to the learner’s vote in the final ensemble. Conversely, learners with error rates close to or above 0.5 contributing less to the final model, emphasizing AdaBoost’s adaptivity in focusing on the most effective classifiers for the given problem.</p>
</section>
<section id="interpretation-and-understanding">
<h2>Interpretation and Understanding<a class="headerlink" href="#interpretation-and-understanding" title="Permalink to this heading">#</a></h2>
<p>The core philosophy behind AdaBoost’s use of weak learners is that a combination of simple, focused corrections can cumulatively solve a complex problem more effectively than a single, highly complex model. This approach not only improves prediction accuracy but also offers insights into which aspects of the data are challenging to classify, guiding further investigation and model improvement.</p>
<p>By understanding the role and optimization of weak learners within AdaBoost, practitioners can more effectively apply this algorithm in various fields, from natural language processing to customer behavior prediction, harnessing the power of ensemble learning to achieve exceptional model performance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Data for the bar chart</span>
<span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Random Guessing&#39;</span><span class="p">,</span> <span class="s1">&#39;Single Weak Learner&#39;</span><span class="p">,</span> <span class="s1">&#39;Aggregated Weak Learners&#39;</span><span class="p">]</span>
<span class="n">performance</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">52</span><span class="p">,</span> <span class="mi">90</span><span class="p">]</span>  <span class="c1"># Assuming a slight improvement with a single weak learner and a significant improvement when aggregating</span>

<span class="c1"># Creating the bar chart</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">bars</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">categories</span><span class="p">,</span> <span class="n">performance</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">])</span>

<span class="c1"># Adding specific details to make the chart more informative</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy (%)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Performance Comparison&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># Adjusting y-axis to show percentages clearly</span>

<span class="c1"># Adding the percentage values on top of each bar for clarity</span>
<span class="k">for</span> <span class="n">bar</span> <span class="ow">in</span> <span class="n">bars</span><span class="p">:</span>
    <span class="n">height</span> <span class="o">=</span> <span class="n">bar</span><span class="o">.</span><span class="n">get_height</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">height</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">,</span>
                <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">bar</span><span class="o">.</span><span class="n">get_x</span><span class="p">()</span> <span class="o">+</span> <span class="n">bar</span><span class="o">.</span><span class="n">get_width</span><span class="p">()</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">height</span><span class="p">),</span>
                <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>  <span class="c1"># 3 points vertical offset</span>
                <span class="n">textcoords</span><span class="o">=</span><span class="s2">&quot;offset points&quot;</span><span class="p">,</span>
                <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Interpretation:</span>
<span class="c1"># This visualization clearly demonstrates the effectiveness of AdaBoost&#39;s approach. By starting with random guessing </span>
<span class="c1"># (accuracy at 50%), incrementally improving with a single weak learner (slightly better than random guessing), and </span>
<span class="c1"># significantly enhancing performance through the aggregation of multiple weak learners, we see how ensemble methods like </span>
<span class="c1"># AdaBoost transform weak learners into a powerful composite classifier.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7d12acd52439c8584cf3b369c28419d4253c655af2269e27caa7d9734dbc73ec.png" src="../_images/7d12acd52439c8584cf3b369c28419d4253c655af2269e27caa7d9734dbc73ec.png" />
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="the-weight-update-rule-in-adaboost">
<h1>The Weight Update Rule in AdaBoost<a class="headerlink" href="#the-weight-update-rule-in-adaboost" title="Permalink to this heading">#</a></h1>
<p>At the heart of the AdaBoost algorithm is a simple yet powerful strategy for iteratively refining the model’s focus on those instances that previous learners have misclassified. This section delves into the weight update rule, a critical mechanism enabling AdaBoost to adjust its learning focus dynamically, thereby enhancing its overall predictive capability.</p>
<section id="id1">
<h2>Definition<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>AdaBoost updates the weights of the training instances through an exponential formula, which is designed to increase the weights of wrongly classified instances and decrease the weights or maintain the weights of correctly classified instances. This makes sure that subsequent classifiers pay more attention to the harder cases.</p>
<p><strong>Weight Update Rule</strong>:</p>
<div class="math notranslate nohighlight">
\[ w_i \leftarrow w_i \times \exp(\alpha_t \times I(y_i \ne f_t(x_i))) \]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(w_i\)</span>: Weight of the <span class="math notranslate nohighlight">\(i^{th}\)</span> instance before the update.</p></li>
<li><p><span class="math notranslate nohighlight">\(\exp\)</span>: Exponential function, ensuring that weight adjustments are non-linear.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha_t\)</span>: The performance weight of the <span class="math notranslate nohighlight">\(t^{th}\)</span> classifier, calculated as <span class="math notranslate nohighlight">\(\alpha_t = 0.5 \cdot \log(\frac{1 - err_t}{err_t})\)</span>, with <span class="math notranslate nohighlight">\(err_t\)</span> being the error rate of the classifier.</p></li>
<li><p><span class="math notranslate nohighlight">\(I(\cdot)\)</span>: Indicator function, returning <span class="math notranslate nohighlight">\(1\)</span> if the condition (<span class="math notranslate nohighlight">\(y_i \ne f_t(x_i)\)</span>) is true (misclassification) and <span class="math notranslate nohighlight">\(0\)</span> otherwise.</p></li>
<li><p><span class="math notranslate nohighlight">\(y_i\)</span>: Actual label of the <span class="math notranslate nohighlight">\(i^{th}\)</span> instance.</p></li>
<li><p><span class="math notranslate nohighlight">\(f_t(x_i)\)</span>: Prediction of the <span class="math notranslate nohighlight">\(t^{th}\)</span> classifier on the <span class="math notranslate nohighlight">\(i^{th}\)</span> instance.</p></li>
</ul>
<p>It is important to note that <span class="math notranslate nohighlight">\(\alpha_t\)</span> is directly tied to the performance of the classifier, meaning that better-performing classifiers will have a greater influence on the weight update process. This dynamic allows AdaBoost to allocate more “say” in the learning process to those classifiers that have proved to be more accurate.</p>
</section>
<section id="applications-and-importance">
<h2>Applications and Importance<a class="headerlink" href="#applications-and-importance" title="Permalink to this heading">#</a></h2>
<p>The weight update mechanism is foundational in AdaBoost’s success across various applications:</p>
<ul class="simple">
<li><p><strong>Enhancing Model Sensitivity</strong>: By progressively focusing more on hard-to-classify instances, AdaBoost becomes increasingly sensitive to subtle patterns that might be missed by a single model or weaker learners.</p></li>
<li><p><strong>Versatile Applicability</strong>: This dynamic tuning process, embodied by the weight update rule, is central to AdaBoost’s effectiveness in areas such as image recognition, customer churn prediction, and fraud detection.</p></li>
<li><p><strong>Error Correction</strong>: The iterative weight adjustment facilitates error correction in sequential models, allowing previous misclassifications to guide future learning in a productive manner.</p></li>
</ul>
<p>Understanding the weight update rule is pivotal in leveraging AdaBoost’s full potential and appreciating its strategic approach to ensemble learning, where it systematically capitalizes on the collective strengths of multiple learners.</p>
<section id="adaboost-with-sklearn">
<h3>Adaboost with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code><a class="headerlink" href="#adaboost-with-sklearn" title="Permalink to this heading">#</a></h3>
<p>We’ll utilize <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> for creating a basic model setup and <code class="docutils literal notranslate"><span class="pre">numpy</span></code> for managing instance weights and calculations related to the weight update rule.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Simulating data: 10 instances with initial equal weights</span>
<span class="n">num_instances</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># Simulating 5 iterations of AdaBoost</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">))</span> <span class="o">/</span> <span class="n">num_instances</span>

<span class="c1"># Simulated error rates and classifier performances (alpha_t) for 5 iterations</span>
<span class="n">error_rates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">])</span>
<span class="n">alpha_t_values</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">error_rates</span><span class="p">)</span> <span class="o">/</span> <span class="n">error_rates</span><span class="p">)</span>

<span class="c1"># Simulated misclassifications for five iterations</span>
<span class="n">misclassified</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="n">misclassified</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Misclassified instances in 1st iteration</span>
<span class="n">misclassified</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Misclassified instances in 2nd iteration</span>
<span class="n">misclassified</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Misclassified instances in 3rd iteration</span>
<span class="n">misclassified</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Misclassified instances in 4th iteration</span>
<span class="n">misclassified</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">True</span>     <span class="c1"># Misclassified instances in 5th iteration</span>

<span class="c1"># Updating weights based on misclassifications and alpha_t values</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">weights</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">alpha_t_values</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">misclassified</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
    <span class="n">weights</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">/=</span> <span class="n">weights</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>  <span class="c1"># Normalizing weights</span>

<span class="c1"># Visualizing how weights change over iterations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_instances</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">weights</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Instance </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Change in Instance Weights Across AdaBoost Iterations&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Weight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Interpretation:</span>
<span class="c1"># This visualization shows how the weights of individual training instances change over several iterations of AdaBoost.</span>
<span class="c1"># Instances that are consistently misclassified (e.g., Instance 7) receive progressively higher weights, emphasizing AdaBoost’s</span>
<span class="c1"># focus on hard-to-classify cases. This illustrates the adaptive nature of the weight update rule in enhancing model sensitivity.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/39687d76e1d869f2ae37e6fee0df7e95f119958508d9622ce05bc9f86465d12c.png" src="../_images/39687d76e1d869f2ae37e6fee0df7e95f119958508d9622ce05bc9f86465d12c.png" />
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="calculating-the-final-output-in-adaboost">
<h1>Calculating the Final Output in AdaBoost<a class="headerlink" href="#calculating-the-final-output-in-adaboost" title="Permalink to this heading">#</a></h1>
<p>After understanding the fundamentals of AdaBoost, it’s crucial to explore how it integrates the plethora of weak learners ‘votes’ to formulate a robust prediction. This segment unpacks the process and mathematics behind calculating the final output in AdaBoost, highlighting the ingenious approach of employing the weighted contributions of each learner.</p>
<section id="id2">
<h2>Definition<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h2>
<p>The culmination of the AdaBoost algorithm is the calculation of the final output, which is the weighted sum of the predictions made by all the weak classifiers. The idea is to give higher weight to the predictions of classifiers that have lower error rates and, consequently, are deemed more accurate. The final prediction is then made based on the sign of the sum of these weighted predictions. Mathematically, this can be represented as:</p>
<ul class="simple">
<li><p>Final output formula: <span class="math notranslate nohighlight">\(f(x) = \text{sign}\left(\sum_{t=1}^{T} \alpha_t \cdot f_t(x)\right)\)</span></p></li>
</ul>
<p>Here, <span class="math notranslate nohighlight">\(\alpha_t\)</span> represents the weight of the <span class="math notranslate nohighlight">\(t^{th}\)</span> classifier (indicative of its accuracy), and <span class="math notranslate nohighlight">\(f_t(x)\)</span> is the prediction made by the <span class="math notranslate nohighlight">\(t^{th}\)</span> classifier for a data point <span class="math notranslate nohighlight">\(x\)</span>. <span class="math notranslate nohighlight">\(T\)</span> is the total number of classifiers. The function <span class="math notranslate nohighlight">\(\text{sign}\)</span> returns <span class="math notranslate nohighlight">\(1\)</span> if the argument is positive and <span class="math notranslate nohighlight">\(-1\)</span> otherwise, representing the final classification outcome based on the combined weighted predictions.</p>
<p>The final output calculation in AdaBoost is a pivotal step that showcases the strength of collective intelligence. By attributing more significance to the more accurate classifiers, AdaBoost ensures that the ensemble’s final decision is more reliable than any individual weak learner’s prediction.</p>
</section>
<section id="exercise-for-the-reader">
<h2>Exercise For The Reader<a class="headerlink" href="#exercise-for-the-reader" title="Permalink to this heading">#</a></h2>
<p>In this exercise, you will have the opportunity to implement the AdaBoost algorithm from scratch using Python. You will work with a simple dataset to solve a binary classification problem, applying the core concepts and mechanisms of AdaBoost as introduced in the lesson. Your task involves four main steps:</p>
<ol class="arabic simple">
<li><p><strong>Initialize the weights</strong> for the training instances.</p></li>
<li><p><strong>Select and train weak learners</strong>, typically using decision stumps (a one-level decision tree) for this purpose.</p></li>
<li><p><strong>Update the weights</strong> of the instances based on the performance of the weak learners, focusing more on the instances that were incorrectly classified.</p></li>
<li><p><strong>Combine the weak learners</strong> into a final model using a weighted vote, where the vote of each weak learner is weighted by its accuracy.</p></li>
</ol>
<section id="id3">
<h3>Definition<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p>Recall from the lesson that AdaBoost iteratively adjusts the weights of incorrectly classified instances so that successive weak learners focus more on the difficult cases. The weight update formula and the process for combining weak learners you’ll need are as follows:</p>
<ol class="arabic simple">
<li><p><strong>Weight Update</strong>: For each instance <span class="math notranslate nohighlight">\(i\)</span>, update its weight <span class="math notranslate nohighlight">\(w_i\)</span> using the formula <span class="math notranslate nohighlight">\(w_i \leftarrow w_i \times \exp(\alpha_t \times I(y_i \ne f_t(x_i)))\)</span>, where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha_t\)</span> is the weight of the <span class="math notranslate nohighlight">\(t^{th}\)</span> classifier (weak learner), which is calculated based on its error rate.</p></li>
<li><p><span class="math notranslate nohighlight">\(I\)</span> is an indicator function that returns <span class="math notranslate nohighlight">\(1\)</span> if the instance is misclassified (<span class="math notranslate nohighlight">\(y_i \ne f_t(x_i)\)</span>) and <span class="math notranslate nohighlight">\(0\)</span> otherwise.</p></li>
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> is the actual label of the <span class="math notranslate nohighlight">\(i^{th}\)</span> instance.</p></li>
<li><p><span class="math notranslate nohighlight">\(f_t(x_i)\)</span> is the prediction by the <span class="math notranslate nohighlight">\(t^{th}\)</span> weak learner for the <span class="math notranslate nohighlight">\(i^{th}\)</span> instance.</p></li>
</ul>
</li>
<li><p><strong>Final Output Calculation</strong>: The ultimate prediction is made by summing the weighted predictions of all the weak learners and taking the sign of this sum as the final classification decision.</p></li>
</ol>
</section>
<section id="task">
<h3>Task<a class="headerlink" href="#task" title="Permalink to this heading">#</a></h3>
<p>Your task is to use Python to implement these concepts. Begin with a dataset of your choice (for simplicity, the Iris dataset or any binary classification dataset can be used). Follow these steps:</p>
<ol class="arabic simple">
<li><p><strong>Initialize</strong> each instance’s weight to <span class="math notranslate nohighlight">\(1/N\)</span> where <span class="math notranslate nohighlight">\(N\)</span> is the total number of instances in the dataset.</p></li>
<li><p><strong>Iteratively</strong>:</p>
<ul class="simple">
<li><p>Train a weak learner using the current weights of the instances.</p></li>
<li><p>Calculate the error rate of the weak learner.</p></li>
<li><p>Compute the weight (<span class="math notranslate nohighlight">\(\alpha\)</span>) of the weak learner using its error rate.</p></li>
<li><p>Update the weights of the instances using the provided formula.</p></li>
</ul>
</li>
<li><p><strong>Combine the weak learners</strong> based on their calculated weights (<span class="math notranslate nohighlight">\(\alpha\)</span> values) to make the final prediction.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Generate a simple binary classification dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">.5</span><span class="p">],</span> <span class="n">flip_y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Convert labels to -1 and 1 for our AdaBoost implementation</span>

<span class="c1"># Initialize weights for all instances</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Number of instances</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>  <span class="c1"># Initial weight for each instance</span>

<span class="c1"># Placeholder for the AdaBoost steps</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Number of iterations</span>
<span class="n">error_rates</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># To track the error rate of the model at each iteration</span>

<span class="c1"># Initialize variables to store the final model</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Store the alpha values of the weak learners</span>
<span class="n">learners</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Store the weak learners</span>


<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="c1"># Step 1: Train a weak learner.</span>
    <span class="c1"># Use a decision stump (depth=1) as the weak learner</span>
    <span class="n">weak_learner</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">weak_learner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    
    <span class="c1"># Step 2: Make predictions and calculate the error rate of the weak learner.</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">weak_learner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">miss</span> <span class="o">=</span> <span class="n">predictions</span> <span class="o">!=</span> <span class="n">y</span>  <span class="c1"># Boolean array indicating whether each prediction is incorrect</span>
    <span class="n">error_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">miss</span><span class="p">)</span> <span class="o">/</span> <span class="n">weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>  <span class="c1"># Weighted error rate</span>
    
    <span class="c1"># Store the error rate</span>
    <span class="n">error_rates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error_rate</span><span class="p">)</span>
    
    <span class="c1"># Step 3: Calculate the weak learner&#39;s weight (alpha) using its error rate</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">error_rate</span><span class="p">)</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">error_rate</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">))</span>
    
    <span class="c1"># Step 4: Update instance weights</span>
    <span class="c1"># Weights are increased for misclassified instances and decreased for correctly classified instances</span>
    <span class="n">weights</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">miss</span> <span class="o">*</span> <span class="p">((</span><span class="n">weights</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)))</span>
    
    <span class="c1"># Normally, you would also update the predictions of the ensemble model here and check for termination conditions.</span>
    <span class="c1"># For simplicity, we&#39;re focusing on the weight update mechanism and the model&#39;s error rate over iterations.</span>

<span class="c1"># Visualization of error rate over iterations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">error_rates</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Error Rate of the AdaBoost Model Over Iterations&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Error Rate&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Interpretation:</span>
<span class="c1"># This plot shows how the error rate of the AdaBoost model changes over iterations.</span>
<span class="c1"># Ideally, the error rate should decrease as the algorithm iterates, indicating that the model is improving.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/5f785eff7eda09b9638b7a0064c977eff39081bed2d9b99e09c6d9843bdcfde2.png" src="../_images/5f785eff7eda09b9638b7a0064c977eff39081bed2d9b99e09c6d9843bdcfde2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Generate a complex and noisy binary classification dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>  <span class="c1"># Increased number of samples</span>
                           <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>  <span class="c1"># Increased number of total features</span>
                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span>  <span class="c1"># Number of informative features</span>
                           <span class="n">n_redundant</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># Increased number of redundant features to add noise</span>
                           <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># More clusters per class</span>
                           <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">.5</span><span class="p">],</span>  <span class="c1"># Balanced classes</span>
                           <span class="n">flip_y</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>  <span class="c1"># 5% label noise</span>
                           <span class="n">class_sep</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span>  <span class="c1"># Decreased class separation</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Convert labels to -1 and 1 for our AdaBoost implementation</span>

<span class="c1"># Initialize weights for all instances</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Number of instances</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>  <span class="c1"># Initial weight for each instance</span>

<span class="c1"># Placeholder for the AdaBoost steps</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Number of iterations</span>
<span class="n">error_rates</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># To track the error rate of the model at each iteration</span>

<span class="c1"># Initialize variables to store the final model</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Store the alpha values of the weak learners</span>
<span class="n">learners</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Store the weak learners</span>

<span class="c1"># To make a final prediction, we need to combine the weak learners based on their alphas</span>
<span class="k">def</span> <span class="nf">adaBoostPredict</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="c1">#</span>
    <span class="c1"># TODO: calculate a prediction (-1 or +1) based on the values in alphas and learners.</span>
    <span class="c1">#</span>
    <span class="n">final_prediction</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>
    <span class="c1"># Return the sign of the prediction as the final class label</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">final_prediction</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="c1"># Step 1: Train a weak learner.</span>
    <span class="c1"># Use a decision stump (depth=1) as the weak learner</span>
    <span class="n">weak_learner</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">weak_learner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
    
    <span class="c1"># Step 2: Make predictions and calculate the error rate of the weak learner.</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">weak_learner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">miss</span> <span class="o">=</span> <span class="n">predictions</span> <span class="o">!=</span> <span class="n">y_train</span>  <span class="c1"># Boolean array indicating whether each prediction is incorrect</span>
    <span class="n">error_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">miss</span><span class="p">)</span> <span class="o">/</span> <span class="n">weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>  <span class="c1"># Weighted error rate</span>
    
    <span class="c1"># Store the error rate for plotting</span>
    <span class="n">error_rates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error_rate</span><span class="p">)</span>
    
    <span class="c1"># Step 3: Calculate the weak learner&#39;s weight (alpha) using its error rate</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">error_rate</span><span class="p">)</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">error_rate</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">))</span>

    <span class="c1"># Save the trained weak learner and its alpha</span>
    <span class="n">learners</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weak_learner</span><span class="p">)</span>
    <span class="n">alphas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
    
    <span class="c1"># Step 4: Update instance weights</span>
    <span class="c1"># Weights are increased for misclassified instances and decreased for correctly classified instances</span>
    <span class="n">weights</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">miss</span> <span class="o">*</span> <span class="p">((</span><span class="n">weights</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)))</span>
    
    <span class="c1"># Normally, you would also update the predictions of the ensemble model here and check for termination conditions.</span>
    <span class="c1"># For simplicity, we&#39;re focusing on the weight update mechanism and the model&#39;s error rate over iterations.</span>



    <span class="c1"># Test the final model on the training data (for demonstration, normally you&#39;d use a separate test set)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">adaBoostPredict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final model accuracy:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="c1"># Visualization of error rate over iterations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">error_rates</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Error Rate of the AdaBoost Model Over Iterations&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Error Rate&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Interpretation:</span>
<span class="c1"># This plot shows how the error rate of the AdaBoost model changes over iterations.</span>
<span class="c1"># Ideally, the error rate should decrease as the algorithm iterates, indicating that the model is improving.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">line</span> <span class="mi">19</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="c1"># Generate a complex and noisy binary classification dataset</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>  <span class="c1"># Increased number of samples</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span>                            <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>  <span class="c1"># Increased number of total features</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span>                            <span class="n">n_informative</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span>  <span class="c1"># Number of informative features</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span>                            <span class="n">class_sep</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span>  <span class="c1"># Decreased class separation</span>
<span class="g g-Whitespace">     </span><span class="mi">17</span>                            <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">19</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">21</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Convert labels to -1 and 1 for our AdaBoost implementation</span>
<span class="g g-Whitespace">     </span><span class="mi">23</span> <span class="c1"># Initialize weights for all instances</span>

<span class="ne">NameError</span>: name &#39;train_test_split&#39; is not defined
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Additional metrics</span>


<span class="c1"># Initialize weights for all training instances</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Number of training instances</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>  <span class="c1"># Initial weight for each instance</span>

<span class="c1"># Placeholder for the AdaBoost steps</span>
<span class="n">training_error_rates</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># To track the training error rate of the model at each iteration</span>
<span class="n">test_error_rates</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># To track the test error rate of the model at each iteration</span>

<span class="n">all_alphas</span> <span class="o">=</span> <span class="n">alphas</span>
<span class="n">all_learners</span> <span class="o">=</span> <span class="n">learners</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="c1"># Test on the first t final learners. Set alpha and learners to a subset of the final weak learners.</span>
    <span class="n">alphas</span> <span class="o">=</span> <span class="n">all_alphas</span><span class="p">[:</span><span class="n">t</span><span class="p">]</span>
    <span class="n">learners</span> <span class="o">=</span> <span class="n">all_learners</span><span class="p">[:</span><span class="n">t</span><span class="p">]</span>
    
    <span class="c1"># Step 2: Calculate the error rate of the weak learner on both training and test sets.</span>
    <span class="n">train_predictions</span> <span class="o">=</span> <span class="n">adaBoostPredict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">test_predictions</span> <span class="o">=</span> <span class="n">adaBoostPredict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    
    <span class="n">train_miss</span> <span class="o">=</span> <span class="n">train_predictions</span> <span class="o">!=</span> <span class="n">y_train</span>  <span class="c1"># Training set misclassifications</span>
    <span class="n">test_miss</span> <span class="o">=</span> <span class="n">test_predictions</span> <span class="o">!=</span> <span class="n">y_test</span>  <span class="c1"># Test set misclassifications</span>
    
    <span class="n">train_error_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">train_miss</span><span class="p">)</span> <span class="o">/</span> <span class="n">weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">test_error_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_miss</span><span class="p">)</span>  <span class="c1"># Simple mean, as test instances are not weighted</span>
    
    <span class="c1"># Store the error rates</span>
    <span class="n">training_error_rates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_error_rate</span><span class="p">)</span>
    <span class="n">test_error_rates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_error_rate</span><span class="p">)</span>
    
    <span class="c1"># Step 3: Calculate the weak learner&#39;s weight (alpha) using its error rate</span>
    <span class="c1">#alpha = 0.5 * np.log((1 - train_error_rate) / max(train_error_rate, 1e-10))</span>
    
    <span class="c1"># Step 4: Update instance weights</span>
    <span class="c1">#weights *= np.exp(alpha * train_miss * ((weights &gt; 0) | (alpha &lt; 0)))</span>
    
<span class="c1"># Visualization of training and test error rates over iterations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">training_error_rates</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Error Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_error_rates</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test Error Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Error Rates of the AdaBoost Model Over Iterations&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Error Rate&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">line</span> <span class="mi">5</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1"># Additional metrics</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> 
<span class="g g-Whitespace">      </span><span class="mi">3</span> 
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="c1"># Initialize weights for all training instances</span>
<span class="ne">----&gt; </span><span class="mi">5</span> <span class="n">N</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Number of training instances</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>  <span class="c1"># Initial weight for each instance</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="c1"># Placeholder for the AdaBoost steps</span>

<span class="ne">NameError</span>: name &#39;X_train&#39; is not defined
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># hint - adaboost should return alpha * prediction for each learner</span>
<span class="c1"># sum(alpha * learner.predict(X) for alpha, learner in zip(alphas, learners))</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Week_07"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Lesson_32.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Day 32: Introduction to Bagging and Random Forests</p>
      </div>
    </a>
    <a class="right-next"
       href="Lesson_34.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Day 34: Introduction to Gradient Boosting Machines (GBM) and Extreme Gradient Boosting (XGBoost)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Day 33: Introduction to AdaBoost</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importance">Importance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-examples">Applications and Examples</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-weak-learners-in-adaboost">Understanding Weak Learners in AdaBoost</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#role-in-adaboost">Role in AdaBoost</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#selection-and-enhancement-of-weak-learners">Selection and Enhancement of Weak Learners</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-and-understanding">Interpretation and Understanding</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#the-weight-update-rule-in-adaboost">The Weight Update Rule in AdaBoost</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-and-importance">Applications and Importance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost-with-sklearn">Adaboost with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-the-final-output-in-adaboost">Calculating the Final Output in AdaBoost</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-for-the-reader">Exercise For The Reader</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#task">Task</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Aaron S. & John M.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
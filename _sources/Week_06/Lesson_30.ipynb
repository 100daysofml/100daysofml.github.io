{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction to Naive Bayes Classification\n",
                "\n",
                "Naive Bayes classification is a simple yet powerful algorithm in the field of machine learning and data science. It falls under the category of supervised learning, where the goal is to learn a mapping from inputs to outputs based on example input-output pairs. Naive Bayes classifiers leverage probability theory to make predictions, making it especially suitable for applications where the dimensionality of the input data is high.\n",
                "\n",
                "## What is Naive Bayes Classification?\n",
                "\n",
                "Naive Bayes classification is a probabilistic machine learning model that is used for classification tasks. The 'naive' aspect of the model comes from the assumption that the features used to make the classification decision are independent of each other, given the target variable. Despite this simplifying assumption, Naive Bayes classifiers often perform remarkably well and are particularly useful when the dataset is not too large and the assumption of feature independence is reasonable.\n",
                "\n",
                "### **Definition:**\n",
                "\n",
                "At its core, Naive Bayes classification applies Bayes\u2019 Theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable. Bayes\u2019 Theorem is mathematically represented as:\n",
                "\n",
                "$$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$\n",
                "\n",
                "Where:\n",
                "- $P(A|B)$ is the posterior probability of class A given predictor B,\n",
                "- $P(B|A)$ is the likelihood, which is the probability of predictor B given class A,\n",
                "- $P(A)$ is the prior probability of class A, and\n",
                "- $P(B)$ is the prior probability of predictor B.\n",
                "\n",
                "For classification, this formula is used to estimate the probability of a particular class given a set of features (predictors).\n",
                "\n",
                "### **Importance:**\n",
                "\n",
                "Naive Bayes Classification is important because of its simplicity, efficiency, and effectiveness, especially in dealing with categorical data. It performs well in case of text classification tasks like spam filtering and sentiment analysis. Its probabilistic nature allows it to deal with uncertainties and make predictions even with incomplete knowledge, by handling the data attributes independently.\n",
                "\n",
                "## Applications and Examples\n",
                "\n",
                "Naive Bayes Classification has a wide range of applications across various fields:\n",
                "\n",
                "1. **Text Classification / Spam Filtering:** By analyzing the frequency of words and their association with spam or non-spam emails, Naive Bayes classifiers can effectively filter out unwanted emails.\n",
                "2. **Sentiment Analysis:** It is used to determine the sentiment expressed in a piece of text, positive or negative, based on the presence and combinations of words.\n",
                "3. **Recommendation Systems:** Naive Bayes can contribute to recommendation systems by classifying items based on user preferences and past user behavior.\n",
                "4. **Medical Diagnosis:** In healthcare, Naive Bayes classifiers can predict the likelihood of a disease given the presence of certain symptoms.\n",
                "\n",
                "Through these applications, it is evident that despite its simplicity, Naive Bayes Classification continues to play a crucial role in the field of machine learning and artificial intelligence, handling both simple and complex classification tasks efficiently.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false
            },
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import make_classification\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "# Generate a simple dataset\n",
                "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
                "\n",
                "# Split the dataset into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
                "\n",
                "# Initialize classifiers\n",
                "classifiers = {\n",
                "    'Naive Bayes': GaussianNB(),\n",
                "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
                "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=200)\n",
                "}\n",
                "\n",
                "# Dictionary to hold accuracy scores\n",
                "accuracy_scores = {}\n",
                "\n",
                "# Fit models and calculate accuracy\n",
                "for name, clf in classifiers.items():\n",
                "    clf.fit(X_train, y_train)\n",
                "    y_pred = clf.predict(X_test)\n",
                "    accuracy_scores[name] = accuracy_score(y_test, y_pred)\n",
                "\n",
                "# Data for plotting\n",
                "names = list(accuracy_scores.keys())\n",
                "values = list(accuracy_scores.values())\n",
                "\n",
                "# Create bar chart\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.bar(names, values, color=['blue', 'green', 'red'])\n",
                "plt.title('Predictive Accuracy Comparison')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.xlabel('Classifier')\n",
                "plt.ylim([0.5, 1.0])  # Set the limits for the y-axis to have a clear comparison\n",
                "plt.xticks(names)  # Ensure classifier names are used as labels on the x-axis\n",
                "\n",
                "# Display the bar chart\n",
                "plt.show()\n",
                "\n",
                "# Interpretation:\n",
                "# This visualization illustrates the comparative predictive accuracy of Naive Bayes, Decision Tree,\n",
                "# and Logistic Regression classifiers on a simple dataset. Such visual comparisons can help in\n",
                "# selecting an appropriate model based on accuracy for specific applications or datasets.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Foundations of Probability in Classification\n",
                "\n",
                "Probability theory plays a critical role in various aspects of data science and machine learning, particularly in classification tasks. Understanding the foundational concepts of probability is essential for algorithms like the Naive Bayes classifier, which relies heavily on these principles to make predictions. This section delves into the essential probability concepts such as conditional probability, joint probability, and independence, laying the groundwork for understanding how Naive Bayes and similar models function.\n",
                "\n",
                "## What is Probability in Classification?\n",
                "\n",
                "**Text:** In the context of classification, probability helps us quantify the uncertainty regarding the assignment of data points to a particular class or category. It enables us to make informed decisions based on the likelihood of different outcomes.\n",
                "\n",
                "**Definition:**\n",
                "- **Conditional Probability:** Given two events $A$ and $B$, the conditional probability of $A$ given $B$ is denoted as $P(A|B)$ and is calculated using the formula:\n",
                "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
                "It represents the probability of event $A$ occurring given that $B$ has already occurred.\n",
                "\n",
                "- **Joint Probability:** The joint probability of two events $A$ and $B$, denoted as $P(A \\cap B)$, represents the probability of both events happening at the same time.\n",
                "\n",
                "- **Independence:** Two events $A$ and $B$ are considered independent if the occurrence of one does not affect the occurrence of the other, mathematically represented as:\n",
                "$$P(A \\cap B) = P(A)P(B)$$\n",
                "\n",
                "**Importance:** These concepts are pivotal for classification algorithms, especially Naive Bayes, which operates under the assumption of feature independence and requires the computation of probabilities to predict the class of a given input. Understanding these foundational concepts allows data scientists to implement, evaluate, and improve models effectively, making them crucial in fields like spam detection, sentiment analysis, and more.\n",
                "\n",
                "## Applications and Examples\n",
                "\n",
                "- **Spam Detection:** Naive Bayes classifiers are widely used in email spam detection. By calculating the conditional probability of an email being spam given the presence of certain words, the classifier can effectively filter out unwanted messages.\n",
                "\n",
                "- **Medical Diagnosis:** Probability in classification can assist in medical diagnosis by estimating the likelihood of a disease given various patient symptoms. For instance, a Naive Bayes model might evaluate the probability of a patient having a certain condition based on their symptoms, helping doctors in decision-making.\n",
                "\n",
                "- **Sentiment Analysis:** In natural language processing, classification models assess the sentiment of text data (e.g., positive, negative, neutral). Using probability, these models can analyze word frequencies and other features to classify the sentiment of user reviews, social media posts, etc.\n",
                "\n",
                "These examples underscore the ubiquity and importance of probability in classification across different domains. By mastering these foundational concepts, one gains a powerful toolkit for tackling a wide array of data science and machine learning challenges, making probability theory an indispensable part of a data scientist's education.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false
            },
            "outputs": [],
            "source": [
                "# Import the necessary library for plotting\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib_venn import venn2, venn2_circles\n",
                "\n",
                "# Set up the figure and axis for the Venn diagram\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "plt.title(\"Probability Concepts in Classification\")\n",
                "\n",
                "# Create the Venn diagram to illustrate joint and conditional probabilities\n",
                "# The sizes are symbolic and do not represent actual probabilities\n",
                "venn = venn2(subsets=(3, 3, 1), set_labels=('P(A)', 'P(B)'))\n",
                "venn_circles = venn2_circles(subsets=(3, 3, 1))\n",
                "\n",
                "# Annotate for Conditional Probability\n",
                "plt.text(-0.80, -0.70, \"P(A|B) = P(A \u2229 B) / P(B)\", fontsize=14)\n",
                "\n",
                "# Annotate for Joint Probability\n",
                "plt.text(-0.20, 0, \"P(A \u2229 B)\", fontsize=14, color=\"white\")\n",
                "\n",
                "# Annotate for Independence\n",
                "plt.text(-0.80, 0.50, \"If Independent:\\nP(A \u2229 B) = P(A)P(B)\", fontsize=14)\n",
                "\n",
                "# Enhance the visualization\n",
                "plt.annotate(\n",
                "    'P(A)P(B) = P(A \u2229 B)\\n(A and B are independent)', \n",
                "    xy=venn.get_label_by_id('10').get_position(), \n",
                "    xytext=(-1.0,-0.3), \n",
                "    arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=-0.5\", color=\"blue\"), \n",
                "    fontsize=12, color=\"blue\",\n",
                "    ha='center')\n",
                "\n",
                "plt.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "This Venn diagram visualizes the relationship between joint probability ($P(A \\cap B)$), conditional probability ($P(A|B)$), and the concept of independence between two events in the context of classification tasks like those performed by Naive Bayes. The diagram illustrates how these fundamental probability concepts interrelate, providing a foundation for understanding how probability informs decision-making in classification algorithms.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Principles of Naive Bayes Classifier\n",
                "\n",
                "The Naive Bayes Classifier is a fundamental algorithm in the field of machine learning, known for its simplicity and effectiveness in handling classification problems. Despite its straightforward approach, the Naive Bayes Classifier plays a crucial role in various applications, from spam filtering to sentiment analysis. This lesson will delve into the principles underpinning the Naive Bayes Classifier, the naive assumption of feature independence, and its various types suitable for different data characteristics.\n",
                "\n",
                "## What is the Naive Bayes Classifier?\n",
                "\n",
                "### Definition\n",
                "The Naive Bayes Classifier is a probabilistic machine learning model used for classification tasks. It is based on Bayes' Theorem, which describes the probability of an event, based on prior knowledge of conditions that might be related to the event. In the context of Naive Bayes, the event is the class label (C) of a data point, and the conditions are the features (X) of that data point. The classifier makes the naive assumption that the features are independent of each other given the class label. Mathematically, Bayes' Theorem can be expressed as:\n",
                "\n",
                "$$ P(C|X) = \\frac{P(X|C) \\times P(C)}{P(X)} $$\n",
                "\n",
                "where:\n",
                "- $P(C|X)$ is the probability of class $C$ given features $X$,\n",
                "- $P(X|C)$ is the probability of observing features $X$ given class $C$,\n",
                "- $P(C)$ is the prior probability of observing class $C$,\n",
                "- $P(X)$ is the prior probability of observing features $X$.\n",
                "\n",
                "### Importance\n",
                "Despite its simplicity, the Naive Bayes Classifier is incredibly powerful, especially in domains where the dimensionality of the data is high. Its assumption of feature independence simplifies the computation, making it highly scalable and efficient for large datasets. Moreover, it performs surprisingly well even when the independence assumption is violated, making it a versatile tool in the machine learning toolkit. The classifier is particularly useful in text classification tasks where features (e.g., words) exhibit high degrees of correlation.\n",
                "\n",
                "## Applications and Examples\n",
                "\n",
                "The Naive Bayes Classifier finds its applications across numerous fields:\n",
                "\n",
                "1. **Spam Detection**: In email clients, Naive Bayes is used to classify emails as spam or ham (not spam) by learning the likelihood of certain words appearing in spam versus legitimate emails.\n",
                "\n",
                "2. **Sentiment Analysis**: It's applied to analyze social media posts, reviews, or any text data to ascertain the sentiment (positive, negative, or neutral) expressed by the text, based on the presence and combinations of words.\n",
                "\n",
                "3. **Document Classification**: Naive Bayes classifiers are used to automatically categorize documents into predefined topics based on their content, streamlining document management in large organizations.\n",
                "\n",
                "4. **Medical Diagnosis**: By analyzing patient data and the symptoms exhibited, Naive Bayes can help in predicting the likelihood of a patient having a certain disease.\n",
                "\n",
                "### Types of Naive Bayes Models\n",
                "\n",
                "The effectiveness of a Naive Bayes Classifier is partly determined by choosing the right model based on the characteristics of the input data:\n",
                "\n",
                "- **Gaussian Naive Bayes**: Assumes the continuous values associated with each feature are distributed according to a Gaussian (normal) distribution. It's best for data with a continuous or real-valued attributes.\n",
                "  \n",
                "- **Multinomial Naive Bayes**: Particularly used for document classification, it assumes that features (e.g., word counts) follow a multinomial distribution. It's ideal for data that can be turned into counts or frequency metrics.\n",
                "  \n",
                "- **Bernoulli Naive Bayes**: Assumes binary-valued features and is suitable for making predictions from binary feature vectors.\n",
                "\n",
                "In practice, the choice among Gaussian, Multinomial, and Bernoulli Naive Bayes depends on the nature of your dataset and the specific requirements of your application.\n",
                "\n",
                "In conclusion, the Naive Bayes Classifier's principles provide a robust foundation for tackling classification problems across a myriad of domains. Its simplicity, coupled with its surprising effectiveness, makes it an invaluable tool for both novice and experienced machine learning practitioners.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false
            },
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Data for plotting\n",
                "data_types = ['Continuous', 'Count', 'Binary']\n",
                "nb_variants = ['Gaussian', 'Multinomial', 'Bernoulli']\n",
                "suitability_scores = {\n",
                "    'Gaussian': [0.9, 0.2, 0.1],\n",
                "    'Multinomial': [0.3, 0.9, 0.4],\n",
                "    'Bernoulli': [0.1, 0.7, 0.9]\n",
                "}\n",
                "\n",
                "bar_width = 0.25\n",
                "index = np.arange(len(data_types))\n",
                "\n",
                "# Plotting\n",
                "fig, ax = plt.subplots()\n",
                "\n",
                "# Creating bars for each Naive Bayes variant\n",
                "for i, variant in enumerate(nb_variants):\n",
                "    plt.bar(index + i * bar_width, suitability_scores[variant], width=bar_width, label=variant)\n",
                "\n",
                "# Customization\n",
                "ax.set_xlabel('Data Type')\n",
                "ax.set_ylabel('Suitability Score')\n",
                "ax.set_title('Suitability of Naive Bayes Variants by Data Type')\n",
                "ax.set_xticks(index + bar_width)\n",
                "ax.set_xticklabels(data_types)\n",
                "ax.legend()\n",
                "\n",
                "# Show plot\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "This code generates a comparison graph using a bar chart to show the suitability of different Naive Bayes classifiers (Gaussian, Multinomial, and Bernoulli) for handling various types of data (Continuous, Count, and Binary). Each Naive Bayes variant is assessed on a suitability score (on a scale from 0 to 1) for each data type, demonstrating the effectiveness or preference of using a particular variant for a specific type of data. For instance, Gaussian Naive Bayes is most suitable for continuous data, while Multinomial and Bernoulli are better suited for count and binary data, respectively.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exercise For The Reader: Implementing Naive Bayes with Scikit-Learn\n",
                "\n",
                "In this section, we will walk through an exercise designed to give you hands-on experience with Naive Bayes classification, one of the simplest yet effective algorithms in the realm of supervised learning. By using the Python library `scikit-learn`, we will tackle a simple dataset, guiding you from data preparation to model evaluation. This practical exercise aims to solidify your understanding of Naive Bayes and demonstrate its efficacy in classification tasks.\n",
                "\n",
                "## What is Naive Bayes Classification?\n",
                "\n",
                "**Text:** Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features. They are remarkably straightforward and efficient, requiring a small amount of training data to estimate the necessary parameters to make predictions about the data.\n",
                "\n",
                "**Definition:** Naive Bayes classification assumes all the features to be independent of each other to predict the probability that a given sample belongs to a certain class. The model is represented mathematically as:\n",
                "\n",
                "$$ P(C_k | x_1, ..., x_n) = \\frac{P(C_k) P(x_1, ..., x_n | C_k)}{P(x_1, ..., x_n)} $$\n",
                "\n",
                "where $C_k$ is a class variable, $x_1, ..., x_n$ are feature variables, $P(C_k | x_1, ..., x_n)$ is the posterior probability of class $C_k$ given predictors $x_1, ..., x_n$.\n",
                "\n",
                "**Importance:** Naive Bayes classifiers work extremely well in many real-world situations, famously for document classification and spam filtering. They require a small amount of training data to estimate the test data's parameters. Furthermore, Naive Bayes can be scaled to large datasets and works well with categorical and numerical data.\n",
                "\n",
                "## Applications and Examples\n",
                "\n",
                "Naive Bayes classifiers find applications in various fields:\n",
                "\n",
                "- **Email Spam Detection:** Classifying emails as spam or not spam based on the frequency of words used.\n",
                "- **Document Classification:** Categorizing news articles into predefined topics based on the text content.\n",
                "- **Sentiment Analysis:** Analyzing social media text to determine the sentiment expressed (positive, negative, or neutral).\n",
                "- **Medical Diagnosis:** Predicting the likelihood of a disease given the symptoms and patient data.\n",
                "\n",
                "## Exercise Steps\n",
                "\n",
                "1. **Loading the Dataset:** Our first step will be to import a dataset. We will work with a simple dataset like the Iris dataset, which is readily available in `scikit-learn`.\n",
                "\n",
                "2. **Splitting Dataset:** We will split our dataset into training and testing sets to prepare our data for the model.\n",
                "\n",
                "3. **Model Fitting:** We will create a Naive Bayes classifier and fit it to our training data. This involves learning the parameters which make our model ready to make predictions.\n",
                "\n",
                "4. **Making Predictions:** With the trained model, we will make predictions on our testing set.\n",
                "\n",
                "5. **Model Evaluation:** Finally, we will evaluate the performance of our Naive Bayes classifier using metrics such as accuracy, precision, recall, and the confusion matrix.\n",
                "\n",
                "Through this exercise, you will gain a practical understanding of preparing data, fitting a model, making predictions, and evaluating a classifier's performance. This hands-on experience is invaluable for grasping the nuances of Naive Bayes classification and its implementation using `scikit-learn`.\n",
                "\n",
                "Embarking on this exercise will demonstrate the simplicity and power of Naive Bayes classifiers and will equip you with the knowledge to apply this model to your datasets. Let's dive in and explore the world of Naive Bayes through this interactive exercise!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false
            },
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.datasets import load_iris\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
                "\n",
                "# Load the Iris dataset\n",
                "iris = load_iris()\n",
                "X = iris.data\n",
                "y = iris.target\n",
                "\n",
                "# Split dataset into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# TODO: Fit the Naive Bayes model\n",
                "# Use GaussianNB() to create a Gaussian Naive Bayes classifier\n",
                "# Fit the classifier to the training data\n",
                "\n",
                "# Example:\n",
                "# model = GaussianNB()\n",
                "# model.fit(X_train, y_train)\n",
                "\n",
                "# TODO: Make predictions\n",
                "# Use the trained model to make predictions on the test set\n",
                "\n",
                "# Example:\n",
                "# predictions = model.predict(X_test)\n",
                "\n",
                "# TODO: Evaluate the model\n",
                "# Calculate and print the accuracy, precision, recall, and confusion matrix using the true labels and your predictions\n",
                "\n",
                "# Example:\n",
                "# accuracy = accuracy_score(y_test, predictions)\n",
                "# precision = precision_score(y_test, predictions, average='micro')\n",
                "# recall = recall_score(y_test, predictions, average='micro')\n",
                "# conf_matrix = confusion_matrix(y_test, predictions)\n",
                "# print(f\"Accuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nConfusion Matrix:\\n{conf_matrix}\")\n",
                "\n",
                "# Fill in the above TODOs to complete the exercise on implementing and evaluating a Naive Bayes classifier.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "This starter code sets up the initial steps for implementing a Naive Bayes classifier using the Iris dataset. The TODO comments guide you through fitting the model, making predictions, and evaluating its performance. This approach allows learners to engage directly with the key steps necessary for applying Naive Bayes classification to a dataset.\n"
            ]
        }
    ],
    "metadata": {},
    "nbformat": 4,
    "nbformat_minor": 0
}
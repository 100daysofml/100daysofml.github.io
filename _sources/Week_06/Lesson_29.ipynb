{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction to Decision Trees\n",
                "\n",
                "Decision Trees are a fundamental machine learning algorithm that finds extensive use in both classification and regression tasks. They serve as an indispensable tool for predictive modeling, offering clear visualization of decision-making processes and straightforward interpretation of data. Decision Trees mimic human decision-making processes, making them an intuitive option for solving complex problems by breaking them down into smaller, manageable parts.\n",
                "\n",
                "## What is a Decision Tree?\n",
                "\n",
                "A Decision Tree is a flowchart-like structure where each internal node represents a \"decision\" on an attribute, each branch represents an outcome of the decision, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.\n",
                "\n",
                "### Definition\n",
                "\n",
                "Formally, a Decision Tree is a binary tree where each internal node splits the dataset into two groups based on the feature that results in the most significant information gain (IG). Information gain is calculated using metrics like Gini impurity or entropy.\n",
                "\n",
                "In mathematical terms, if we denote a dataset as $D$ which consists of instances $(x_i, y_i), i=1,2,...,N$, where $x_i$ is the feature vector and $y_i$ is the target label, then the goal of the Decision Tree is to partition $D$ into subsets $D_1, D_2, ... , D_k$ based on feature values that optimize a given objective criterion (e.g., maximizing information gain).\n",
                "\n",
                "### Importance\n",
                "\n",
                "Decision Trees are important for several reasons:\n",
                "\n",
                "1. **Simplicity:** They are easy to understand and interpret, making them accessible to people with non-technical backgrounds.\n",
                "2. **Versatility:** They can handle both numerical and categorical data and can be used for both regression and classification tasks.\n",
                "3. **Feature Importance:** They inherently perform feature selection, indicating which features are most important for prediction.\n",
                "4. **Visualization:** The tree structure can be easily visualized, allowing for a straightforward inspection of decision paths.\n",
                "\n",
                "## Applications and Examples\n",
                "\n",
                "Decision Trees find applications across diverse fields due to their simplicity and versatility.\n",
                "\n",
                "- **Finance:** For credit scoring by analyzing customer data to predict their likelihood of defaulting on loans.\n",
                "- **Healthcare:** For diagnosing patients based on their symptoms and medical history.\n",
                "- **Marketing:** To identify potential customer segments and target them with specific marketing strategies.\n",
                "- **Manufacturing:** For predicting equipment failures by analyzing operation data.\n",
                "- **Computer Science:** In the development of recommendation systems that suggest products or content based on user preferences and past behavior.\n",
                "\n",
                "For example, in the healthcare field, a Decision Tree might be used to diagnose a disease based on patient symptoms. The root node could represent the most significant symptom, with branches leading to nodes representing secondary symptoms, and leaf nodes representing possible diagnoses.\n",
                "\n",
                "In summary, Decision Trees play a crucial role in the fields of machine learning and artificial intelligence. They provide an effective approach for both data classification and regression, with the added benefits of simplicity, interpretability, and application in a wide range of disciplines.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false
            },
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Create a basic diagram of a decision tree structure\n",
                "\n",
                "# Create figure and axis\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "\n",
                "# Coordinates of each node\n",
                "nodes = {\n",
                "    \"Root\": (5, 5),\n",
                "    \"Decision 1\": (3, 4),\n",
                "    \"Decision 2\": (7, 4),\n",
                "    \"Leaf 1\": (2, 3),\n",
                "    \"Leaf 2\": (4, 3),\n",
                "    \"Leaf 3\": (6, 3),\n",
                "    \"Leaf 4\": (8, 3)\n",
                "}\n",
                "\n",
                "# Lines connecting nodes to simulate branches\n",
                "edges = [\n",
                "    (\"Root\", \"Decision 1\"),\n",
                "    (\"Root\", \"Decision 2\"),\n",
                "    (\"Decision 1\", \"Leaf 1\"),\n",
                "    (\"Decision 1\", \"Leaf 2\"),\n",
                "    (\"Decision 2\", \"Leaf 3\"),\n",
                "    (\"Decision 2\", \"Leaf 4\")\n",
                "]\n",
                "\n",
                "# Plot nodes\n",
                "for node, (x, y) in nodes.items():\n",
                "    ax.scatter(x, y, s=1000, c='skyblue')\n",
                "    ax.text(x, y, node, ha='center', va='center')\n",
                "\n",
                "# Plot edges (branches)\n",
                "for start, end in edges:\n",
                "    start_x, start_y = nodes[start]\n",
                "    end_x, end_y = nodes[end]\n",
                "    ax.plot([start_x, end_x], [start_y, end_y], 'k-')\n",
                "\n",
                "# Hide axes\n",
                "ax.set_axis_off()\n",
                "\n",
                "# Title\n",
                "plt.title(\"Example Decision Tree Structure\")\n",
                "\n",
                "plt.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "This visualization provides a simple representation of a Decision Tree with one root node, two decision nodes, and four leaf nodes. It illustrates how data is split at each node until it reaches a decision (leaf nodes). The root node is the starting point, decision nodes represent the branching based on certain conditions, and leaf nodes represent the final decision or prediction.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Entropy and Information Gain in Decision Trees\n",
                "\n",
                "Decision Trees are a popular machine learning method used for both classification and regression tasks. At their core, they model decisions and their possible consequences, including chance event outcomes, resource costs, and utility. Two fundamental concepts that guide the construction of a decision tree are Entropy and Information Gain. Understanding these concepts is crucial for grasifying how decision trees decide where to split the data.\n",
                "\n",
                "## What is Entropy?\n",
                "\n",
                "### Text\n",
                "\n",
                "Entropy is a measure borrowed from physics and information theory that represents the degree of disorder, randomness, or uncertainty in a dataset. In the context of machine learning, and more specifically in decision trees, it plays a pivotal role in determining how a dataset can be split in the most informative way.\n",
                "\n",
                "### Definition\n",
                "\n",
                "In a classification problem, entropy can be mathematically expressed as:\n",
                "\n",
                "$$\n",
                "- \\sum_{i=1}^{n} p_i \\log_2(p_i)\n",
                "$$\n",
                "\n",
                "where $n$ is the number of classes and $p_i$ is the probability of class $i$ within the subset. For each class, it multiplies the probability of class $i$ ($p_i$) by the log base 2 of $p_i$, sums across all classes, and takes the negative of that sum.\n",
                "\n",
                "### Importance\n",
                "\n",
                "Entropy serves as a measure of purity or homogeneity in a dataset. In the context of decision trees, it's employed to determine how a dataset should be split at each node. High entropy in a dataset means more disorder\u2014it indicates that the data is more mixed (contains a higher variety of classes). Conversely, low entropy suggests a more orderly distribution of data, or that most elements belong to the same class. Therefore, decreasing entropy through splits allows the model to make more accurate predictions, as nodes become increasingly homogeneous.\n",
                "\n",
                "## What is Information Gain?\n",
                "\n",
                "### Text\n",
                "\n",
                "Following the concept of entropy, Information Gain measures the reduction in entropy or disorder in a dataset after a split. It quantifies how much information a feature gives us about the class.\n",
                "\n",
                "### Definition\n",
                "\n",
                "Information Gain is calculated as the difference between the initial entropy of the entire dataset and the weighted entropy after splitting the dataset based on an attribute. Mathematically, it's represented as:\n",
                "\n",
                "$$\n",
                "IG(D, A) = Entropy(D) - \\sum_{v \\in Values(A)} \\frac{|D_v|}{|D|} Entropy(D_v)\n",
                "$$\n",
                "\n",
                "where:\n",
                "- $IG(D, A)$ is the information gain of dataset $D$ after being split based on attribute $A$,\n",
                "- $Entropy(D)$ is the original entropy of the dataset,\n",
                "- $Values(A)$ are the different values of attribute $A$,\n",
                "- $|D_v|$ is the number of instances in $D$ that have value $v$ for attribute $A$,\n",
                "- and $Entropy(D_v)$ is the entropy of the subset of $D$ that has value $v$ for attribute $A$.\n",
                "\n",
                "### Importance\n",
                "\n",
                "Information gain is used in decision trees to select the attribute that best splits the dataset at each node. An attribute with higher information gain will result in a purer child node\u2014or, in other words, nodes with lower entropy. Therefore, maximizing information gain at each step of building a tree ensures that the model asks the most informative questions first, leading to a faster reduction in uncertainty or disorder in the dataset.\n",
                "\n",
                "## Applications and Examples\n",
                "\n",
                "Decision trees, guided by principles of entropy and information gain, are widely applicable in various fields for classification and regression tasks. For example:\n",
                "\n",
                "- **In medicine,** they can help diagnose diseases based on a series of symptoms and patient data, efficiently narrowing down possible conditions.\n",
                "- **In finance,** decision trees are used for credit scoring, identifying which variables and customer features influence credit risk the most.\n",
                "- **In marketing,** these models can help predict customer behavior and segment customers based on their likelihood to purchase a product.\n",
                "\n",
                "Each of these applications involves making decisions based on the data's attributes, where understanding and managing uncertainty and disorder through entropy and information gain become crucial to building effective models.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false
            },
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Sample dataset before and after split\n",
                "# Suppose we have a binary classification problem with 'Red' and 'Blue' as classes\n",
                "# Initially, the dataset is mixed: 6 Red and 6 Blue points\n",
                "# After a split (for example, based on a certain feature), we get two subsets:\n",
                "# Subset 1: 5 Red and 1 Blue, Subset 2: 1 Red and 5 Blue\n",
                "\n",
                "# Function to calculate entropy\n",
                "def entropy(elements):\n",
                "    total = sum(elements)\n",
                "    return -sum((p/total) * np.log2(p/total) for p in elements if p != 0)\n",
                "\n",
                "# Initial entropy\n",
                "initial_entropy = entropy([6, 6])\n",
                "\n",
                "# Entropy after split\n",
                "entropy_subset1 = entropy([5, 1])\n",
                "entropy_subset2 = entropy([1, 5])\n",
                "\n",
                "# Weighted entropy after split\n",
                "weighted_entropy = (6/12) * entropy_subset1 + (6/12) * entropy_subset2\n",
                "\n",
                "# Information Gain\n",
                "information_gain = initial_entropy - weighted_entropy\n",
                "\n",
                "# Visualization\n",
                "fig, ax = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
                "\n",
                "# Initial dataset\n",
                "ax[0].bar(['Red', 'Blue'], [6, 6], color=['red', 'blue'])\n",
                "ax[0].set_title('Initial Dataset\\nEntropy = {:.2f}'.format(initial_entropy))\n",
                "ax[0].set_ylim(0, 7)\n",
                "\n",
                "# Subset 1\n",
                "ax[1].bar(['Red', 'Blue'], [5, 1], color=['red', 'blue'])\n",
                "ax[1].set_title('Subset 1 After Split\\nEntropy = {:.2f}'.format(entropy_subset1))\n",
                "\n",
                "# Subset 2\n",
                "ax[2].bar(['Red', 'Blue'], [1, 5], color=['red', 'blue'])\n",
                "ax[2].set_title('Subset 2 After Split\\nEntropy = {:.2f}'.format(entropy_subset2))\n",
                "\n",
                "plt.suptitle('Entropy and Information Gain from a Split', fontsize=16)\n",
                "plt.figtext(0.5, 0.01, 'Information Gain = {:.2f}'.format(information_gain), ha='center', fontsize=14)\n",
                "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
                "plt.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "This Python script visualizes a simple dataset before and after a hypothetical split, illustrating the concept of entropy and information gain as discussed. Initially, the dataset is evenly split between two classes ('Red' and 'Blue'). After splitting based on a specific attribute, we obtain two subsets with differing compositions. The script calculates and displays the entropy for each subset and the dataset before splitting, as well as the overall information gain achieved by the split. The visual representation supports the explanation by showing how a split affects the composition of the subsets, aiming to reduce entropy and thereby increasing information gain\u2014key principles in the construction of decision trees.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Building and Interpreting Decision Trees in Python\n",
                "\n",
                "In this section, we embark on the journey of understanding and implementing Decision Trees using Python\u2019s very own toolkit, scikit-learn. Not just a powerful predictive model, Decision Trees also offer the rare capability of being quite interpretable. We will also delve into visualizing how these models make decisions, thereby offering insights into their inner workings.\n",
                "\n",
                "## What is a Decision Tree?\n",
                "\n",
                "**Text**: A Decision Tree is akin to a flowchart where each internal node represents a \"test\" or \"decision\" on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label (a decision taken after computing all attributes). The paths from root to leaf represent classification rules.\n",
                "\n",
                "**Definition**: Mathematically, a decision tree is a model that recursively splits data into subsets based on the value of input features. This process can be represented as:\n",
                "\n",
                "- Given a dataset $D$, a decision tree splits it into subsets $\\{D_1, D_2, ..., D_k\\}$ using some feature $X$, where $k$ is determined by the unique values of $X$ in $D$ if $X$ is categorical, or a threshold if $X$ is numerical.\n",
                "- This process repeats at each node with the subsets, till a stopping criterion is met. \n",
                "\n",
                "The objective function that measures the quality of a split varies; common ones include Gini impurity $G(D) = 1 - \\sum_{i=1}^{n}{p_i}^2$ and entropy $H(D) = - \\sum_{i=1}^{n}p_i \\log_{2}p_i$, where $p_i$ is the probability of class $i$ in the dataset $D$.\n",
                "\n",
                "**Importance**: Decision Trees are crucial for various reasons. Their ability to break down complex decision-making processes into simpler, understandable rules is invaluable for transparency and interpretability. Furthermore, they are versatile, being applicable for both classification and regression tasks. In fields ranging from finance for credit scoring to healthcare for diagnosing diseases, their simplicity in concept yet profound utility in application cannot be overstated.\n",
                "\n",
                "## Applications and Examples\n",
                "\n",
                "### Finance: Credit Scoring\n",
                "In the finance industry, decision trees can evaluate potential borrowers' creditworthiness by analyzing various attributes such as income, debt-to-income ratio, and credit history. For instance, a decision tree might classify applicants into 'low risk' and 'high risk' categories, optimizing the lending process.\n",
                "\n",
                "### Healthcare: Diagnosing Diseases\n",
                "Decision trees have proven exceptionally useful in the healthcare sector, where they help diagnose diseases by systematically assessing symptoms and test results. A well-crafted decision tree could help distinguish between different types of illnesses based on input variables such as age, temperature, blood pressure, etc.\n",
                "\n",
                "### Marketing: Customer Segmentation\n",
                "Marketing teams use decision trees to segment customers based on behaviors and preferences. This segmentation allows for targeted marketing campaigns, where a decision tree might help identify which segments are more likely to respond to a specific advertising strategy.\n",
                "\n",
                "### Understanding and Tackling Overfitting\n",
                "While decision trees are powerful, they are prone to overfitting, especially in scenarios with complex datasets or when the trees are allowed to grow without constraints. Overfitting happens when the model learns the noise in the training data, reducing its ability to generalize to new data.\n",
                "\n",
                "To mitigate overfitting, we employ techniques like pruning. Pruning can be done in two ways:\n",
                "\n",
                "- **Pre-pruning**: Limiting the growth of trees by setting parameters such as `max_depth`, `min_samples_leaf`, etc.\n",
                "- **Post-pruning**: Allowing the tree to grow fully and then removing insignificant branches.\n",
                "\n",
                "Both techniques are crucial in enhancing the model's generalization capabilities.\n",
                "\n",
                "In the following sections, we\u2019ll dive into the practical steps of setting up, visualizing, and optimizing Decision Trees using Python\u2019s scikit-learn, matplotlib, and graphviz libraries, ensuring a balance between model complexity and generalization.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false
            },
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import load_iris\n",
                "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Load the Iris dataset\n",
                "iris = load_iris()\n",
                "X = iris.data\n",
                "y = iris.target\n",
                "\n",
                "# Split the dataset into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Decision Tree fitting with different max_depth values to illustrate overfitting and pruning\n",
                "depth_values = [1, 3, None] # None implies full growth of the tree (potential overfitting)\n",
                "fig, axes = plt.subplots(nrows=1, ncols=len(depth_values), figsize=(20, 4), dpi=300)\n",
                "\n",
                "for index, max_depth in enumerate(depth_values):\n",
                "    # Fit the Decision Tree model\n",
                "    dt_clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
                "    dt_clf.fit(X_train, y_train)\n",
                "    \n",
                "    # Plot the trained Decision Tree\n",
                "    plot_title = f\"Decision Tree with max_depth = {max_depth}\" if max_depth is not None else \"Decision Tree (No Pruning)\"\n",
                "    plot_tree(dt_clf, ax=axes[index], feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
                "    axes[index].set_title(plot_title)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Interpretation: \n",
                "# The first tree (max_depth=1) is an example of underfitting - too simple to capture patterns.\n",
                "# The second tree (max_depth=3) may represent a balanced model - a good middle ground.\n",
                "# The third tree with no pruning (max_depth=None) may overfit the data by learning too much detail, including noise.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "This code snippet demonstrates fitting a Decision Tree model to the Iris dataset with scikit-learn and how the `max_depth` parameter affects the model's complexity and potential for overfitting. By visualizing trees with different `max_depth` values (including without a limit, leading to full growth), we illustrate the concept of pruning and its role in preventing overfitting. Through these visualized trees, one can observe how limiting the depth of the tree (pruning) can help in making the model simpler and potentially more generalizable to unseen data, balancing between underfitting and overfitting.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exercise For The Reader: Building and Visualizing a Decision Tree Model\n",
                "\n",
                "In this exercise, we'll embark on the exciting journey of applying your newfound knowledge in machine learning by building and interpreting a decision tree model. This hands-on task will not only solidify your understanding of decision trees but also introduce you to the essential steps of working with real datasets.\n",
                "\n",
                "## What is a Decision Tree?\n",
                "\n",
                "A decision tree is one of the most intuitive and widespread machine learning algorithms used for both classification and regression tasks.\n",
                "\n",
                "**Definition:** A decision tree is a flowchart-like tree structure, where an internal node represents a feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner called recursive partitioning. This flowchart-like structure helps in decision making. Its visualization helps in easily understanding the model.\n",
                "\n",
                "$$\n",
                "\\text{Information Gain} = \\text{Entropy(parent)} - \\left(\\frac{\\text{Number of samples in left node}}{\\text{Total samples in parent}}\\times \\text{Entropy(left node)} + \\frac{\\text{Number of samples in right node}}{\\text{Total samples in parent}}\\times \\text{Entropy(right node)}\\right)\n",
                "$$\n",
                "\n",
                "**Importance:** The simplicity of decision trees is their biggest advantage. They easily handle categorical variables and do not require any data preprocessing like normalization or standardization. Being a non-parametric method, they are considered quite robust to outliers. Decision trees can easily visualize and interpret the model's decisions, making them vital in sectors requiring transparency and explainability, such as finance and healthcare.\n",
                "\n",
                "## Applications and Examples\n",
                "\n",
                "Decision trees are versatile and can be applied in various domains:\n",
                "\n",
                "- **Banking:** For assessing the creditworthiness of applicants.\n",
                "- **Medicine:** For diagnosing patients based on their symptoms.\n",
                "- **Manufacturing:** For predicting the failure times of machines or equipment.\n",
                "- **E-commerce:** For recommending products based on user behavior.\n",
                "\n",
                "### Exercise Instructions\n",
                "\n",
                "Your task is to apply decision tree algorithms on a provided dataset, following these steps:\n",
                "\n",
                "1. **Preprocessing the Dataset:**\n",
                "   - Begin by loading the dataset.\n",
                "   - Perform necessary preprocessing steps such as dealing with missing values, encoding categorical variables, and splitting the dataset into training and testing sets.\n",
                "\n",
                "2. **Building the Decision Tree:**\n",
                "   - Use `scikit-learn` to fit a decision tree model to the training data.\n",
                "   - Experiment with different parameters such as `max_depth` and `min_samples_split` to observe how they affect overfitting and the complexity of the tree.\n",
                "\n",
                "3. **Visualizing the Decision Tree:**\n",
                "   - Utilize tools such as `graphviz` or `matplotlib` to visualize the tree.\n",
                "   - Interpret and understand the decision-making process of the model by examining the visualized tree.\n",
                "\n",
                "4. **Experimentation:**\n",
                "   - Adjust the parameters of the model to explore the trade-offs between model complexity and generalizability.\n",
                "   - Reflect on the impact of changes in parameters on the performance of the model on the training and testing sets.\n",
                "\n",
                "As you work through this exercise, think about the decision tree's structure and how altering its parameters affects its ability to generalize from the training data to unseen data. This exercise is an excellent opportunity for you to explore the practical aspects of building and tuning machine learning models.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false
            },
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import load_iris\n",
                "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "# Load sample dataset (Iris dataset)\n",
                "data = load_iris()\n",
                "X = data.data\n",
                "y = data.target\n",
                "\n",
                "# Preprocessing the dataset\n",
                "# Step 1: Splitting the dataset into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Building the Decision Tree\n",
                "# Step 2: Initialize the Decision Tree Classifier\n",
                "# Note: The reader can experiment with different parameters e.g., 'max_depth', 'min_samples_split' here.\n",
                "clf = DecisionTreeClassifier(random_state=42)\n",
                "\n",
                "# Step 3: Fit the model to the training data\n",
                "clf.fit(X_train, y_train)\n",
                "\n",
                "# Step 4: Make predictions on the test set\n",
                "y_pred = clf.predict(X_test)\n",
                "\n",
                "# Evaluating the model\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "print(f\"Accuracy: {accuracy:.2f}\")\n",
                "\n",
                "# Visualizing the Decision Tree\n",
                "# Step 5: Plotting the tree structure\n",
                "plt.figure(figsize=(20,10))\n",
                "plot_tree(clf, filled=True, feature_names=data.feature_names, class_names=data.target_names)\n",
                "plt.title('Decision Tree - Iris Dataset')\n",
                "plt.show()\n",
                "\n",
                "# Interpretation:\n",
                "# The visualization above shows the tree structure of the decision tree model trained on the Iris dataset.\n",
                "# Each node in the tree represents a decision rule based on one of the features, and the leaves represent the outcomes.\n",
                "# Experiment with different model parameters to see how the structure and performance of the tree change.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "This code provides a basic framework for loading a sample dataset, preprocessing it, fitting a decision tree model, assessing its accuracy, and visualizing the tree structure. It is intended for educational purposes, allowing readers to experiment with different parameters and understand their effects on the model.\n"
            ]
        }
    ],
    "metadata": {},
    "nbformat": 4,
    "nbformat_minor": 0
}